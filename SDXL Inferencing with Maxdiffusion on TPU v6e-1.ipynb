{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ayagup/stablediffusion/blob/main/SDXL%20Inferencing%20with%20Maxdiffusion%20on%20TPU%20v6e-1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JyG45Qk3qQLS"
      },
      "source": [
        "# Cells\n",
        "A notebook is a list of cells. Cells contain either explanatory text or executable code and its output. Click a cell to select it."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KR921S_OQSHG"
      },
      "source": [
        "## Code cells\n",
        "Below is a **code cell**. Once the toolbar button indicates CONNECTED, click in the cell to select it and execute the contents in the following ways:\n",
        "\n",
        "* Click the **Play icon** in the left gutter of the cell;\n",
        "* Type **Cmd/Ctrl+Enter** to run the cell in place;\n",
        "* Type **Shift+Enter** to run the cell and move focus to the next cell (adding one if none exists); or\n",
        "* Type **Alt+Enter** to run the cell and insert a new code cell immediately below it.\n",
        "\n",
        "There are additional options for running some or all cells in the **Runtime** menu.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "both",
        "id": "WUtu4316QSHL"
      },
      "outputs": [],
      "source": [
        "# a = 10\n",
        "# a"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Id6tDF1HQSHD"
      },
      "source": [
        "## Text cells\n",
        "This is a **text cell**. You can **double-click** to edit this cell. Text cells\n",
        "use markdown syntax. To learn more, see our [markdown\n",
        "guide](/notebooks/markdown_guide.ipynb).\n",
        "\n",
        "You can also add math to text cells using [LaTeX](http://www.latex-project.org/)\n",
        "to be rendered by [MathJax](https://www.mathjax.org). Just place the statement\n",
        "within a pair of **\\$** signs. For example `$\\sqrt{3x-1}+(1+x)^2$` becomes\n",
        "$\\sqrt{3x-1}+(1+x)^2.$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7bqjkGKwQSHW"
      },
      "source": [
        "## Adding and moving cells\n",
        "You can add new cells by using the **+ CODE** and **+ TEXT** buttons that show when you hover between cells. These buttons are also in the toolbar above the notebook where they can be used to add a cell below the currently selected cell.\n",
        "\n",
        "You can move a cell by selecting it and clicking **Cell Up** or **Cell Down** in the top toolbar.\n",
        "\n",
        "Consecutive cells can be selected by \"lasso selection\" by dragging from outside one cell and through the group.  Non-adjacent cells can be selected concurrently by clicking one and then holding down Ctrl while clicking another.  Similarly, using Shift instead of Ctrl will select all intermediate cells."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GOwlZRXEQSHZ"
      },
      "source": [
        "# Working with python\n",
        "Colaboratory is built on top of [Jupyter Notebook](https://jupyter.org/). Below are some examples of convenience functions provided."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RVuqWUXPQSHa"
      },
      "source": [
        "Long running python processes can be interrupted. Run the following cell and select **Runtime -> Interrupt execution** (*hotkey: Cmd/Ctrl-M I*) to stop execution."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "both",
        "id": "d-S-3nYLQSHb"
      },
      "outputs": [],
      "source": [
        "# import time\n",
        "# print(\"Sleeping\")\n",
        "# time.sleep(30) # sleep for a while; interrupt me!\n",
        "# print(\"Done Sleeping\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wej_mEyXQSHc"
      },
      "source": [
        "## System aliases\n",
        "\n",
        "Jupyter includes shortcuts for common operations, such as ls:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "both",
        "id": "5OCYEvK5QSHf"
      },
      "outputs": [],
      "source": [
        "# !ls /bin"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y8Da6JWKQSHh"
      },
      "source": [
        "That `!ls` probably generated a large output. You can select the cell and clear the output by either:\n",
        "\n",
        "1. Clicking on the clear output button (x) in the toolbar above the cell; or\n",
        "2. Right clicking the left gutter of the output area and selecting \"Clear output\" from the context menu.\n",
        "\n",
        "Execute any other process using `!` with string interpolation from python variables, and note the result can be assigned to a variable:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "both",
        "id": "zqGrv0blQSHj"
      },
      "outputs": [],
      "source": [
        "# # In https://github.com/ipython/ipython/pull/10545, single quote strings are ignored\n",
        "# message = 'Colaboratory is great!'\n",
        "# foo = !unset message && echo -e '{message}\\n{message}\\n'$message\"\\n$message\"\n",
        "# foo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qM4myQGfQboQ"
      },
      "source": [
        "## Magics\n",
        "Colaboratory shares the notion of magics from Jupyter. There are shorthand annotations that change how a cell's text is executed. To learn more, see [Jupyter's magics page](http://nbviewer.jupyter.org/github/ipython/ipython/blob/1.x/examples/notebooks/Cell%20Magics.ipynb).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "both",
        "id": "odfM-_GxWbCy"
      },
      "outputs": [],
      "source": [
        "# %%html\n",
        "# <marquee style='width: 30%; color: blue;'><b>Whee!</b></marquee>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_YrTcK7k22Fp"
      },
      "outputs": [],
      "source": [
        "# %%html\n",
        "# <svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 450 400\" width=\"200\" height=\"200\">\n",
        "#   <rect x=\"80\" y=\"60\" width=\"250\" height=\"250\" rx=\"20\" style=\"fill:red; stroke:black; fill-opacity:0.7\" />\n",
        "#   <rect x=\"180\" y=\"110\" width=\"250\" height=\"250\" rx=\"40\" style=\"fill:blue; stroke:black; fill-opacity:0.5;\" />\n",
        "# </svg>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d4L9TOP9QSHn"
      },
      "source": [
        "## Automatic completions and exploring code\n",
        "\n",
        "Colab provides automatic completions to explore attributes of Python objects, as well as to quickly view documentation strings. As an example, first run the following cell to import the  [`numpy`](http://www.numpy.org) module."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "both",
        "id": "Q0JKWcmtQSHp"
      },
      "outputs": [],
      "source": [
        "# import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1M890-bXeyYp"
      },
      "source": [
        "If you now insert your cursor after `np` and press **Period**(`.`), you will see the list of available completions within the `np` module. Completions can be opened again by using **Ctrl+Space**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "both",
        "id": "j6QRIfUHQSHq"
      },
      "outputs": [],
      "source": [
        "# np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g6MfomFhQSHs"
      },
      "source": [
        "If you type an open parenthesis after any function or class in the module, you will see a pop-up of its documentation string:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "both",
        "id": "SD0XnrVhQSHt"
      },
      "outputs": [],
      "source": [
        "# np.ndarray"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GVIVDgdaRjPQ"
      },
      "source": [
        "The documentation can be opened again using **Ctrl+Shift+Space** or you can view the documentation for method by mouse hovering over the method name.\n",
        "\n",
        "When hovering over the method name the `Open in tab` link will open the documentation in a persistent pane. The `View source` link will navigate to the source code for the method."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TYTBdJXxfqiJ"
      },
      "source": [
        "## Exception Formatting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4bqAVK-aQSHx"
      },
      "source": [
        "Exceptions are formatted nicely in Colab outputs:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "both",
        "id": "CrJf1PEmQSHx"
      },
      "outputs": [],
      "source": [
        "# x = 1\n",
        "# y = 4\n",
        "# z = y/(1-x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7cRnhv_7N4Pa"
      },
      "source": [
        "## Rich, interactive outputs\n",
        "Until now all of the generated outputs have been text, but they can be more interesting, like the chart below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JVXnTqyE9RET"
      },
      "outputs": [],
      "source": [
        "# import numpy as np\n",
        "# from matplotlib import pyplot as plt\n",
        "\n",
        "# ys = 200 + np.random.randn(100)\n",
        "# x = [x for x in range(len(ys))]\n",
        "\n",
        "# plt.plot(x, ys, '-')\n",
        "# plt.fill_between(x, ys, 195, where=(ys > 195), facecolor='g', alpha=0.6)\n",
        "\n",
        "# plt.title(\"Fills and Alpha Example\")\n",
        "# plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aro-UJgUQSH1"
      },
      "source": [
        "# Integration with Drive\n",
        "\n",
        "Colaboratory is integrated with Google Drive. It allows you to share, comment, and collaborate on the same document with multiple people:\n",
        "\n",
        "* The **SHARE** button (top-right of the toolbar) allows you to share the notebook and control permissions set on it.\n",
        "\n",
        "* **File->Make a Copy** creates a copy of the notebook in Drive.\n",
        "\n",
        "* **File->Save** saves the File to Drive. **File->Save and checkpoint** pins the version so it doesn't get deleted from the revision history.\n",
        "\n",
        "* **File->Revision history** shows the notebook's revision history."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4hfV37gxpP_c"
      },
      "source": [
        "## Commenting on a cell\n",
        "You can comment on a Colaboratory notebook like you would on a Google Document. Comments are attached to cells, and are displayed next to the cell they refer to. If you have **comment-only** permissions, you will see a comment button on the top right of the cell when you hover over it.\n",
        "\n",
        "If you have edit or comment permissions you can comment on a cell in one of three ways:\n",
        "\n",
        "1. Select a cell and click the comment button in the toolbar above the top-right corner of the cell.\n",
        "1. Right click a text cell and select **Add a comment** from the context menu.\n",
        "3. Use the shortcut **Ctrl+Shift+M** to add a comment to the currently selected cell.\n",
        "\n",
        "You can resolve and reply to comments, and you can target comments to specific collaborators by typing *+[email address]* (e.g., `+user@domain.com`). Addressed collaborators will be emailed.\n",
        "\n",
        "The Comment button in the top-right corner of the page shows all comments attached to the notebook."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import auth\n",
        "\n",
        "# Authenticate if using Cloud TPU resources outside the local Colab instance\n",
        "auth.authenticate_user()"
      ],
      "metadata": {
        "id": "4cKwPJ8u7_1e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Install the specialized vLLM build for TPUs\n",
        "!pip install vllm-tpu"
      ],
      "metadata": {
        "id": "jls3GpYl8LNi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "help(LLM.__init__)"
      ],
      "metadata": {
        "id": "695b8bYY8xXF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!export MAX_MODEL_LEN=2048\n",
        "!export TP=1 # number of chips"
      ],
      "metadata": {
        "id": "E7qbI8GnJzJd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !VLLM_LOGGING_LEVEL=DEBUG VLLM_TRACE_FUNCTION=1 vllm serve meta-llama/Llama-3.1-8B-Instruct \\\n",
        "!vllm serve meta-llama/Llama-3.1-8B-Instruct \\\n",
        "    --seed 42 \\\n",
        "    --disable-log-requests \\\n",
        "    --no-enable-prefix-caching \\\n",
        "    --async-scheduling \\\n",
        "    --gpu-memory-utilization 0.98 \\\n",
        "    --max-num-batched-tokens 4096 \\\n",
        "    --max-num-seqs 128 \\\n",
        "    --tensor-parallel-size 1 \\\n",
        "    --max-model-len 4096"
      ],
      "metadata": {
        "id": "6tnqHRbaMg4n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import login\n",
        "from google.colab import userdata\n",
        "\n",
        "# Automatically retrieves the token from your Colab Secrets\n",
        "hf_token = userdata.get('HF_TOKEN')\n",
        "login(hf_token)"
      ],
      "metadata": {
        "id": "907ZzpRYMhuF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!curl http://127.0.0.1:8000/ping"
      ],
      "metadata": {
        "id": "loamJTgVOAJ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import sys\n",
        "import tempfile\n",
        "from vllm import LLM, SamplingParams\n",
        "\n",
        "# Set environment variables\n",
        "os.environ['VLLM_LOGGING_LEVEL'] = 'INFO'\n",
        "\n",
        "# Workaround for Jupyter/Colab: Create a wrapper for stdout that supports fileno()\n",
        "class StdoutWrapper:\n",
        "    \"\"\"Wrapper for sys.stdout that provides a fileno() method for Jupyter compatibility\"\"\"\n",
        "    def __init__(self, original_stdout):\n",
        "        self.original_stdout = original_stdout\n",
        "        self._temp_file = tempfile.TemporaryFile(mode='w+', buffering=1)\n",
        "\n",
        "    def fileno(self):\n",
        "        return self._temp_file.fileno()\n",
        "\n",
        "    def __getattr__(self, name):\n",
        "        return getattr(self.original_stdout, name)\n",
        "\n",
        "    def __del__(self):\n",
        "        try:\n",
        "            self._temp_file.close()\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "# Patch stdout/stderr before vLLM initialization\n",
        "print(\"üîß Applying Jupyter compatibility patches...\")\n",
        "original_stdout = sys.stdout\n",
        "original_stderr = sys.stderr\n",
        "\n",
        "sys.stdout = StdoutWrapper(original_stdout)\n",
        "sys.stderr = StdoutWrapper(original_stderr)\n",
        "\n",
        "try:\n",
        "    print(\"=\" * 80)\n",
        "    print(\"Initializing vLLM Engine\")\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "    print(f\"\\nüì° Engine Configuration:\")\n",
        "    print(f\"  - Model: meta-llama/Llama-3.1-8B-Instruct\")\n",
        "    print(f\"  - Max Model Length: 4096 tokens\")\n",
        "    print(f\"  - Max Sequences: 128\")\n",
        "    print(f\"  - GPU Memory: 98%\")\n",
        "    print(\"\\n‚öôÔ∏è  Initializing engine (this may take 10-15 minutes for model compilation)...\\n\")\n",
        "\n",
        "    # Create the LLM engine\n",
        "    llm = LLM(\n",
        "        model=\"meta-llama/Llama-3.1-8B-Instruct\",\n",
        "        seed=42,\n",
        "        gpu_memory_utilization=0.98,\n",
        "        max_model_len=4096,\n",
        "        max_num_batched_tokens=4096,\n",
        "        max_num_seqs=128,\n",
        "        tensor_parallel_size=1,\n",
        "    )\n",
        "\n",
        "    print(\"\\n‚úÖ Engine initialized successfully!\")\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(\"Engine ready for inference!\")\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "finally:\n",
        "    # Restore original stdout/stderr\n",
        "    sys.stdout = original_stdout\n",
        "    sys.stderr = original_stderr\n",
        "\n",
        "# Example usage function\n",
        "def generate_text(prompt: str, temperature: float = 0.7, max_tokens: int = 2048):\n",
        "    \"\"\"Generate text using the vLLM engine\"\"\"\n",
        "    sampling_params = SamplingParams(\n",
        "        temperature=temperature,\n",
        "        max_tokens=max_tokens,\n",
        "    )\n",
        "\n",
        "    outputs = llm.generate([prompt], sampling_params)\n",
        "    return outputs[0].outputs[0].text\n",
        "\n",
        "print(\"\\nüìù Example Usage:\")\n",
        "print(\"\"\"\n",
        "# Generate text directly\n",
        "prompt = \"Explain quantum computing in simple terms:\"\n",
        "result = generate_text(prompt, temperature=0.7, max_tokens=512)\n",
        "print(result)\n",
        "\"\"\")\n",
        "print(\"\\n‚úÖ Engine is ready! Use the generate_text() function above to generate text.\")"
      ],
      "metadata": {
        "id": "_jhWZFYNURsL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"Explain quantum computing in simple terms. Respond in .md format.\"\n",
        "result = generate_text(prompt, temperature=0.7, max_tokens=4096)\n",
        "print(result)"
      ],
      "metadata": {
        "id": "zM_ux6NeYoRn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "09D26CpOgP4D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "yPp7IVx_kNXv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "6-nDP_OUlzDN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import sys\n",
        "import tempfile\n",
        "from vllm import LLM, SamplingParams\n",
        "\n",
        "# Set environment variables\n",
        "os.environ['VLLM_LOGGING_LEVEL'] = 'INFO'\n",
        "\n",
        "# Workaround for Jupyter/Colab: Create a wrapper for stdout that supports fileno()\n",
        "class StdoutWrapper:\n",
        "    \"\"\"Wrapper for sys.stdout that provides a fileno() method for Jupyter compatibility\"\"\"\n",
        "    def __init__(self, original_stdout):\n",
        "        self.original_stdout = original_stdout\n",
        "        self._temp_file = tempfile.TemporaryFile(mode='w+', buffering=1)\n",
        "\n",
        "    def fileno(self):\n",
        "        return self._temp_file.fileno()\n",
        "\n",
        "    def __getattr__(self, name):\n",
        "        return getattr(self.original_stdout, name)\n",
        "\n",
        "    def __del__(self):\n",
        "        try:\n",
        "            self._temp_file.close()\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "# Patch stdout/stderr before vLLM initialization\n",
        "print(\"üîß Applying Jupyter compatibility patches...\")\n",
        "original_stdout = sys.stdout\n",
        "original_stderr = sys.stderr\n",
        "\n",
        "sys.stdout = StdoutWrapper(original_stdout)\n",
        "sys.stderr = StdoutWrapper(original_stderr)\n",
        "\n",
        "try:\n",
        "    print(\"=\" * 80)\n",
        "    print(\"Initializing vLLM Engine\")\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "    print(f\"\\nüì° Engine Configuration:\")\n",
        "    print(f\"  - Model: Qwen/Qwen3-32B\")\n",
        "    print(f\"  - Max Model Length: 4096 tokens\")\n",
        "    print(f\"  - Max Sequences: 128\")\n",
        "    print(f\"  - GPU Memory: 98%\")\n",
        "    print(\"\\n‚öôÔ∏è  Initializing engine (this may take 10-15 minutes for model compilation)...\\n\")\n",
        "\n",
        "    # Create the LLM engine\n",
        "    llm = LLM(\n",
        "        model=\"Qwen/Qwen3-32B\",\n",
        "        seed=42,\n",
        "        gpu_memory_utilization=0.98,\n",
        "        max_model_len=4096,\n",
        "        max_num_batched_tokens=2048,\n",
        "        max_num_seqs=256,\n",
        "        tensor_parallel_size=1,\n",
        "    )\n",
        "\n",
        "    print(\"\\n‚úÖ Engine initialized successfully!\")\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(\"Engine ready for inference!\")\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "finally:\n",
        "    # Restore original stdout/stderr\n",
        "    sys.stdout = original_stdout\n",
        "    sys.stderr = original_stderr\n",
        "\n",
        "# Example usage function\n",
        "def generate_text(prompt: str, temperature: float = 0.7, max_tokens: int = 2048):\n",
        "    \"\"\"Generate text using the vLLM engine\"\"\"\n",
        "    sampling_params = SamplingParams(\n",
        "        temperature=temperature,\n",
        "        max_tokens=max_tokens,\n",
        "    )\n",
        "\n",
        "    outputs = llm.generate([prompt], sampling_params)\n",
        "    return outputs[0].outputs[0].text\n",
        "\n",
        "print(\"\\nüìù Example Usage:\")\n",
        "print(\"\"\"\n",
        "# Generate text directly\n",
        "prompt = \"Explain quantum computing in simple terms:\"\n",
        "result = generate_text(prompt, temperature=0.7, max_tokens=512)\n",
        "print(result)\n",
        "\"\"\")\n",
        "print(\"\\n‚úÖ Engine is ready! Use the generate_text() function above to generate text.\")"
      ],
      "metadata": {
        "id": "3L5odSlikNdf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !VLLM_LOGGING_LEVEL=DEBUG VLLM_TRACE_FUNCTION=1 vllm serve meta-llama/Llama-3.1-8B-Instruct \\\n",
        "!vllm serve Qwen/Qwen3-14B \\\n",
        "    --seed 42 \\\n",
        "    --disable-log-requests \\\n",
        "    --no-enable-prefix-caching \\\n",
        "    --async-scheduling \\\n",
        "    --gpu-memory-utilization 0.98 \\\n",
        "    --max-num-batched-tokens 2048 \\\n",
        "    --max-num-seqs 256 \\\n",
        "    --tensor-parallel-size 1 \\\n",
        "    --max-model-len 8192"
      ],
      "metadata": {
        "id": "F5pxv6A1lVMG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !VLLM_LOGGING_LEVEL=DEBUG VLLM_TRACE_FUNCTION=1 vllm serve meta-llama/Llama-3.1-8B-Instruct \\\n",
        "!vllm serve meta-llama/CodeLlama-7b-Instruct-hf \\\n",
        "    --seed 42 \\\n",
        "    --disable-log-requests \\\n",
        "    --no-enable-prefix-caching \\\n",
        "    --async-scheduling \\\n",
        "    --gpu-memory-utilization 0.98 \\\n",
        "    --max-num-batched-tokens 2048 \\\n",
        "    --max-num-seqs 256 \\\n",
        "    --tensor-parallel-size 1 \\\n",
        "    --max-model-len 4096"
      ],
      "metadata": {
        "id": "KhezOQJRrgU6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !VLLM_LOGGING_LEVEL=DEBUG VLLM_TRACE_FUNCTION=1 vllm serve meta-llama/Llama-3.1-8B-Instruct \\\n",
        "!vllm serve microsoft/phi-4 \\\n",
        "    --seed 42 \\\n",
        "    --disable-log-requests \\\n",
        "    --no-enable-prefix-caching \\\n",
        "    --async-scheduling \\\n",
        "    --gpu-memory-utilization 0.98 \\\n",
        "    --max-num-batched-tokens 2048 \\\n",
        "    --max-num-seqs 256 \\\n",
        "    --tensor-parallel-size 1 \\\n",
        "    --max-model-len 4096"
      ],
      "metadata": {
        "id": "P-fQU168wV6p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GG_hmZDIePDz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eazQSsA7pqU6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hnNyqhqYpqXt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/AI-Hypercomputer/maxdiffusion.git"
      ],
      "metadata": {
        "id": "lSt0Q1v0pqag"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cd maxdiffusion && bash setup.sh MODE=stable DEVICE=tpu"
      ],
      "metadata": {
        "id": "62c5XW0OpstW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !cd maxdiffusion && pip install -e .\n",
        "!cd maxdiffusion && pip install -r requirements.txt"
      ],
      "metadata": {
        "id": "NpSCJgz5p-8J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!cd maxdiffusion && export PYTHONPATH=$PYTHONPATH:$(pwd)/src && python -m maxdiffusion.generate_video \\\n",
        "  src/maxdiffusion/configs/svd_xt_v6e.yml \\\n",
        "  image_path=\"./input_image.png\" \\\n",
        "  output_path=\"./output_video.mp4\""
      ],
      "metadata": {
        "id": "zylk4snlqqA_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!export PYTHONPATH=$PYTHONPATH:/content/maxdiffusion/src && python3 -m maxdiffusion.generate_video src/maxdiffusion/configs/svd_xt_v6e.yml"
      ],
      "metadata": {
        "id": "Wy_a4NjruwEo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Running in Colab with TPU v6e-1\n",
        "import os\n",
        "import sys\n",
        "\n",
        "# 1. Add the 'src' directory to the system path so Python can find 'maxdiffusion'\n",
        "# This is the Python-native equivalent of setting PYTHONPATH\n",
        "sys.path.append('/content/maxdiffusion/src')\n",
        "\n",
        "# 2. Set the environment variables required for TPU v6e performance\n",
        "os.environ[\"LIBTPU_INIT_ARGS\"] = \"--xla_tpu_rwb_fusion=false --xla_tpu_dot_dot_fusion_duplicated=true\"\n",
        "os.environ[\"PYTHONPATH\"] = \"/content/maxdiffusion/src\"\n",
        "\n",
        "# 3. Run the generation script using the full module path\n",
        "# %run /content/maxdiffusion/src/maxdiffusion/generate_sdxl.py /content/maxdiffusion/src/maxdiffusion/configs/svd_xt_v6e.yml\n",
        "%run /content/maxdiffusion/src/maxdiffusion/generate_sdxl.py /content/maxdiffusion/src/maxdiffusion/configs/base_xl.yml"
      ],
      "metadata": {
        "id": "gAkOgqnYvSCc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Running in Colab with TPU v6e-1\n",
        "import os\n",
        "import sys\n",
        "\n",
        "# 1. Add the 'src' directory to the system path so Python can find 'maxdiffusion'\n",
        "# This is the Python-native equivalent of setting PYTHONPATH\n",
        "sys.path.append('/content/maxdiffusion/src')\n",
        "\n",
        "# 2. Set the environment variables required for TPU v6e performance\n",
        "os.environ[\"LIBTPU_INIT_ARGS\"] = \"--xla_tpu_rwb_fusion=false --xla_tpu_dot_dot_fusion_duplicated=true\"\n",
        "os.environ[\"PYTHONPATH\"] = \"/content/maxdiffusion/src\"\n",
        "\n",
        "# 3. Run the generation script using the full module path\n",
        "# %run /content/maxdiffusion/src/maxdiffusion/generate_sdxl.py /content/maxdiffusion/src/maxdiffusion/configs/svd_xt_v6e.yml\n",
        "%run /content/maxdiffusion/src/maxdiffusion/generate_sdxl.py /content/maxdiffusion/src/maxdiffusion/configs/ltx_video.yml"
      ],
      "metadata": {
        "id": "uM7SJfPSvZOe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import login\n",
        "from google.colab import userdata\n",
        "\n",
        "# Automatically retrieves the token from your Colab Secrets\n",
        "hf_token = userdata.get('HF_TOKEN')\n",
        "login(hf_token)"
      ],
      "metadata": {
        "id": "DBf6fkCFwr3M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Q9emI8QyCp_O"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "Overview of Colaboratory Features",
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "V6E1",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "TPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}