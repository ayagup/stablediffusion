{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.13",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [],
      "dockerImageVersionId": 31089,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "name": "hf-llm-lora-train-gpu-dataset",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ayagup/stablediffusion/blob/main/hf_llm_lora_train_gpu_dataset.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers datasets peft accelerate bitsandbytes torch"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-08-28T12:31:18.055114Z",
          "iopub.execute_input": "2025-08-28T12:31:18.05527Z",
          "iopub.status.idle": "2025-08-28T12:32:30.427055Z",
          "shell.execute_reply.started": "2025-08-28T12:31:18.055255Z",
          "shell.execute_reply": "2025-08-28T12:32:30.426123Z"
        },
        "id": "O6WjL5E6t6Jb"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "!mkdir -p /root/.config/kaggle\n",
        "!echo \"{\\\"username\\\":\\\"maygup123\\\",\\\"key\\\":\\\"e8ff771508f59b00b55d840f011f1916\\\"}\" > /root/.config/kaggle/kaggle.json"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-08-28T12:33:25.193763Z",
          "iopub.execute_input": "2025-08-28T12:33:25.194295Z",
          "iopub.status.idle": "2025-08-28T12:33:25.423427Z",
          "shell.execute_reply.started": "2025-08-28T12:33:25.19427Z",
          "shell.execute_reply": "2025-08-28T12:33:25.422658Z"
        },
        "id": "C7w1bF8At6Jl"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import kaggle\n",
        "\n",
        "kaggle.api.dataset_download_files('mohammadnouralawad/spider-text-sql', path='./data', unzip=True)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-08-28T12:33:27.290926Z",
          "iopub.execute_input": "2025-08-28T12:33:27.2912Z",
          "iopub.status.idle": "2025-08-28T12:33:28.099424Z",
          "shell.execute_reply.started": "2025-08-28T12:33:27.291174Z",
          "shell.execute_reply": "2025-08-28T12:33:28.098846Z"
        },
        "id": "Wqd_roQkt6Jn"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "print(os.getcwd())"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-08-28T12:33:31.563903Z",
          "iopub.execute_input": "2025-08-28T12:33:31.564641Z",
          "iopub.status.idle": "2025-08-28T12:33:31.568564Z",
          "shell.execute_reply.started": "2025-08-28T12:33:31.564615Z",
          "shell.execute_reply": "2025-08-28T12:33:31.567854Z"
        },
        "id": "buLeoiqWt6Jo"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "!head /kaggle/working/data/spider_text_sql.csv"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-08-28T12:33:32.058877Z",
          "iopub.execute_input": "2025-08-28T12:33:32.059139Z",
          "iopub.status.idle": "2025-08-28T12:33:32.179325Z",
          "shell.execute_reply.started": "2025-08-28T12:33:32.059118Z",
          "shell.execute_reply": "2025-08-28T12:33:32.178658Z"
        },
        "id": "--FeoXoDt6Jp"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# import pandas as pd\n",
        "\n",
        "# df = pd.read_csv('/kaggle/working/data/spider_text_sql.csv')\n",
        "# df['type'] = 'Write a SELECT query'\n",
        "# df = df[['type', 'text_query', 'sql_command']]\n",
        "# list_of_tuples = [tuple(row) for row in df.itertuples(index=False)]\n",
        "# sql_examples.extend(list_of_tuples)\n",
        "# df.head()"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-08-28T12:33:37.042871Z",
          "iopub.execute_input": "2025-08-28T12:33:37.043497Z",
          "iopub.status.idle": "2025-08-28T12:33:37.047319Z",
          "shell.execute_reply.started": "2025-08-28T12:33:37.043467Z",
          "shell.execute_reply": "2025-08-28T12:33:37.046587Z"
        },
        "id": "tftq4lu2t6Jr"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import os\n",
        "import gc\n",
        "import json\n",
        "import random\n",
        "from datetime import datetime\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForCausalLM,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        "    DataCollatorForLanguageModeling,\n",
        "    TrainerCallback\n",
        ")\n",
        "from datasets import Dataset\n",
        "from peft import (\n",
        "    LoraConfig,\n",
        "    get_peft_model,\n",
        "    TaskType,\n",
        "    PeftModel,\n",
        "    prepare_model_for_kbit_training\n",
        ")\n",
        "import numpy as np\n",
        "\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-08-28T12:33:37.066606Z",
          "iopub.execute_input": "2025-08-28T12:33:37.066809Z",
          "iopub.status.idle": "2025-08-28T12:34:04.334386Z",
          "shell.execute_reply.started": "2025-08-28T12:33:37.066793Z",
          "shell.execute_reply": "2025-08-28T12:34:04.333818Z"
        },
        "id": "PqiP3Pq3t6Js"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "def check_gpu():\n",
        "    \"\"\"Check and display GPU information\"\"\"\n",
        "    if torch.cuda.is_available():\n",
        "        gpu_name = torch.cuda.get_device_name(0)\n",
        "        gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
        "        print(f\"üöÄ GPU Available: {gpu_name}\")\n",
        "        print(f\"üíæ GPU Memory: {gpu_memory:.1f}GB\")\n",
        "        return torch.device(\"cuda\")\n",
        "    else:\n",
        "        print(\"‚ö†Ô∏è  No GPU available, using CPU\")\n",
        "        return torch.device(\"cpu\")\n",
        "\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-08-28T12:34:04.335469Z",
          "iopub.execute_input": "2025-08-28T12:34:04.335947Z",
          "iopub.status.idle": "2025-08-28T12:34:04.340895Z",
          "shell.execute_reply.started": "2025-08-28T12:34:04.335929Z",
          "shell.execute_reply": "2025-08-28T12:34:04.340197Z"
        },
        "id": "jeMJ35Ydt6Ju"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "class WorkingLoRASQLTrainer:\n",
        "    def __init__(self):\n",
        "        print(\"üîß Initializing Working LoRA SQL Training Pipeline\")\n",
        "        self.device = check_gpu()\n",
        "        self.timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "        self.output_dir = f\"./lora_sql_training_{self.timestamp}\"\n",
        "\n",
        "        os.makedirs(self.output_dir, exist_ok=True)\n",
        "        print(f\"üìÅ Output directory: {self.output_dir}\")\n",
        "\n",
        "        if torch.cuda.is_available():\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "    def download_and_setup_model(self, model_name=\"microsoft/DialoGPT-medium\"):\n",
        "        \"\"\"Download and setup model for LoRA training\"\"\"\n",
        "        print(f\"üì• Setting up model: {model_name}\")\n",
        "\n",
        "        try:\n",
        "            # Load tokenizer\n",
        "            tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "            if tokenizer.pad_token is None:\n",
        "                tokenizer.pad_token = tokenizer.eos_token\n",
        "                tokenizer.pad_token_id = tokenizer.eos_token_id\n",
        "\n",
        "            print(f\"üìù Tokenizer loaded, vocab size: {len(tokenizer)}\")\n",
        "\n",
        "            # Load model with proper settings\n",
        "            model = AutoModelForCausalLM.from_pretrained(\n",
        "                model_name,\n",
        "                torch_dtype=torch.float16 if self.device.type == 'cuda' else torch.float32,\n",
        "                device_map=None,\n",
        "                trust_remote_code=True,\n",
        "                use_cache=False  # Disable cache for training\n",
        "            )\n",
        "\n",
        "            # Move to device\n",
        "            model = model.to(self.device)\n",
        "\n",
        "            # Prepare for LoRA training\n",
        "            if self.device.type == 'cuda':\n",
        "                model = prepare_model_for_kbit_training(model)\n",
        "\n",
        "            print(f\"ü§ñ Model prepared for LoRA training\")\n",
        "            print(f\"üß† Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
        "\n",
        "            return model, tokenizer\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Error setting up model: {e}\")\n",
        "            raise\n",
        "\n",
        "    def create_sql_dataset(self, num_samples=200):\n",
        "        \"\"\"Create focused SQL dataset\"\"\"\n",
        "        import pandas as pd\n",
        "\n",
        "        print(f\"üìä Creating SQL dataset with {num_samples} samples...\")\n",
        "\n",
        "        sql_examples = [\n",
        "            # Basic SELECT queries\n",
        "            (\"Write a SELECT query\", \"Get all records from users table\", \"SELECT * FROM users;\"),\n",
        "            (\"Write a SELECT query\", \"Get name and email from users\", \"SELECT name, email FROM users;\"),\n",
        "            (\"Write a SELECT query\", \"Get all products\", \"SELECT * FROM products;\"),\n",
        "\n",
        "            # WHERE clauses\n",
        "            (\"Write a WHERE query\", \"Find users older than 25\", \"SELECT * FROM users WHERE age > 25;\"),\n",
        "            (\"Write a WHERE query\", \"Find active users\", \"SELECT * FROM users WHERE status = 'active';\"),\n",
        "            (\"Write a WHERE query\", \"Find products under $50\", \"SELECT * FROM products WHERE price < 50;\"),\n",
        "\n",
        "            # COUNT queries\n",
        "            (\"Write a COUNT query\", \"Count all users\", \"SELECT COUNT(*) FROM users;\"),\n",
        "            (\"Write a COUNT query\", \"Count active orders\", \"SELECT COUNT(*) FROM orders WHERE status = 'active';\"),\n",
        "\n",
        "            # ORDER BY queries\n",
        "            (\"Write an ORDER BY query\", \"Sort users by name\", \"SELECT * FROM users ORDER BY name;\"),\n",
        "            (\"Write an ORDER BY query\", \"Sort products by price descending\", \"SELECT * FROM products ORDER BY price DESC;\"),\n",
        "\n",
        "            # GROUP BY queries\n",
        "            (\"Write a GROUP BY query\", \"Count users by department\", \"SELECT department, COUNT(*) FROM users GROUP BY department;\"),\n",
        "            (\"Write a GROUP BY query\", \"Sum sales by region\", \"SELECT region, SUM(amount) FROM sales GROUP BY region;\"),\n",
        "\n",
        "            # JOIN queries\n",
        "            (\"Write a JOIN query\", \"Join users and orders\", \"SELECT u.name, o.total FROM users u JOIN orders o ON u.id = o.user_id;\"),\n",
        "            (\"Write a JOIN query\", \"Join products and categories\", \"SELECT p.name, c.category_name FROM products p JOIN categories c ON p.category_id = c.id;\"),\n",
        "\n",
        "            # INSERT queries\n",
        "            (\"Write an INSERT query\", \"Insert new user\", \"INSERT INTO users (name, email) VALUES ('John Doe', 'john@email.com');\"),\n",
        "            (\"Write an INSERT query\", \"Insert new product\", \"INSERT INTO products (name, price) VALUES ('Widget', 19.99);\"),\n",
        "\n",
        "            # UPDATE queries\n",
        "            (\"Write an UPDATE query\", \"Update user email\", \"UPDATE users SET email = 'new@email.com' WHERE id = 1;\"),\n",
        "            (\"Write an UPDATE query\", \"Update product price\", \"UPDATE products SET price = 29.99 WHERE name = 'Widget';\"),\n",
        "\n",
        "            # DELETE queries\n",
        "            (\"Write a DELETE query\", \"Delete inactive users\", \"DELETE FROM users WHERE status = 'inactive';\"),\n",
        "            (\"Write a DELETE query\", \"Delete old orders\", \"DELETE FROM orders WHERE date < '2023-01-01';\"),\n",
        "        ]\n",
        "\n",
        "        df = pd.read_csv('/kaggle/working/data/spider_text_sql.csv')\n",
        "        df['type'] = 'Write a SELECT query'\n",
        "        df = df[['type', 'text_query', 'sql_command']]\n",
        "        list_of_tuples = [tuple(row) for row in df.itertuples(index=False)]\n",
        "        sql_examples.extend(list_of_tuples)\n",
        "\n",
        "        # Generate dataset by cycling through examples\n",
        "        dataset = []\n",
        "        for i in range(num_samples):\n",
        "            example = sql_examples[i % len(sql_examples)]\n",
        "            instruction, problem, solution = example\n",
        "\n",
        "            # Simple format that works well\n",
        "            text = f\"Instruction: {instruction}\\nInput: {problem}\\nOutput: {solution}\"\n",
        "            dataset.append({\"text\": text})\n",
        "\n",
        "        print(f\"‚úÖ Created {len(dataset)} SQL examples\")\n",
        "        return Dataset.from_list(dataset)\n",
        "\n",
        "    def setup_lora_config(self):\n",
        "        \"\"\"Setup LoRA configuration\"\"\"\n",
        "        print(\"üîß Setting up LoRA configuration...\")\n",
        "\n",
        "        lora_config = LoraConfig(\n",
        "            task_type=TaskType.CAUSAL_LM,\n",
        "            inference_mode=False,\n",
        "            r=4,                     # Very small rank for stability\n",
        "            lora_alpha=8,            # Alpha = 2 * r\n",
        "            lora_dropout=0.05,\n",
        "            target_modules=[\"c_attn\"],  # Only target attention for simplicity\n",
        "            bias=\"none\",\n",
        "            modules_to_save=None,\n",
        "        )\n",
        "\n",
        "        print(f\"‚úÖ LoRA Config: r={lora_config.r}, alpha={lora_config.lora_alpha}\")\n",
        "        return lora_config\n",
        "\n",
        "    def apply_lora_to_model(self, model, lora_config):\n",
        "        \"\"\"Apply LoRA to model\"\"\"\n",
        "        print(\"üîÑ Applying LoRA to model...\")\n",
        "\n",
        "        try:\n",
        "            peft_model = get_peft_model(model, lora_config)\n",
        "            peft_model.train()\n",
        "\n",
        "            # Count parameters\n",
        "            trainable_params = sum(p.numel() for p in peft_model.parameters() if p.requires_grad)\n",
        "            total_params = sum(p.numel() for p in peft_model.parameters())\n",
        "\n",
        "            print(f\"üß† Total parameters: {total_params:,}\")\n",
        "            print(f\"üéØ Trainable parameters: {trainable_params:,}\")\n",
        "            print(f\"üìä Trainable percentage: {(trainable_params/total_params)*100:.2f}%\")\n",
        "\n",
        "            if trainable_params == 0:\n",
        "                raise RuntimeError(\"No trainable parameters found!\")\n",
        "\n",
        "            return peft_model\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Error applying LoRA: {e}\")\n",
        "            raise\n",
        "\n",
        "    def tokenize_dataset(self, dataset, tokenizer):\n",
        "        \"\"\"Tokenize dataset\"\"\"\n",
        "        print(\"üî§ Tokenizing dataset...\")\n",
        "\n",
        "        def tokenize_function(examples):\n",
        "            result = tokenizer(\n",
        "                examples[\"text\"],\n",
        "                truncation=True,\n",
        "                padding=True,\n",
        "                max_length=200,  # Short for memory efficiency\n",
        "                return_tensors=\"np\"\n",
        "            )\n",
        "\n",
        "            tokenized = {key: values.tolist() for key, values in result.items()}\n",
        "            tokenized[\"labels\"] = tokenized[\"input_ids\"].copy()\n",
        "            return tokenized\n",
        "\n",
        "        tokenized_dataset = dataset.map(\n",
        "            tokenize_function,\n",
        "            batched=True,\n",
        "            batch_size=50,\n",
        "            remove_columns=dataset.column_names,\n",
        "            desc=\"Tokenizing\"\n",
        "        )\n",
        "\n",
        "        # Filter short sequences\n",
        "        tokenized_dataset = tokenized_dataset.filter(lambda x: len(x[\"input_ids\"]) > 10)\n",
        "\n",
        "        print(f\"‚úÖ Tokenized {len(tokenized_dataset)} examples\")\n",
        "        return tokenized_dataset\n",
        "\n",
        "    def create_training_arguments(self):\n",
        "        \"\"\"Create training arguments\"\"\"\n",
        "        return TrainingArguments(\n",
        "            output_dir=self.output_dir,\n",
        "            num_train_epochs=20,              # Single epoch for quick test\n",
        "            per_device_train_batch_size=1,   # Very small batch\n",
        "            gradient_accumulation_steps=4,\n",
        "            learning_rate=5e-4,              # Higher LR for LoRA\n",
        "            lr_scheduler_type=\"linear\",\n",
        "            warmup_steps=10,\n",
        "            weight_decay=0.01,\n",
        "            max_grad_norm=0.3,\n",
        "            fp16=True if self.device.type == 'cuda' else False,\n",
        "            gradient_checkpointing=False,\n",
        "            dataloader_drop_last=True,\n",
        "            dataloader_num_workers=0,\n",
        "            logging_steps=5,\n",
        "            save_steps=50,\n",
        "            save_total_limit=1,\n",
        "            eval_strategy=\"no\",\n",
        "            prediction_loss_only=True,\n",
        "            seed=42,\n",
        "            report_to=[],\n",
        "            remove_unused_columns=True,\n",
        "        )\n",
        "\n",
        "    def test_model_generation(self, model, tokenizer, stage=\"\"):\n",
        "        \"\"\"Test model generation\"\"\"\n",
        "        print(f\"üß™ Testing model generation {stage}...\")\n",
        "\n",
        "        test_prompt = \"Instruction: Write a SELECT query\\nInput: Get all users\\nOutput:\"\n",
        "        inputs = tokenizer(test_prompt, return_tensors=\"pt\").to(self.device)\n",
        "\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            generated = model.generate(\n",
        "                input_ids=inputs.input_ids,\n",
        "                max_new_tokens=20,\n",
        "                temperature=0.8,\n",
        "                do_sample=True,\n",
        "                pad_token_id=tokenizer.pad_token_id\n",
        "            )\n",
        "\n",
        "            new_tokens = generated[0][inputs.input_ids.shape[1]:]\n",
        "            generated_text = tokenizer.decode(new_tokens, skip_special_tokens=True)\n",
        "            print(f\"üéØ Generated {stage}: {generated_text.strip()}\")\n",
        "\n",
        "        model.train()\n",
        "\n",
        "    def run_lora_training(self):\n",
        "        \"\"\"Run complete LoRA training\"\"\"\n",
        "        try:\n",
        "            print(\"üöÄ Starting Working LoRA SQL Training\")\n",
        "            print(\"=\" * 60)\n",
        "\n",
        "            # Setup components\n",
        "            model, tokenizer = self.download_and_setup_model()\n",
        "            dataset = self.create_sql_dataset(num_samples=100)  # Small dataset\n",
        "\n",
        "            # Apply LoRA\n",
        "            lora_config = self.setup_lora_config()\n",
        "            lora_model = self.apply_lora_to_model(model, lora_config)\n",
        "\n",
        "            # Test before training\n",
        "            self.test_model_generation(lora_model, tokenizer, \"before training\")\n",
        "\n",
        "            # Tokenize\n",
        "            tokenized_dataset = self.tokenize_dataset(dataset, tokenizer)\n",
        "\n",
        "            # Setup training\n",
        "            training_args = self.create_training_arguments()\n",
        "            data_collator = DataCollatorForLanguageModeling(\n",
        "                tokenizer=tokenizer,\n",
        "                mlm=False,\n",
        "                return_tensors=\"pt\"\n",
        "            )\n",
        "\n",
        "            # Create trainer WITHOUT callbacks first\n",
        "            trainer = Trainer(\n",
        "                model=lora_model,\n",
        "                args=training_args,\n",
        "                train_dataset=tokenized_dataset,\n",
        "                data_collator=data_collator,\n",
        "                processing_class=tokenizer,\n",
        "            )\n",
        "\n",
        "            # Add a proper callback class\n",
        "            class SimpleProgressCallback(TrainerCallback):\n",
        "                def on_log(self, args, state, control, logs=None, **kwargs):\n",
        "                    if logs and 'train_loss' in logs:\n",
        "                        step = state.global_step\n",
        "                        loss = logs['train_loss']\n",
        "                        print(f\"üìä Step {step}: Loss = {loss:.4f}\")\n",
        "\n",
        "                def on_train_begin(self, args, state, control, **kwargs):\n",
        "                    print(f\"üèÅ Training started!\")\n",
        "\n",
        "                def on_train_end(self, args, state, control, **kwargs):\n",
        "                    print(f\"üèÅ Training completed!\")\n",
        "\n",
        "            trainer.add_callback(SimpleProgressCallback())\n",
        "\n",
        "            # Start training\n",
        "            print(f\"\\nüèÉ Starting LoRA training...\")\n",
        "            training_result = trainer.train()\n",
        "\n",
        "            print(f\"‚úÖ Training completed!\")\n",
        "            print(f\"üìä Final loss: {training_result.training_loss:.4f}\")\n",
        "\n",
        "            # Test after training\n",
        "            self.test_model_generation(lora_model, tokenizer, \"after training\")\n",
        "\n",
        "            # Save LoRA adapters\n",
        "            adapter_path = f\"{self.output_dir}/lora_adapters\"\n",
        "            print(f\"\\nüíæ Saving LoRA adapters to {adapter_path}\")\n",
        "            lora_model.save_pretrained(adapter_path)\n",
        "            tokenizer.save_pretrained(adapter_path)\n",
        "\n",
        "            # Save training info\n",
        "            training_info = {\n",
        "                \"timestamp\": self.timestamp,\n",
        "                \"base_model\": \"microsoft/DialoGPT-medium\",\n",
        "                \"dataset_size\": len(dataset),\n",
        "                \"lora_rank\": lora_config.r,\n",
        "                \"final_loss\": float(training_result.training_loss),\n",
        "                \"device\": str(self.device)\n",
        "            }\n",
        "\n",
        "            with open(f\"{self.output_dir}/training_info.json\", \"w\") as f:\n",
        "                json.dump(training_info, f, indent=2)\n",
        "\n",
        "            print(f\"üìÅ All outputs saved to: {self.output_dir}\")\n",
        "            print(\"üéâ LoRA training completed successfully!\")\n",
        "\n",
        "            return lora_model, tokenizer, adapter_path\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå LoRA training failed: {e}\")\n",
        "            import traceback\n",
        "            traceback.print_exc()\n",
        "\n",
        "            # Cleanup\n",
        "            gc.collect()\n",
        "            if torch.cuda.is_available():\n",
        "                torch.cuda.empty_cache()\n",
        "            raise\n",
        "\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-08-28T12:34:04.341927Z",
          "iopub.execute_input": "2025-08-28T12:34:04.342213Z",
          "iopub.status.idle": "2025-08-28T12:34:04.37261Z",
          "shell.execute_reply.started": "2025-08-28T12:34:04.342189Z",
          "shell.execute_reply": "2025-08-28T12:34:04.371891Z"
        },
        "id": "Ulu_Ncidt6Jw"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "def demonstrate_trained_model(adapter_path):\n",
        "    \"\"\"Demonstrate the trained model\"\"\"\n",
        "    print(\"\\nüéØ Demonstrating Trained LoRA Model\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    try:\n",
        "        # Load model\n",
        "        tokenizer = AutoTokenizer.from_pretrained(adapter_path)\n",
        "        base_model = AutoModelForCausalLM.from_pretrained(\n",
        "            \"microsoft/DialoGPT-medium\",\n",
        "            torch_dtype=torch.float16,\n",
        "            device_map=\"auto\"\n",
        "        )\n",
        "        model = PeftModel.from_pretrained(base_model, adapter_path)\n",
        "\n",
        "        # Test prompts\n",
        "        test_prompts = [\n",
        "            \"Instruction: Write a SELECT query\\nInput: Get all customers\\nOutput:\",\n",
        "            \"Instruction: Write a COUNT query\\nInput: Count total orders\\nOutput:\",\n",
        "            \"Instruction: Write a JOIN query\\nInput: Join users and orders\\nOutput:\",\n",
        "        ]\n",
        "\n",
        "        model.eval()\n",
        "        for i, prompt in enumerate(test_prompts, 1):\n",
        "            print(f\"\\nüß™ Test {i}: {prompt.split('Input: ')[1].split('Output:')[0].strip()}\")\n",
        "\n",
        "            inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                generated = model.generate(\n",
        "                    **inputs,\n",
        "                    max_new_tokens=30,\n",
        "                    temperature=0.7,\n",
        "                    do_sample=True,\n",
        "                    pad_token_id=tokenizer.pad_token_id\n",
        "                )\n",
        "\n",
        "                new_tokens = generated[0][inputs.input_ids.shape[1]:]\n",
        "                generated_sql = tokenizer.decode(new_tokens, skip_special_tokens=True)\n",
        "                print(f\"üéâ Generated: {generated_sql.strip()}\")\n",
        "\n",
        "        print(\"\\n‚úÖ Model demonstration completed!\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Demo failed: {e}\")\n",
        "\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-08-28T12:34:04.373835Z",
          "iopub.execute_input": "2025-08-28T12:34:04.374031Z",
          "iopub.status.idle": "2025-08-28T12:34:04.394179Z",
          "shell.execute_reply.started": "2025-08-28T12:34:04.374016Z",
          "shell.execute_reply": "2025-08-28T12:34:04.393617Z"
        },
        "id": "hvRgYgjbt6J3"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# ===================================================================\n",
        "# Main Execution\n",
        "# ===================================================================\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Run LoRA training\n",
        "    trainer = WorkingLoRASQLTrainer()\n",
        "    lora_model, tokenizer, adapter_path = trainer.run_lora_training()\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"üéâ LoRA Training Complete!\")\n",
        "    print(f\"üìÅ Adapters saved to: {adapter_path}\")\n",
        "\n",
        "    # Demonstrate the trained model\n",
        "    demonstrate_trained_model(adapter_path)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-08-28T12:34:04.394835Z",
          "iopub.execute_input": "2025-08-28T12:34:04.395063Z",
          "iopub.status.idle": "2025-08-28T12:37:03.813916Z",
          "shell.execute_reply.started": "2025-08-28T12:34:04.395042Z",
          "shell.execute_reply": "2025-08-28T12:37:03.813255Z"
        },
        "id": "k8eorkaEt6J5"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "H0vQHW-Ft6J6"
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}