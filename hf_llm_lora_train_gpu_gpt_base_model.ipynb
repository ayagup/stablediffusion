{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.13",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [],
      "dockerImageVersionId": 31090,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "name": "hf_llm_lora_train_gpu_gpt_base_model",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ayagup/stablediffusion/blob/main/hf_llm_lora_train_gpu_gpt_base_model.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers datasets peft accelerate bitsandbytes torch"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-08-28T11:51:28.806491Z",
          "iopub.execute_input": "2025-08-28T11:51:28.808104Z",
          "iopub.status.idle": "2025-08-28T11:52:47.889887Z",
          "shell.execute_reply.started": "2025-08-28T11:51:28.808059Z",
          "shell.execute_reply": "2025-08-28T11:52:47.888914Z"
        },
        "id": "Ons7D0IUtV9a",
        "outputId": "f33bb7e4-5ccb-43a3-eb65-27df3bbb2100"
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.52.4)\nRequirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.6.0)\nRequirement already satisfied: peft in /usr/local/lib/python3.11/dist-packages (0.15.2)\nRequirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (1.8.1)\nCollecting bitsandbytes\n  Using cached bitsandbytes-0.47.0-py3-none-manylinux_2_24_x86_64.whl.metadata (11 kB)\nRequirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\nRequirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.33.1)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (25.0)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.4)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.2)\nRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\nRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\nRequirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (19.0.1)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\nRequirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.3)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\nRequirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\nCollecting fsspec<=2025.3.0,>=2023.1.0 (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets)\n  Using cached fsspec-2025.3.0-py3-none-any.whl.metadata (11 kB)\nRequirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from peft) (7.0.0)\nRequirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.14.0)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.5)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\nCollecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch)\n  Using cached nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-runtime-cu12==12.4.127 (from torch)\n  Using cached nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cuda-cupti-cu12==12.4.127 (from torch)\n  Using cached nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cudnn-cu12==9.1.0.70 (from torch)\n  Using cached nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cublas-cu12==12.4.5.8 (from torch)\n  Using cached nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cufft-cu12==11.2.1.3 (from torch)\n  Using cached nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-curand-cu12==10.3.5.147 (from torch)\n  Using cached nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nCollecting nvidia-cusolver-cu12==11.6.1.9 (from torch)\n  Using cached nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nCollecting nvidia-cusparse-cu12==12.3.1.170 (from torch)\n  Using cached nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\nRequirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch) (0.6.2)\nRequirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch) (12.4.127)\nCollecting nvidia-nvjitlink-cu12==12.4.127 (from torch)\n  Using cached nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\nRequirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\nRequirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.12.13)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.1.5)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->transformers) (2.4.1)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.5.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.6.15)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\nRequirement already satisfied: aiohappyeyeballs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.3.2)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.7.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.6.3)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.2)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.1)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->transformers) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->transformers) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->transformers) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->transformers) (2024.2.0)\nUsing cached bitsandbytes-0.47.0-py3-none-manylinux_2_24_x86_64.whl (61.3 MB)\nUsing cached nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\nUsing cached nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\nUsing cached nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\nUsing cached nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\nDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m29.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m64.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading fsspec-2025.3.0-py3-none-any.whl (193 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, fsspec, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, bitsandbytes\n  Attempting uninstall: nvidia-nvjitlink-cu12\n    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n  Attempting uninstall: nvidia-curand-cu12\n    Found existing installation: nvidia-curand-cu12 10.3.6.82\n    Uninstalling nvidia-curand-cu12-10.3.6.82:\n      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n  Attempting uninstall: nvidia-cufft-cu12\n    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n  Attempting uninstall: nvidia-cuda-runtime-cu12\n    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n  Attempting uninstall: nvidia-cuda-cupti-cu12\n    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n  Attempting uninstall: nvidia-cublas-cu12\n    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n  Attempting uninstall: fsspec\n    Found existing installation: fsspec 2025.5.1\n    Uninstalling fsspec-2025.5.1:\n      Successfully uninstalled fsspec-2025.5.1\n  Attempting uninstall: nvidia-cusparse-cu12\n    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n  Attempting uninstall: nvidia-cudnn-cu12\n    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n  Attempting uninstall: nvidia-cusolver-cu12\n    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbigframes 2.8.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\ncesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\ngcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\nbigframes 2.8.0 requires google-cloud-bigquery[bqstorage,pandas]>=3.31.0, but you have google-cloud-bigquery 3.25.0 which is incompatible.\nbigframes 2.8.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0mSuccessfully installed bitsandbytes-0.47.0 fsspec-2025.3.0 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import os\n",
        "import gc\n",
        "import json\n",
        "import random\n",
        "from datetime import datetime\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForCausalLM,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        "    DataCollatorForLanguageModeling,\n",
        "    TrainerCallback\n",
        ")\n",
        "from datasets import Dataset\n",
        "from peft import (\n",
        "    LoraConfig,\n",
        "    get_peft_model,\n",
        "    TaskType,\n",
        "    PeftModel,\n",
        "    prepare_model_for_kbit_training\n",
        ")\n",
        "import numpy as np\n",
        "\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-08-28T11:55:42.490393Z",
          "iopub.execute_input": "2025-08-28T11:55:42.491102Z",
          "iopub.status.idle": "2025-08-28T11:56:11.714498Z",
          "shell.execute_reply.started": "2025-08-28T11:55:42.491068Z",
          "shell.execute_reply": "2025-08-28T11:56:11.713907Z"
        },
        "id": "H1tS83TjtV9f",
        "outputId": "18c17444-acc0-48a4-96ce-380216a7c714"
      },
      "outputs": [
        {
          "name": "stderr",
          "text": "2025-08-28 11:55:56.867243: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1756382157.098337      36 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1756382157.164340      36 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "def check_gpu():\n",
        "    \"\"\"Check and display GPU information\"\"\"\n",
        "    if torch.cuda.is_available():\n",
        "        gpu_name = torch.cuda.get_device_name(0)\n",
        "        gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
        "        print(f\"🚀 GPU Available: {gpu_name}\")\n",
        "        print(f\"💾 GPU Memory: {gpu_memory:.1f}GB\")\n",
        "        return torch.device(\"cuda\")\n",
        "    else:\n",
        "        print(\"⚠️  No GPU available, using CPU\")\n",
        "        return torch.device(\"cpu\")\n",
        "\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-08-28T11:56:16.569545Z",
          "iopub.execute_input": "2025-08-28T11:56:16.570418Z",
          "iopub.status.idle": "2025-08-28T11:56:16.575081Z",
          "shell.execute_reply.started": "2025-08-28T11:56:16.570389Z",
          "shell.execute_reply": "2025-08-28T11:56:16.574252Z"
        },
        "id": "Tkfaw7LstV9g"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "class WorkingLoRASQLTrainer:\n",
        "    def __init__(self):\n",
        "        print(\"🔧 Initializing Working LoRA SQL Training Pipeline\")\n",
        "        self.device = check_gpu()\n",
        "        self.timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "        self.output_dir = f\"./lora_sql_training_{self.timestamp}\"\n",
        "\n",
        "        os.makedirs(self.output_dir, exist_ok=True)\n",
        "        print(f\"📁 Output directory: {self.output_dir}\")\n",
        "\n",
        "        if torch.cuda.is_available():\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "    def download_and_setup_model(self, model_name=\"microsoft/DialoGPT-medium\"):\n",
        "        \"\"\"Download and setup model for LoRA training\"\"\"\n",
        "        print(f\"📥 Setting up model: {model_name}\")\n",
        "\n",
        "        try:\n",
        "            # Load tokenizer\n",
        "            tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "            if tokenizer.pad_token is None:\n",
        "                tokenizer.pad_token = tokenizer.eos_token\n",
        "                tokenizer.pad_token_id = tokenizer.eos_token_id\n",
        "\n",
        "            print(f\"📝 Tokenizer loaded, vocab size: {len(tokenizer)}\")\n",
        "\n",
        "            # Load model with proper settings\n",
        "            model = AutoModelForCausalLM.from_pretrained(\n",
        "                model_name,\n",
        "                torch_dtype=torch.float16 if self.device.type == 'cuda' else torch.float32,\n",
        "                device_map=None,\n",
        "                trust_remote_code=True,\n",
        "                use_cache=False  # Disable cache for training\n",
        "            )\n",
        "\n",
        "            # Move to device\n",
        "            model = model.to(self.device)\n",
        "\n",
        "            # Prepare for LoRA training\n",
        "            if self.device.type == 'cuda':\n",
        "                model = prepare_model_for_kbit_training(model)\n",
        "\n",
        "            print(f\"🤖 Model prepared for LoRA training\")\n",
        "            print(f\"🧠 Model parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
        "\n",
        "            return model, tokenizer\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"❌ Error setting up model: {e}\")\n",
        "            raise\n",
        "\n",
        "    def create_sql_dataset(self, num_samples=200):\n",
        "        \"\"\"Create focused SQL dataset\"\"\"\n",
        "        print(f\"📊 Creating SQL dataset with {num_samples} samples...\")\n",
        "\n",
        "        sql_examples = [\n",
        "            # Basic SELECT queries\n",
        "            (\"Write a SELECT query\", \"Get all records from users table\", \"SELECT * FROM users;\"),\n",
        "            (\"Write a SELECT query\", \"Get name and email from users\", \"SELECT name, email FROM users;\"),\n",
        "            (\"Write a SELECT query\", \"Get all products\", \"SELECT * FROM products;\"),\n",
        "\n",
        "            # WHERE clauses\n",
        "            (\"Write a WHERE query\", \"Find users older than 25\", \"SELECT * FROM users WHERE age > 25;\"),\n",
        "            (\"Write a WHERE query\", \"Find active users\", \"SELECT * FROM users WHERE status = 'active';\"),\n",
        "            (\"Write a WHERE query\", \"Find products under $50\", \"SELECT * FROM products WHERE price < 50;\"),\n",
        "\n",
        "            # COUNT queries\n",
        "            (\"Write a COUNT query\", \"Count all users\", \"SELECT COUNT(*) FROM users;\"),\n",
        "            (\"Write a COUNT query\", \"Count active orders\", \"SELECT COUNT(*) FROM orders WHERE status = 'active';\"),\n",
        "\n",
        "            # ORDER BY queries\n",
        "            (\"Write an ORDER BY query\", \"Sort users by name\", \"SELECT * FROM users ORDER BY name;\"),\n",
        "            (\"Write an ORDER BY query\", \"Sort products by price descending\", \"SELECT * FROM products ORDER BY price DESC;\"),\n",
        "\n",
        "            # GROUP BY queries\n",
        "            (\"Write a GROUP BY query\", \"Count users by department\", \"SELECT department, COUNT(*) FROM users GROUP BY department;\"),\n",
        "            (\"Write a GROUP BY query\", \"Sum sales by region\", \"SELECT region, SUM(amount) FROM sales GROUP BY region;\"),\n",
        "\n",
        "            # JOIN queries\n",
        "            (\"Write a JOIN query\", \"Join users and orders\", \"SELECT u.name, o.total FROM users u JOIN orders o ON u.id = o.user_id;\"),\n",
        "            (\"Write a JOIN query\", \"Join products and categories\", \"SELECT p.name, c.category_name FROM products p JOIN categories c ON p.category_id = c.id;\"),\n",
        "\n",
        "            # INSERT queries\n",
        "            (\"Write an INSERT query\", \"Insert new user\", \"INSERT INTO users (name, email) VALUES ('John Doe', 'john@email.com');\"),\n",
        "            (\"Write an INSERT query\", \"Insert new product\", \"INSERT INTO products (name, price) VALUES ('Widget', 19.99);\"),\n",
        "\n",
        "            # UPDATE queries\n",
        "            (\"Write an UPDATE query\", \"Update user email\", \"UPDATE users SET email = 'new@email.com' WHERE id = 1;\"),\n",
        "            (\"Write an UPDATE query\", \"Update product price\", \"UPDATE products SET price = 29.99 WHERE name = 'Widget';\"),\n",
        "\n",
        "            # DELETE queries\n",
        "            (\"Write a DELETE query\", \"Delete inactive users\", \"DELETE FROM users WHERE status = 'inactive';\"),\n",
        "            (\"Write a DELETE query\", \"Delete old orders\", \"DELETE FROM orders WHERE date < '2023-01-01';\"),\n",
        "        ]\n",
        "\n",
        "        # Generate dataset by cycling through examples\n",
        "        dataset = []\n",
        "        for i in range(num_samples):\n",
        "            example = sql_examples[i % len(sql_examples)]\n",
        "            instruction, problem, solution = example\n",
        "\n",
        "            # Simple format that works well\n",
        "            text = f\"Instruction: {instruction}\\nInput: {problem}\\nOutput: {solution}\"\n",
        "            dataset.append({\"text\": text})\n",
        "\n",
        "        print(f\"✅ Created {len(dataset)} SQL examples\")\n",
        "        return Dataset.from_list(dataset)\n",
        "\n",
        "    def setup_lora_config(self):\n",
        "        \"\"\"Setup LoRA configuration\"\"\"\n",
        "        print(\"🔧 Setting up LoRA configuration...\")\n",
        "\n",
        "        lora_config = LoraConfig(\n",
        "            task_type=TaskType.CAUSAL_LM,\n",
        "            inference_mode=False,\n",
        "            r=4,                     # Very small rank for stability\n",
        "            lora_alpha=8,            # Alpha = 2 * r\n",
        "            lora_dropout=0.05,\n",
        "            target_modules=[\"c_attn\"],  # Only target attention for simplicity\n",
        "            bias=\"none\",\n",
        "            modules_to_save=None,\n",
        "        )\n",
        "\n",
        "        print(f\"✅ LoRA Config: r={lora_config.r}, alpha={lora_config.lora_alpha}\")\n",
        "        return lora_config\n",
        "\n",
        "    def apply_lora_to_model(self, model, lora_config):\n",
        "        \"\"\"Apply LoRA to model\"\"\"\n",
        "        print(\"🔄 Applying LoRA to model...\")\n",
        "\n",
        "        try:\n",
        "            peft_model = get_peft_model(model, lora_config)\n",
        "            peft_model.train()\n",
        "\n",
        "            # Count parameters\n",
        "            trainable_params = sum(p.numel() for p in peft_model.parameters() if p.requires_grad)\n",
        "            total_params = sum(p.numel() for p in peft_model.parameters())\n",
        "\n",
        "            print(f\"🧠 Total parameters: {total_params:,}\")\n",
        "            print(f\"🎯 Trainable parameters: {trainable_params:,}\")\n",
        "            print(f\"📊 Trainable percentage: {(trainable_params/total_params)*100:.2f}%\")\n",
        "\n",
        "            if trainable_params == 0:\n",
        "                raise RuntimeError(\"No trainable parameters found!\")\n",
        "\n",
        "            return peft_model\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"❌ Error applying LoRA: {e}\")\n",
        "            raise\n",
        "\n",
        "    def tokenize_dataset(self, dataset, tokenizer):\n",
        "        \"\"\"Tokenize dataset\"\"\"\n",
        "        print(\"🔤 Tokenizing dataset...\")\n",
        "\n",
        "        def tokenize_function(examples):\n",
        "            result = tokenizer(\n",
        "                examples[\"text\"],\n",
        "                truncation=True,\n",
        "                padding=True,\n",
        "                max_length=200,  # Short for memory efficiency\n",
        "                return_tensors=\"np\"\n",
        "            )\n",
        "\n",
        "            tokenized = {key: values.tolist() for key, values in result.items()}\n",
        "            tokenized[\"labels\"] = tokenized[\"input_ids\"].copy()\n",
        "            return tokenized\n",
        "\n",
        "        tokenized_dataset = dataset.map(\n",
        "            tokenize_function,\n",
        "            batched=True,\n",
        "            batch_size=50,\n",
        "            remove_columns=dataset.column_names,\n",
        "            desc=\"Tokenizing\"\n",
        "        )\n",
        "\n",
        "        # Filter short sequences\n",
        "        tokenized_dataset = tokenized_dataset.filter(lambda x: len(x[\"input_ids\"]) > 10)\n",
        "\n",
        "        print(f\"✅ Tokenized {len(tokenized_dataset)} examples\")\n",
        "        return tokenized_dataset\n",
        "\n",
        "    def create_training_arguments(self):\n",
        "        \"\"\"Create training arguments\"\"\"\n",
        "        return TrainingArguments(\n",
        "            output_dir=self.output_dir,\n",
        "            num_train_epochs=20,              # Single epoch for quick test\n",
        "            per_device_train_batch_size=1,   # Very small batch\n",
        "            gradient_accumulation_steps=4,\n",
        "            learning_rate=5e-4,              # Higher LR for LoRA\n",
        "            lr_scheduler_type=\"linear\",\n",
        "            warmup_steps=10,\n",
        "            weight_decay=0.01,\n",
        "            max_grad_norm=0.3,\n",
        "            fp16=True if self.device.type == 'cuda' else False,\n",
        "            gradient_checkpointing=False,\n",
        "            dataloader_drop_last=True,\n",
        "            dataloader_num_workers=0,\n",
        "            logging_steps=5,\n",
        "            save_steps=50,\n",
        "            save_total_limit=1,\n",
        "            eval_strategy=\"no\",\n",
        "            prediction_loss_only=True,\n",
        "            seed=42,\n",
        "            report_to=[],\n",
        "            remove_unused_columns=True,\n",
        "        )\n",
        "\n",
        "    def test_model_generation(self, model, tokenizer, stage=\"\"):\n",
        "        \"\"\"Test model generation\"\"\"\n",
        "        print(f\"🧪 Testing model generation {stage}...\")\n",
        "\n",
        "        test_prompt = \"Instruction: Write a SELECT query\\nInput: Get all users\\nOutput:\"\n",
        "        inputs = tokenizer(test_prompt, return_tensors=\"pt\").to(self.device)\n",
        "\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            generated = model.generate(\n",
        "                input_ids=inputs.input_ids,\n",
        "                max_new_tokens=20,\n",
        "                temperature=0.8,\n",
        "                do_sample=True,\n",
        "                pad_token_id=tokenizer.pad_token_id\n",
        "            )\n",
        "\n",
        "            new_tokens = generated[0][inputs.input_ids.shape[1]:]\n",
        "            generated_text = tokenizer.decode(new_tokens, skip_special_tokens=True)\n",
        "            print(f\"🎯 Generated {stage}: {generated_text.strip()}\")\n",
        "\n",
        "        model.train()\n",
        "\n",
        "    def run_lora_training(self):\n",
        "        \"\"\"Run complete LoRA training\"\"\"\n",
        "        try:\n",
        "            print(\"🚀 Starting Working LoRA SQL Training\")\n",
        "            print(\"=\" * 60)\n",
        "\n",
        "            # Setup components\n",
        "            model, tokenizer = self.download_and_setup_model()\n",
        "            dataset = self.create_sql_dataset(num_samples=100)  # Small dataset\n",
        "\n",
        "            # Apply LoRA\n",
        "            lora_config = self.setup_lora_config()\n",
        "            lora_model = self.apply_lora_to_model(model, lora_config)\n",
        "\n",
        "            # Test before training\n",
        "            self.test_model_generation(lora_model, tokenizer, \"before training\")\n",
        "\n",
        "            # Tokenize\n",
        "            tokenized_dataset = self.tokenize_dataset(dataset, tokenizer)\n",
        "\n",
        "            # Setup training\n",
        "            training_args = self.create_training_arguments()\n",
        "            data_collator = DataCollatorForLanguageModeling(\n",
        "                tokenizer=tokenizer,\n",
        "                mlm=False,\n",
        "                return_tensors=\"pt\"\n",
        "            )\n",
        "\n",
        "            # Create trainer WITHOUT callbacks first\n",
        "            trainer = Trainer(\n",
        "                model=lora_model,\n",
        "                args=training_args,\n",
        "                train_dataset=tokenized_dataset,\n",
        "                data_collator=data_collator,\n",
        "                processing_class=tokenizer,\n",
        "            )\n",
        "\n",
        "            # Add a proper callback class\n",
        "            class SimpleProgressCallback(TrainerCallback):\n",
        "                def on_log(self, args, state, control, logs=None, **kwargs):\n",
        "                    if logs and 'train_loss' in logs:\n",
        "                        step = state.global_step\n",
        "                        loss = logs['train_loss']\n",
        "                        print(f\"📊 Step {step}: Loss = {loss:.4f}\")\n",
        "\n",
        "                def on_train_begin(self, args, state, control, **kwargs):\n",
        "                    print(f\"🏁 Training started!\")\n",
        "\n",
        "                def on_train_end(self, args, state, control, **kwargs):\n",
        "                    print(f\"🏁 Training completed!\")\n",
        "\n",
        "            trainer.add_callback(SimpleProgressCallback())\n",
        "\n",
        "            # Start training\n",
        "            print(f\"\\n🏃 Starting LoRA training...\")\n",
        "            training_result = trainer.train()\n",
        "\n",
        "            print(f\"✅ Training completed!\")\n",
        "            print(f\"📊 Final loss: {training_result.training_loss:.4f}\")\n",
        "\n",
        "            # Test after training\n",
        "            self.test_model_generation(lora_model, tokenizer, \"after training\")\n",
        "\n",
        "            # Save LoRA adapters\n",
        "            adapter_path = f\"{self.output_dir}/lora_adapters\"\n",
        "            print(f\"\\n💾 Saving LoRA adapters to {adapter_path}\")\n",
        "            lora_model.save_pretrained(adapter_path)\n",
        "            tokenizer.save_pretrained(adapter_path)\n",
        "\n",
        "            # Save training info\n",
        "            training_info = {\n",
        "                \"timestamp\": self.timestamp,\n",
        "                \"base_model\": \"microsoft/DialoGPT-medium\",\n",
        "                \"dataset_size\": len(dataset),\n",
        "                \"lora_rank\": lora_config.r,\n",
        "                \"final_loss\": float(training_result.training_loss),\n",
        "                \"device\": str(self.device)\n",
        "            }\n",
        "\n",
        "            with open(f\"{self.output_dir}/training_info.json\", \"w\") as f:\n",
        "                json.dump(training_info, f, indent=2)\n",
        "\n",
        "            print(f\"📁 All outputs saved to: {self.output_dir}\")\n",
        "            print(\"🎉 LoRA training completed successfully!\")\n",
        "\n",
        "            return lora_model, tokenizer, adapter_path\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"❌ LoRA training failed: {e}\")\n",
        "            import traceback\n",
        "            traceback.print_exc()\n",
        "\n",
        "            # Cleanup\n",
        "            gc.collect()\n",
        "            if torch.cuda.is_available():\n",
        "                torch.cuda.empty_cache()\n",
        "            raise\n",
        "\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-08-28T11:57:13.126168Z",
          "iopub.execute_input": "2025-08-28T11:57:13.126471Z",
          "iopub.status.idle": "2025-08-28T11:57:13.176595Z",
          "shell.execute_reply.started": "2025-08-28T11:57:13.126448Z",
          "shell.execute_reply": "2025-08-28T11:57:13.176012Z"
        },
        "id": "u9pMiSf_tV9h"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "def demonstrate_trained_model(adapter_path):\n",
        "    \"\"\"Demonstrate the trained model\"\"\"\n",
        "    print(\"\\n🎯 Demonstrating Trained LoRA Model\")\n",
        "    print(\"=\" * 50)\n",
        "\n",
        "    try:\n",
        "        # Load model\n",
        "        tokenizer = AutoTokenizer.from_pretrained(adapter_path)\n",
        "        base_model = AutoModelForCausalLM.from_pretrained(\n",
        "            \"microsoft/DialoGPT-medium\",\n",
        "            torch_dtype=torch.float16,\n",
        "            device_map=\"auto\"\n",
        "        )\n",
        "        model = PeftModel.from_pretrained(base_model, adapter_path)\n",
        "\n",
        "        # Test prompts\n",
        "        test_prompts = [\n",
        "            \"Instruction: Write a SELECT query\\nInput: Get all customers\\nOutput:\",\n",
        "            \"Instruction: Write a COUNT query\\nInput: Count total orders\\nOutput:\",\n",
        "            \"Instruction: Write a JOIN query\\nInput: Join users and orders\\nOutput:\",\n",
        "        ]\n",
        "\n",
        "        model.eval()\n",
        "        for i, prompt in enumerate(test_prompts, 1):\n",
        "            print(f\"\\n🧪 Test {i}: {prompt.split('Input: ')[1].split('Output:')[0].strip()}\")\n",
        "\n",
        "            inputs = tokenizer(prompt, return_tensors=\"pt\").to(model.device)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                generated = model.generate(\n",
        "                    **inputs,\n",
        "                    max_new_tokens=30,\n",
        "                    temperature=0.7,\n",
        "                    do_sample=True,\n",
        "                    pad_token_id=tokenizer.pad_token_id\n",
        "                )\n",
        "\n",
        "                new_tokens = generated[0][inputs.input_ids.shape[1]:]\n",
        "                generated_sql = tokenizer.decode(new_tokens, skip_special_tokens=True)\n",
        "                print(f\"🎉 Generated: {generated_sql.strip()}\")\n",
        "\n",
        "        print(\"\\n✅ Model demonstration completed!\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"❌ Demo failed: {e}\")\n",
        "\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-08-28T11:56:30.888709Z",
          "iopub.execute_input": "2025-08-28T11:56:30.889465Z",
          "iopub.status.idle": "2025-08-28T11:56:30.895672Z",
          "shell.execute_reply.started": "2025-08-28T11:56:30.889437Z",
          "shell.execute_reply": "2025-08-28T11:56:30.894926Z"
        },
        "id": "MmVPJhAQtV9k"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# ===================================================================\n",
        "# Main Execution\n",
        "# ===================================================================\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Run LoRA training\n",
        "    trainer = WorkingLoRASQLTrainer()\n",
        "    lora_model, tokenizer, adapter_path = trainer.run_lora_training()\n",
        "\n",
        "    print(\"\\n\" + \"=\"*60)\n",
        "    print(\"🎉 LoRA Training Complete!\")\n",
        "    print(f\"📁 Adapters saved to: {adapter_path}\")\n",
        "\n",
        "    # Demonstrate the trained model\n",
        "    demonstrate_trained_model(adapter_path)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-08-28T11:57:19.631022Z",
          "iopub.execute_input": "2025-08-28T11:57:19.631725Z",
          "iopub.status.idle": "2025-08-28T11:59:37.078769Z",
          "shell.execute_reply.started": "2025-08-28T11:57:19.6317Z",
          "shell.execute_reply": "2025-08-28T11:59:37.077987Z"
        },
        "id": "HOIsnp_dtV9l",
        "outputId": "80effaa4-a0af-4ed6-86d5-271789e32640",
        "colab": {
          "referenced_widgets": [
            "6058e96f3453456c92a2d58b0cc6664d",
            "c8f97b0a366249af8756d77c23dd07b2"
          ]
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "🔧 Initializing Working LoRA SQL Training Pipeline\n🚀 GPU Available: Tesla P100-PCIE-16GB\n💾 GPU Memory: 17.1GB\n📁 Output directory: ./lora_sql_training_20250828_115719\n🚀 Starting Working LoRA SQL Training\n============================================================\n📥 Setting up model: microsoft/DialoGPT-medium\n📝 Tokenizer loaded, vocab size: 50257\n🤖 Model prepared for LoRA training\n🧠 Model parameters: 354,823,168\n📊 Creating SQL dataset with 100 samples...\n✅ Created 100 SQL examples\n🔧 Setting up LoRA configuration...\n✅ LoRA Config: r=4, alpha=8\n🔄 Applying LoRA to model...\n🧠 Total parameters: 355,216,384\n🎯 Trainable parameters: 393,216\n📊 Trainable percentage: 0.11%\n🧪 Testing model generation before training...\n🎯 Generated before training: Get all users therein\n🔤 Tokenizing dataset...\n",
          "output_type": "stream"
        },
        {
          "name": "stderr",
          "text": "/usr/local/lib/python3.11/dist-packages/peft/tuners/lora/layer.py:1768: UserWarning: fan_in_fan_out is set to False but the target module is `Conv1D`. Setting fan_in_fan_out to True.\n  warnings.warn(\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Tokenizing:   0%|          | 0/100 [00:00<?, ? examples/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6058e96f3453456c92a2d58b0cc6664d"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Filter:   0%|          | 0/100 [00:00<?, ? examples/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c8f97b0a366249af8756d77c23dd07b2"
            }
          },
          "metadata": {}
        },
        {
          "name": "stderr",
          "text": "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "✅ Tokenized 100 examples\n\n🏃 Starting LoRA training...\n🏁 Training started!\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "<IPython.core.display.HTML object>",
            "text/html": "\n    <div>\n      \n      <progress value='500' max='500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [500/500 02:09, Epoch 20/20]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>5</td>\n      <td>9.172000</td>\n    </tr>\n    <tr>\n      <td>10</td>\n      <td>8.864400</td>\n    </tr>\n    <tr>\n      <td>15</td>\n      <td>8.899900</td>\n    </tr>\n    <tr>\n      <td>20</td>\n      <td>8.446500</td>\n    </tr>\n    <tr>\n      <td>25</td>\n      <td>7.574200</td>\n    </tr>\n    <tr>\n      <td>30</td>\n      <td>6.453600</td>\n    </tr>\n    <tr>\n      <td>35</td>\n      <td>5.713700</td>\n    </tr>\n    <tr>\n      <td>40</td>\n      <td>5.336400</td>\n    </tr>\n    <tr>\n      <td>45</td>\n      <td>4.722900</td>\n    </tr>\n    <tr>\n      <td>50</td>\n      <td>4.222600</td>\n    </tr>\n    <tr>\n      <td>55</td>\n      <td>3.996400</td>\n    </tr>\n    <tr>\n      <td>60</td>\n      <td>3.755900</td>\n    </tr>\n    <tr>\n      <td>65</td>\n      <td>3.306800</td>\n    </tr>\n    <tr>\n      <td>70</td>\n      <td>3.158200</td>\n    </tr>\n    <tr>\n      <td>75</td>\n      <td>2.884700</td>\n    </tr>\n    <tr>\n      <td>80</td>\n      <td>2.626700</td>\n    </tr>\n    <tr>\n      <td>85</td>\n      <td>2.283200</td>\n    </tr>\n    <tr>\n      <td>90</td>\n      <td>2.277800</td>\n    </tr>\n    <tr>\n      <td>95</td>\n      <td>2.059200</td>\n    </tr>\n    <tr>\n      <td>100</td>\n      <td>2.031000</td>\n    </tr>\n    <tr>\n      <td>105</td>\n      <td>1.821400</td>\n    </tr>\n    <tr>\n      <td>110</td>\n      <td>1.768000</td>\n    </tr>\n    <tr>\n      <td>115</td>\n      <td>1.633700</td>\n    </tr>\n    <tr>\n      <td>120</td>\n      <td>1.654300</td>\n    </tr>\n    <tr>\n      <td>125</td>\n      <td>1.767400</td>\n    </tr>\n    <tr>\n      <td>130</td>\n      <td>1.584100</td>\n    </tr>\n    <tr>\n      <td>135</td>\n      <td>1.463900</td>\n    </tr>\n    <tr>\n      <td>140</td>\n      <td>1.465300</td>\n    </tr>\n    <tr>\n      <td>145</td>\n      <td>1.286200</td>\n    </tr>\n    <tr>\n      <td>150</td>\n      <td>1.345500</td>\n    </tr>\n    <tr>\n      <td>155</td>\n      <td>1.336200</td>\n    </tr>\n    <tr>\n      <td>160</td>\n      <td>1.355800</td>\n    </tr>\n    <tr>\n      <td>165</td>\n      <td>1.274500</td>\n    </tr>\n    <tr>\n      <td>170</td>\n      <td>1.186200</td>\n    </tr>\n    <tr>\n      <td>175</td>\n      <td>1.211300</td>\n    </tr>\n    <tr>\n      <td>180</td>\n      <td>1.162200</td>\n    </tr>\n    <tr>\n      <td>185</td>\n      <td>1.084200</td>\n    </tr>\n    <tr>\n      <td>190</td>\n      <td>1.171000</td>\n    </tr>\n    <tr>\n      <td>195</td>\n      <td>1.120100</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>1.061600</td>\n    </tr>\n    <tr>\n      <td>205</td>\n      <td>1.058000</td>\n    </tr>\n    <tr>\n      <td>210</td>\n      <td>1.095200</td>\n    </tr>\n    <tr>\n      <td>215</td>\n      <td>0.968700</td>\n    </tr>\n    <tr>\n      <td>220</td>\n      <td>1.015700</td>\n    </tr>\n    <tr>\n      <td>225</td>\n      <td>0.995600</td>\n    </tr>\n    <tr>\n      <td>230</td>\n      <td>0.935200</td>\n    </tr>\n    <tr>\n      <td>235</td>\n      <td>0.936100</td>\n    </tr>\n    <tr>\n      <td>240</td>\n      <td>0.982600</td>\n    </tr>\n    <tr>\n      <td>245</td>\n      <td>0.915000</td>\n    </tr>\n    <tr>\n      <td>250</td>\n      <td>0.889300</td>\n    </tr>\n    <tr>\n      <td>255</td>\n      <td>0.912600</td>\n    </tr>\n    <tr>\n      <td>260</td>\n      <td>0.874800</td>\n    </tr>\n    <tr>\n      <td>265</td>\n      <td>0.838400</td>\n    </tr>\n    <tr>\n      <td>270</td>\n      <td>0.909700</td>\n    </tr>\n    <tr>\n      <td>275</td>\n      <td>0.890500</td>\n    </tr>\n    <tr>\n      <td>280</td>\n      <td>0.852500</td>\n    </tr>\n    <tr>\n      <td>285</td>\n      <td>0.845300</td>\n    </tr>\n    <tr>\n      <td>290</td>\n      <td>0.854200</td>\n    </tr>\n    <tr>\n      <td>295</td>\n      <td>0.834500</td>\n    </tr>\n    <tr>\n      <td>300</td>\n      <td>0.787200</td>\n    </tr>\n    <tr>\n      <td>305</td>\n      <td>0.785700</td>\n    </tr>\n    <tr>\n      <td>310</td>\n      <td>0.771700</td>\n    </tr>\n    <tr>\n      <td>315</td>\n      <td>0.816100</td>\n    </tr>\n    <tr>\n      <td>320</td>\n      <td>0.823800</td>\n    </tr>\n    <tr>\n      <td>325</td>\n      <td>0.792600</td>\n    </tr>\n    <tr>\n      <td>330</td>\n      <td>0.760500</td>\n    </tr>\n    <tr>\n      <td>335</td>\n      <td>0.738200</td>\n    </tr>\n    <tr>\n      <td>340</td>\n      <td>0.795700</td>\n    </tr>\n    <tr>\n      <td>345</td>\n      <td>0.740400</td>\n    </tr>\n    <tr>\n      <td>350</td>\n      <td>0.770800</td>\n    </tr>\n    <tr>\n      <td>355</td>\n      <td>0.739000</td>\n    </tr>\n    <tr>\n      <td>360</td>\n      <td>0.692800</td>\n    </tr>\n    <tr>\n      <td>365</td>\n      <td>0.733800</td>\n    </tr>\n    <tr>\n      <td>370</td>\n      <td>0.741300</td>\n    </tr>\n    <tr>\n      <td>375</td>\n      <td>0.739800</td>\n    </tr>\n    <tr>\n      <td>380</td>\n      <td>0.669400</td>\n    </tr>\n    <tr>\n      <td>385</td>\n      <td>0.714000</td>\n    </tr>\n    <tr>\n      <td>390</td>\n      <td>0.728600</td>\n    </tr>\n    <tr>\n      <td>395</td>\n      <td>0.772500</td>\n    </tr>\n    <tr>\n      <td>400</td>\n      <td>0.736800</td>\n    </tr>\n    <tr>\n      <td>405</td>\n      <td>0.686400</td>\n    </tr>\n    <tr>\n      <td>410</td>\n      <td>0.687500</td>\n    </tr>\n    <tr>\n      <td>415</td>\n      <td>0.741400</td>\n    </tr>\n    <tr>\n      <td>420</td>\n      <td>0.705100</td>\n    </tr>\n    <tr>\n      <td>425</td>\n      <td>0.679300</td>\n    </tr>\n    <tr>\n      <td>430</td>\n      <td>0.669500</td>\n    </tr>\n    <tr>\n      <td>435</td>\n      <td>0.707900</td>\n    </tr>\n    <tr>\n      <td>440</td>\n      <td>0.652000</td>\n    </tr>\n    <tr>\n      <td>445</td>\n      <td>0.742700</td>\n    </tr>\n    <tr>\n      <td>450</td>\n      <td>0.665700</td>\n    </tr>\n    <tr>\n      <td>455</td>\n      <td>0.661700</td>\n    </tr>\n    <tr>\n      <td>460</td>\n      <td>0.702300</td>\n    </tr>\n    <tr>\n      <td>465</td>\n      <td>0.661500</td>\n    </tr>\n    <tr>\n      <td>470</td>\n      <td>0.686400</td>\n    </tr>\n    <tr>\n      <td>475</td>\n      <td>0.695000</td>\n    </tr>\n    <tr>\n      <td>480</td>\n      <td>0.672800</td>\n    </tr>\n    <tr>\n      <td>485</td>\n      <td>0.710600</td>\n    </tr>\n    <tr>\n      <td>490</td>\n      <td>0.724500</td>\n    </tr>\n    <tr>\n      <td>495</td>\n      <td>0.666000</td>\n    </tr>\n    <tr>\n      <td>500</td>\n      <td>0.660900</td>\n    </tr>\n  </tbody>\n</table><p>"
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "text": "📊 Step 500: Loss = 1.7277\n🏁 Training completed!\n✅ Training completed!\n📊 Final loss: 1.7277\n🧪 Testing model generation after training...\n🎯 Generated after training: SELECT * FROM users; name, email; email; FROM users; name, email; count (\n\n💾 Saving LoRA adapters to ./lora_sql_training_20250828_115719/lora_adapters\n📁 All outputs saved to: ./lora_sql_training_20250828_115719\n🎉 LoRA training completed successfully!\n\n============================================================\n🎉 LoRA Training Complete!\n📁 Adapters saved to: ./lora_sql_training_20250828_115719/lora_adapters\n\n🎯 Demonstrating Trained LoRA Model\n==================================================\n\n🧪 Test 1: Get all customers\n🎉 Generated: SELECT * FROM customers; name, email; email; id = 'name@email.com'; wait await_id; wait(); DELETE\n\n🧪 Test 2: Count total orders\n🎉 Generated: SELECT COUNT(*) FROM orders WHERE price < 19.99;) VALUES ('Widget', 19.99);'OUNT(*) FROM orders\n\n🧪 Test 3: Join users and orders\n🎉 Generated: SELECT u.name, o.total FROM users u JOIN orders o ON u.id = o.user_id; u.user_\n\n✅ Model demonstration completed!\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "UG5_uIHHtV9m"
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}