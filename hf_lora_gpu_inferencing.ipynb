{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ayagup/stablediffusion/blob/main/hf_lora_gpu_inferencing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torch transformers peft accelerate bitsandbytes"
      ],
      "metadata": {
        "collapsed": true,
        "id": "4dX7Nq6Qi3Gp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForCausalLM,\n",
        "    GenerationConfig,\n",
        "    BitsAndBytesConfig\n",
        ")\n",
        "from peft import PeftModel, PeftConfig, get_peft_model, LoraConfig\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "import time\n",
        "import gc\n",
        "\n",
        "# Check GPU availability\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU device: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
        "    device = torch.device(\"cuda\")\n",
        "else:\n",
        "    print(\"Using CPU\")\n",
        "    device = torch.device(\"cpu\")\n"
      ],
      "metadata": {
        "id": "7PGPmxM4i5iq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class LoRAInferenceEngine:\n",
        "    def __init__(self):\n",
        "        self.tokenizer = None\n",
        "        self.model = None\n",
        "        self.generation_config = None\n",
        "        self.base_model_name = None\n",
        "        self.lora_model_name = None\n",
        "\n",
        "    def download_and_load_models(self,\n",
        "                                base_model_name=\"microsoft/DialoGPT-medium\",\n",
        "                                lora_model_name=None,\n",
        "                                create_synthetic_lora=True,\n",
        "                                use_4bit=False):\n",
        "        \"\"\"\n",
        "        Download and load base model and LoRA adapter\n",
        "        \"\"\"\n",
        "        self.base_model_name = base_model_name\n",
        "        self.lora_model_name = lora_model_name\n",
        "\n",
        "        print(f\"\\n{'='*70}\")\n",
        "        print(f\"ðŸ“¥ LOADING MODELS\")\n",
        "        print(f\"Base model: {base_model_name}\")\n",
        "        print(f\"LoRA model: {lora_model_name if lora_model_name else 'Synthetic LoRA'}\")\n",
        "        print(f\"{'='*70}\")\n",
        "\n",
        "        # Load tokenizer\n",
        "        print(\"Loading tokenizer...\")\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n",
        "\n",
        "        # Set up tokenizer properly\n",
        "        if self.tokenizer.pad_token is None:\n",
        "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
        "\n",
        "        print(\"âœ… Tokenizer loaded successfully\")\n",
        "\n",
        "        # Configure quantization for memory efficiency (optional)\n",
        "        model_kwargs = {\n",
        "            \"torch_dtype\": torch.float16 if torch.cuda.is_available() else torch.float32,\n",
        "            \"device_map\": \"auto\" if torch.cuda.is_available() else None,\n",
        "            \"trust_remote_code\": True,\n",
        "        }\n",
        "\n",
        "        if use_4bit and torch.cuda.is_available():\n",
        "            print(\"Using 4-bit quantization for memory efficiency...\")\n",
        "            quantization_config = BitsAndBytesConfig(\n",
        "                load_in_4bit=True,\n",
        "                bnb_4bit_compute_dtype=torch.float16,\n",
        "                bnb_4bit_quant_type=\"nf4\",\n",
        "                bnb_4bit_use_double_quant=True,\n",
        "            )\n",
        "            model_kwargs[\"quantization_config\"] = quantization_config\n",
        "\n",
        "        # Load base model\n",
        "        print(\"Loading base model...\")\n",
        "        self.model = AutoModelForCausalLM.from_pretrained(\n",
        "            base_model_name,\n",
        "            **model_kwargs\n",
        "        )\n",
        "\n",
        "        print(f\"âœ… Base model loaded successfully\")\n",
        "        print(f\"Model parameters: {sum(p.numel() for p in self.model.parameters()):,}\")\n",
        "\n",
        "        # Load or create LoRA adapter\n",
        "        if lora_model_name and not create_synthetic_lora:\n",
        "            print(f\"Loading LoRA adapter: {lora_model_name}\")\n",
        "            self.load_real_lora_adapter(lora_model_name)\n",
        "        else:\n",
        "            print(\"Creating synthetic LoRA adapter...\")\n",
        "            self.create_synthetic_lora()\n",
        "\n",
        "        # Move to device if not using device_map\n",
        "        if not torch.cuda.is_available() or not use_4bit:\n",
        "            print(f\"Moving model to {device}...\")\n",
        "            self.model = self.model.to(device)\n",
        "\n",
        "        # Setup generation configuration\n",
        "        self.generation_config = GenerationConfig(\n",
        "            max_new_tokens=150,\n",
        "            do_sample=True,\n",
        "            temperature=0.7,\n",
        "            top_p=0.9,\n",
        "            top_k=50,\n",
        "            repetition_penalty=1.1,\n",
        "            pad_token_id=self.tokenizer.pad_token_id,\n",
        "            eos_token_id=self.tokenizer.eos_token_id,\n",
        "        )\n",
        "\n",
        "        print(\"âœ… Model setup completed!\")\n",
        "        self.print_gpu_memory()\n",
        "\n",
        "    def load_real_lora_adapter(self, lora_model_name):\n",
        "        \"\"\"\n",
        "        Load a real LoRA adapter from Hugging Face\n",
        "        \"\"\"\n",
        "        try:\n",
        "            # Load LoRA config\n",
        "            peft_config = PeftConfig.from_pretrained(lora_model_name)\n",
        "            print(f\"LoRA config: {peft_config}\")\n",
        "\n",
        "            # Load LoRA model\n",
        "            self.model = PeftModel.from_pretrained(\n",
        "                self.model,\n",
        "                lora_model_name,\n",
        "                torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32\n",
        "            )\n",
        "\n",
        "            print(f\"âœ… LoRA adapter loaded from {lora_model_name}\")\n",
        "\n",
        "            # Print LoRA info\n",
        "            if hasattr(self.model, 'peft_config'):\n",
        "                print(f\"LoRA rank: {self.model.peft_config.get('default', {}).get('r', 'N/A')}\")\n",
        "                print(f\"LoRA alpha: {self.model.peft_config.get('default', {}).get('lora_alpha', 'N/A')}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"âŒ Failed to load LoRA adapter: {e}\")\n",
        "            print(\"Creating synthetic LoRA instead...\")\n",
        "            self.create_synthetic_lora()\n",
        "\n",
        "    def create_synthetic_lora(self):\n",
        "        \"\"\"\n",
        "        Create a synthetic LoRA adapter for demonstration\n",
        "        \"\"\"\n",
        "        # Define LoRA configuration\n",
        "        lora_config = LoraConfig(\n",
        "            r=16,  # rank\n",
        "            lora_alpha=32,\n",
        "            target_modules=[\"c_attn\", \"c_proj\", \"c_fc\"],  # Common transformer modules\n",
        "            lora_dropout=0.1,\n",
        "            bias=\"none\",\n",
        "            task_type=\"CAUSAL_LM\",\n",
        "        )\n",
        "\n",
        "        # Add LoRA adapter to the model\n",
        "        self.model = get_peft_model(self.model, lora_config)\n",
        "\n",
        "        print(f\"âœ… Synthetic LoRA adapter created\")\n",
        "        print(f\"LoRA rank: {lora_config.r}\")\n",
        "        print(f\"LoRA alpha: {lora_config.lora_alpha}\")\n",
        "        print(f\"Target modules: {lora_config.target_modules}\")\n",
        "\n",
        "        trainable_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)\n",
        "        total_params = sum(p.numel() for p in self.model.parameters())\n",
        "        print(f\"Trainable parameters: {trainable_params:,}\")\n",
        "        print(f\"Total parameters: {total_params:,}\")\n",
        "        print(f\"Trainable %: {100 * trainable_params / total_params:.2f}%\")\n",
        "\n",
        "        # Simulate some training by slightly modifying LoRA weights\n",
        "        self.simulate_lora_training()\n",
        "\n",
        "    def simulate_lora_training(self):\n",
        "        \"\"\"\n",
        "        Simulate LoRA training by adding small random modifications\n",
        "        \"\"\"\n",
        "        print(\"Simulating LoRA fine-tuning...\")\n",
        "\n",
        "        with torch.no_grad():\n",
        "            modified_params = 0\n",
        "            for name, param in self.model.named_parameters():\n",
        "                if 'lora_' in name and param.requires_grad:\n",
        "                    # Add small random noise to simulate training\n",
        "                    noise = torch.randn_like(param) * 0.01\n",
        "                    param.add_(noise)\n",
        "                    modified_params += 1\n",
        "\n",
        "        print(f\"âœ… Simulated training on {modified_params} LoRA parameters\")\n",
        "\n",
        "    def merge_lora_weights(self):\n",
        "        \"\"\"\n",
        "        Merge LoRA weights into base model for faster inference\n",
        "        \"\"\"\n",
        "        if hasattr(self.model, 'merge_and_unload'):\n",
        "            print(\"Merging LoRA weights into base model...\")\n",
        "            self.model = self.model.merge_and_unload()\n",
        "            print(\"âœ… LoRA weights merged successfully\")\n",
        "            self.print_gpu_memory()\n",
        "        else:\n",
        "            print(\"Model doesn't support LoRA merging, continuing with adapter\")\n",
        "\n",
        "    def generate_response(self, prompt, max_new_tokens=100, temperature=0.7):\n",
        "        \"\"\"\n",
        "        Generate response for a given prompt\n",
        "        \"\"\"\n",
        "        # Prepare input\n",
        "        formatted_prompt = f\"Human: {prompt}\\nAssistant:\"\n",
        "\n",
        "        # Tokenize\n",
        "        inputs = self.tokenizer(\n",
        "            formatted_prompt,\n",
        "            return_tensors=\"pt\",\n",
        "            truncation=True,\n",
        "            max_length=512,\n",
        "            padding=False\n",
        "        )\n",
        "\n",
        "        # Move to device\n",
        "        input_ids = inputs['input_ids'].to(self.model.device)\n",
        "        attention_mask = inputs.get('attention_mask')\n",
        "        if attention_mask is not None:\n",
        "            attention_mask = attention_mask.to(self.model.device)\n",
        "\n",
        "        # Generate\n",
        "        start_time = time.time()\n",
        "        with torch.no_grad():\n",
        "            outputs = self.model.generate(\n",
        "                input_ids=input_ids,\n",
        "                attention_mask=attention_mask,\n",
        "                max_new_tokens=max_new_tokens,\n",
        "                do_sample=True,\n",
        "                temperature=temperature,\n",
        "                top_p=0.9,\n",
        "                top_k=50,\n",
        "                repetition_penalty=1.1,\n",
        "                pad_token_id=self.tokenizer.pad_token_id,\n",
        "                eos_token_id=self.tokenizer.eos_token_id,\n",
        "            )\n",
        "        generation_time = time.time() - start_time\n",
        "\n",
        "        # Decode response\n",
        "        full_text = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "        # Extract generated part\n",
        "        if \"Assistant:\" in full_text:\n",
        "            response = full_text.split(\"Assistant:\")[-1].strip()\n",
        "        else:\n",
        "            response = full_text[len(formatted_prompt):].strip()\n",
        "\n",
        "        return response, generation_time\n",
        "\n",
        "    def batch_inference(self, prompts, max_new_tokens=100, temperature=0.7):\n",
        "        \"\"\"\n",
        "        Perform inference on multiple prompts\n",
        "        \"\"\"\n",
        "        print(f\"\\n{'='*80}\")\n",
        "        print(f\"ðŸ”¥ PERFORMING INFERENCE ON {len(prompts)} PROMPTS\")\n",
        "        print(f\"Base Model: {self.base_model_name}\")\n",
        "        print(f\"LoRA Model: {self.lora_model_name or 'Synthetic'}\")\n",
        "        print(f\"Device: {self.model.device}\")\n",
        "        print(f\"Max new tokens: {max_new_tokens}\")\n",
        "        print(f\"Temperature: {temperature}\")\n",
        "        print(f\"{'='*80}\")\n",
        "\n",
        "        results = []\n",
        "        total_time = 0\n",
        "\n",
        "        for i, prompt in enumerate(prompts, 1):\n",
        "            print(f\"\\nðŸ“ PROMPT {i}/{len(prompts)}:\")\n",
        "            print(f\"{'â”€'*60}\")\n",
        "            print(f\"ðŸ’­ Input: {prompt}\")\n",
        "            print(f\"{'â”€'*60}\")\n",
        "\n",
        "            try:\n",
        "                response, gen_time = self.generate_response(\n",
        "                    prompt, max_new_tokens, temperature\n",
        "                )\n",
        "                total_time += gen_time\n",
        "\n",
        "                print(f\"ðŸ¤– Output: {response}\")\n",
        "                print(f\"â±ï¸  Generation time: {gen_time:.2f}s\")\n",
        "                print(f\"ðŸ“ Response length: {len(response.split())} words\")\n",
        "\n",
        "                results.append({\n",
        "                    'prompt': prompt,\n",
        "                    'response': response,\n",
        "                    'generation_time': gen_time,\n",
        "                    'success': True\n",
        "                })\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"âŒ Error: {e}\")\n",
        "                results.append({\n",
        "                    'prompt': prompt,\n",
        "                    'response': f\"Error: {e}\",\n",
        "                    'generation_time': 0,\n",
        "                    'success': False\n",
        "                })\n",
        "\n",
        "            print(f\"{'â”€'*60}\")\n",
        "\n",
        "            # Clean up GPU memory periodically\n",
        "            if torch.cuda.is_available() and i % 3 == 0:\n",
        "                torch.cuda.empty_cache()\n",
        "\n",
        "        # Print summary\n",
        "        successful_results = [r for r in results if r['success']]\n",
        "        print(f\"\\n{'='*80}\")\n",
        "        print(f\"ðŸ“Š INFERENCE SUMMARY\")\n",
        "        print(f\"{'='*80}\")\n",
        "        print(f\"Total prompts: {len(prompts)}\")\n",
        "        print(f\"Successful: {len(successful_results)}\")\n",
        "        print(f\"Failed: {len(prompts) - len(successful_results)}\")\n",
        "        if successful_results:\n",
        "            print(f\"Total time: {total_time:.2f}s\")\n",
        "            print(f\"Average time per prompt: {total_time/len(successful_results):.2f}s\")\n",
        "\n",
        "        self.print_gpu_memory()\n",
        "        print(f\"{'='*80}\")\n",
        "\n",
        "        return results\n",
        "\n",
        "    def print_gpu_memory(self):\n",
        "        \"\"\"Print GPU memory usage\"\"\"\n",
        "        if torch.cuda.is_available():\n",
        "            allocated = torch.cuda.memory_allocated() / 1024**3\n",
        "            reserved = torch.cuda.memory_reserved() / 1024**3\n",
        "            print(f\"ðŸ”§ GPU Memory - Allocated: {allocated:.1f}GB, Reserved: {reserved:.1f}GB\")\n"
      ],
      "metadata": {
        "id": "ITi0WmFGjEHS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def create_sample_prompts():\n",
        "    \"\"\"\n",
        "    Create diverse sample prompts for testing\n",
        "    \"\"\"\n",
        "    prompts = [\n",
        "        # Conversational prompts\n",
        "        \"Hello! How are you today?\",\n",
        "        \"What's your favorite book and why?\",\n",
        "        \"Can you tell me a joke?\",\n",
        "\n",
        "        # Creative writing\n",
        "        \"Write a short story about a time traveler\",\n",
        "        \"Describe a beautiful sunset over the ocean\",\n",
        "        \"What would happen if animals could talk?\",\n",
        "\n",
        "        # Question answering\n",
        "        \"Explain artificial intelligence in simple terms\",\n",
        "        \"What are the benefits of renewable energy?\",\n",
        "        \"How does the internet work?\",\n",
        "\n",
        "        # Problem solving\n",
        "        \"Give me 3 tips for staying productive\",\n",
        "        \"How can I learn a new language effectively?\",\n",
        "        \"What's the best way to reduce stress?\",\n",
        "\n",
        "        # Technical topics\n",
        "        \"Explain machine learning algorithms\",\n",
        "        \"What is cloud computing?\",\n",
        "        \"How do neural networks work?\",\n",
        "\n",
        "        # Fun and philosophical\n",
        "        \"If you could have any superpower, what would it be?\",\n",
        "        \"What's the meaning of life?\",\n",
        "        \"Describe your ideal vacation destination\"\n",
        "    ]\n",
        "\n",
        "    return prompts\n"
      ],
      "metadata": {
        "id": "CEPXWk3LjRir"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def test_different_configurations(engine):\n",
        "    \"\"\"\n",
        "    Test different generation configurations\n",
        "    \"\"\"\n",
        "    print(f\"\\n{'='*80}\")\n",
        "    print(f\"ðŸ§ª TESTING DIFFERENT CONFIGURATIONS\")\n",
        "    print(f\"{'='*80}\")\n",
        "\n",
        "    test_prompt = \"The future of technology is\"\n",
        "\n",
        "    configurations = [\n",
        "        {\"name\": \"Conservative\", \"temperature\": 0.3, \"max_tokens\": 50},\n",
        "        {\"name\": \"Balanced\", \"temperature\": 0.7, \"max_tokens\": 80},\n",
        "        {\"name\": \"Creative\", \"temperature\": 1.0, \"max_tokens\": 100},\n",
        "    ]\n",
        "\n",
        "    for config in configurations:\n",
        "        print(f\"\\nðŸ”§ {config['name']} Configuration:\")\n",
        "        print(f\"   Temperature: {config['temperature']}\")\n",
        "        print(f\"   Max tokens: {config['max_tokens']}\")\n",
        "        print(f\"{'â”€'*50}\")\n",
        "\n",
        "        response, gen_time = engine.generate_response(\n",
        "            test_prompt,\n",
        "            max_new_tokens=config['max_tokens'],\n",
        "            temperature=config['temperature']\n",
        "        )\n",
        "\n",
        "        print(f\"ðŸ’­ Prompt: {test_prompt}\")\n",
        "        print(f\"ðŸ¤– Response: {response}\")\n",
        "        print(f\"â±ï¸  Time: {gen_time:.2f}s\")\n"
      ],
      "metadata": {
        "id": "zlBEqCRPjX66"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def main():\n",
        "    \"\"\"\n",
        "    Main execution function\n",
        "    \"\"\"\n",
        "    try:\n",
        "        print(\"ðŸš€ Starting LoRA Inference Engine on GPU...\")\n",
        "\n",
        "        # Initialize inference engine\n",
        "        engine = LoRAInferenceEngine()\n",
        "\n",
        "        # You can try different model combinations:\n",
        "        model_options = [\n",
        "            {\n",
        "                \"base\": \"microsoft/DialoGPT-medium\",\n",
        "                \"lora\": None,  # Will create synthetic LoRA\n",
        "                \"use_4bit\": False\n",
        "            },\n",
        "            # Example with a real LoRA (uncomment if you have one):\n",
        "            # {\n",
        "            #     \"base\": \"microsoft/DialoGPT-medium\",\n",
        "            #     \"lora\": \"some-username/dialogpt-lora-adapter\",\n",
        "            #     \"use_4bit\": True\n",
        "            # }\n",
        "        ]\n",
        "\n",
        "        # Use the first option\n",
        "        config = model_options[0]\n",
        "\n",
        "        # Load models\n",
        "        engine.download_and_load_models(\n",
        "            base_model_name=config[\"base\"],\n",
        "            lora_model_name=config[\"lora\"],\n",
        "            create_synthetic_lora=True,\n",
        "            use_4bit=config[\"use_4bit\"]\n",
        "        )\n",
        "\n",
        "        # Optionally merge LoRA weights for faster inference\n",
        "        # engine.merge_lora_weights()\n",
        "\n",
        "        # Test different configurations\n",
        "        test_different_configurations(engine)\n",
        "\n",
        "        # Create sample prompts\n",
        "        sample_prompts = create_sample_prompts()\n",
        "\n",
        "        # Perform batch inference\n",
        "        results = engine.batch_inference(sample_prompts[:10])  # Use first 10 prompts\n",
        "\n",
        "        # Show best results\n",
        "        successful_results = [r for r in results if r['success']]\n",
        "        if successful_results:\n",
        "            print(f\"\\n{'='*80}\")\n",
        "            print(f\"ðŸ† TOP RESPONSES\")\n",
        "            print(f\"{'='*80}\")\n",
        "\n",
        "            # Sort by generation time and quality\n",
        "            sorted_results = sorted(successful_results,\n",
        "                                  key=lambda x: (x['generation_time'], -len(x['response'])))\n",
        "\n",
        "            for i, result in enumerate(sorted_results[:5], 1):\n",
        "                print(f\"\\n{i}. âš¡ Response ({result['generation_time']:.2f}s):\")\n",
        "                print(f\"   ðŸ’­ Prompt: {result['prompt']}\")\n",
        "                print(f\"   ðŸ¤– Response: {result['response'][:200]}...\")\n",
        "\n",
        "        print(f\"\\nâœ… LoRA Inference completed successfully!\")\n",
        "        return engine, results\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ Error: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        return None, None\n"
      ],
      "metadata": {
        "id": "2mHTqaTujcvj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def quick_gpu_test():\n",
        "    \"\"\"\n",
        "    Quick test to verify GPU functionality\n",
        "    \"\"\"\n",
        "    print(\"ðŸ”¬ Quick GPU Test\")\n",
        "\n",
        "    if not torch.cuda.is_available():\n",
        "        print(\"âŒ CUDA not available. Make sure you're using GPU runtime.\")\n",
        "        return False\n",
        "\n",
        "    try:\n",
        "        # Test basic GPU operations\n",
        "        x = torch.randn(1000, 1000).cuda()\n",
        "        y = torch.randn(1000, 1000).cuda()\n",
        "        z = torch.matmul(x, y)\n",
        "        print(f\"âœ… GPU computation test passed!\")\n",
        "        print(f\"Result shape: {z.shape}\")\n",
        "\n",
        "        # Clean up\n",
        "        del x, y, z\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "        return True\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ GPU test failed: {e}\")\n",
        "        return False\n"
      ],
      "metadata": {
        "id": "43NRk2B_jfI5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"ðŸŽ¯ LoRA Inference Engine on GPU\")\n",
        "    print(\"Make sure you're using GPU runtime in Colab!\")\n",
        "    print(\"Runtime -> Change runtime type -> GPU\")\n",
        "\n",
        "    # Quick GPU test first\n",
        "    if quick_gpu_test():\n",
        "        # Run main program\n",
        "        engine, results = main()\n",
        "    else:\n",
        "        print(\"Please switch to GPU runtime and try again.\")"
      ],
      "metadata": {
        "id": "bDnONWSJjftr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rW_Y_ZTLjgX7"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "Welcome To Colab",
      "toc_visible": true,
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}