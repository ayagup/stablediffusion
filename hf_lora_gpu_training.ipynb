{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ayagup/stablediffusion/blob/main/hf_lora_gpu_training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForCausalLM,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        "    DataCollatorForLanguageModeling\n",
        ")\n",
        "from peft import (\n",
        "    get_peft_model,\n",
        "    LoraConfig,\n",
        "    TaskType,\n",
        "    prepare_model_for_kbit_training\n",
        ")\n",
        "from datasets import Dataset\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "import time\n",
        "import json\n",
        "import gc\n",
        "import os\n",
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "# Check GPU availability\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU device: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"GPU memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")\n",
        "    device = torch.device(\"cuda\")\n",
        "else:\n",
        "    print(\"Using CPU\")\n",
        "    device = torch.device(\"cpu\")\n"
      ],
      "metadata": {
        "id": "K_SCrOqJocIx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class TensorBoardCallback:\n",
        "    \"\"\"Custom callback for TensorBoard logging\"\"\"\n",
        "\n",
        "    def __init__(self, log_dir=\"./logs\"):\n",
        "        self.log_dir = log_dir\n",
        "        self.writer = None\n",
        "\n",
        "    def setup(self, trainer):\n",
        "        \"\"\"Setup TensorBoard writer\"\"\"\n",
        "        if not os.path.exists(self.log_dir):\n",
        "            os.makedirs(self.log_dir)\n",
        "        self.writer = SummaryWriter(log_dir=self.log_dir)\n",
        "        print(f\"üìä TensorBoard logging to: {self.log_dir}\")\n",
        "\n",
        "    def on_log(self, trainer, logs=None):\n",
        "        \"\"\"Log metrics to TensorBoard\"\"\"\n",
        "        if self.writer and logs:\n",
        "            step = trainer.state.global_step\n",
        "\n",
        "            for key, value in logs.items():\n",
        "                if isinstance(value, (int, float)):\n",
        "                    self.writer.add_scalar(f\"train/{key}\", value, step)\n",
        "\n",
        "            # Log learning rate\n",
        "            if hasattr(trainer, 'lr_scheduler') and trainer.lr_scheduler is not None:\n",
        "                try:\n",
        "                    current_lr = trainer.lr_scheduler.get_last_lr()[0]\n",
        "                    self.writer.add_scalar(\"train/learning_rate\", current_lr, step)\n",
        "                except:\n",
        "                    pass\n",
        "\n",
        "            # Log GPU memory usage\n",
        "            if torch.cuda.is_available():\n",
        "                gpu_memory = torch.cuda.memory_allocated() / 1024**3\n",
        "                self.writer.add_scalar(\"system/gpu_memory_gb\", gpu_memory, step)\n",
        "\n",
        "    def on_evaluate(self, trainer, logs=None):\n",
        "        \"\"\"Log evaluation metrics\"\"\"\n",
        "        if self.writer and logs:\n",
        "            step = trainer.state.global_step\n",
        "\n",
        "            for key, value in logs.items():\n",
        "                if isinstance(value, (int, float)) and key.startswith('eval_'):\n",
        "                    clean_key = key.replace('eval_', '')\n",
        "                    self.writer.add_scalar(f\"eval/{clean_key}\", value, step)\n",
        "\n",
        "    def close(self):\n",
        "        \"\"\"Close TensorBoard writer\"\"\"\n",
        "        if self.writer:\n",
        "            self.writer.close()\n"
      ],
      "metadata": {
        "id": "Zx8Xb8Svor26"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class CustomTrainer(Trainer):\n",
        "    \"\"\"Custom Trainer with TensorBoard integration - FIXED\"\"\"\n",
        "\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        self.tb_callback = kwargs.pop('tb_callback', None)\n",
        "        super().__init__(*args, **kwargs)\n",
        "\n",
        "        if self.tb_callback:\n",
        "            self.tb_callback.setup(self)\n",
        "\n",
        "    def log(self, logs, start_time=None):\n",
        "        \"\"\"FIXED: Override log method with correct signature\"\"\"\n",
        "        # Call parent method with all arguments\n",
        "        if start_time is not None:\n",
        "            super().log(logs, start_time)\n",
        "        else:\n",
        "            super().log(logs)\n",
        "\n",
        "        # Add TensorBoard logging\n",
        "        if self.tb_callback:\n",
        "            self.tb_callback.on_log(self, logs)\n",
        "\n",
        "    def evaluate(self, *args, **kwargs):\n",
        "        \"\"\"Override evaluate to log to TensorBoard\"\"\"\n",
        "        result = super().evaluate(*args, **kwargs)\n",
        "\n",
        "        if self.tb_callback:\n",
        "            self.tb_callback.on_evaluate(self, result)\n",
        "\n",
        "        return result\n"
      ],
      "metadata": {
        "id": "IALVPCTVoti8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class SQLLoRATrainer:\n",
        "    def __init__(self, log_dir=\"./tensorboard_logs\"):\n",
        "        self.tokenizer = None\n",
        "        self.model = None\n",
        "        self.dataset = None\n",
        "        self.trained_model = None\n",
        "        self.tb_callback = TensorBoardCallback(log_dir)\n",
        "        self.log_dir = log_dir\n",
        "\n",
        "    def create_sql_dataset(self, num_samples=200):\n",
        "        \"\"\"\n",
        "        Create a synthetic SQL problems dataset with consistent formatting\n",
        "        \"\"\"\n",
        "        print(f\"\\n{'='*60}\")\n",
        "        print(f\"üóÉÔ∏è  CREATING SQL DATASET\")\n",
        "        print(f\"{'='*60}\")\n",
        "\n",
        "        # Define SQL problem templates with consistent structure\n",
        "        sql_problems = []\n",
        "\n",
        "        # Template 1: SELECT queries\n",
        "        select_templates = [\n",
        "            {\n",
        "                \"description\": \"Find all customers from a specific city\",\n",
        "                \"sql\": \"SELECT * FROM customers WHERE city = 'New York';\",\n",
        "                \"explanation\": \"This query selects all columns from the customers table where the city is New York.\"\n",
        "            },\n",
        "            {\n",
        "                \"description\": \"Get customer names and emails ordered by name\",\n",
        "                \"sql\": \"SELECT name, email FROM customers ORDER BY name ASC;\",\n",
        "                \"explanation\": \"This query retrieves customer names and emails, sorted alphabetically by name.\"\n",
        "            },\n",
        "            {\n",
        "                \"description\": \"Count total number of orders\",\n",
        "                \"sql\": \"SELECT COUNT(*) FROM orders;\",\n",
        "                \"explanation\": \"This query counts the total number of records in the orders table.\"\n",
        "            },\n",
        "            {\n",
        "                \"description\": \"Find products with price greater than 100\",\n",
        "                \"sql\": \"SELECT product_name, price FROM products WHERE price > 100;\",\n",
        "                \"explanation\": \"This query finds all products with a price greater than 100.\"\n",
        "            },\n",
        "            {\n",
        "                \"description\": \"Get unique customer cities\",\n",
        "                \"sql\": \"SELECT DISTINCT city FROM customers;\",\n",
        "                \"explanation\": \"This query returns unique city values from the customers table.\"\n",
        "            }\n",
        "        ]\n",
        "\n",
        "        # Template 2: JOIN queries\n",
        "        join_templates = [\n",
        "            {\n",
        "                \"description\": \"Get customer names with their order details\",\n",
        "                \"sql\": \"SELECT c.name, o.order_date, o.total FROM customers c JOIN orders o ON c.customer_id = o.customer_id;\",\n",
        "                \"explanation\": \"This query joins customers and orders tables to show customer names with their order information.\"\n",
        "            },\n",
        "            {\n",
        "                \"description\": \"Find products ordered by each customer\",\n",
        "                \"sql\": \"SELECT c.name, p.product_name FROM customers c JOIN orders o ON c.customer_id = o.customer_id JOIN order_items oi ON o.order_id = oi.order_id JOIN products p ON oi.product_id = p.product_id;\",\n",
        "                \"explanation\": \"This query uses multiple joins to connect customers with the products they ordered.\"\n",
        "            },\n",
        "        ]\n",
        "\n",
        "        # Template 3: Aggregate queries\n",
        "        aggregate_templates = [\n",
        "            {\n",
        "                \"description\": \"Calculate total sales by customer\",\n",
        "                \"sql\": \"SELECT customer_id, SUM(total) as total_sales FROM orders GROUP BY customer_id;\",\n",
        "                \"explanation\": \"This query calculates the total sales amount for each customer using GROUP BY and SUM.\"\n",
        "            },\n",
        "            {\n",
        "                \"description\": \"Find average product price by category\",\n",
        "                \"sql\": \"SELECT category, AVG(price) as avg_price FROM products GROUP BY category;\",\n",
        "                \"explanation\": \"This query calculates the average price of products in each category.\"\n",
        "            },\n",
        "        ]\n",
        "\n",
        "        # Combine all templates\n",
        "        all_templates = select_templates + join_templates + aggregate_templates\n",
        "\n",
        "        # Generate training data with consistent format\n",
        "        for i in range(num_samples):\n",
        "            template = np.random.choice(all_templates)\n",
        "\n",
        "            # Create training example with consistent structure\n",
        "            problem = {\n",
        "                \"instruction\": f\"Write a SQL query to: {template['description']}\",\n",
        "                \"output\": f\"```sql\\n{template['sql']}\\n```\\n\\nExplanation: {template['explanation']}\"\n",
        "            }\n",
        "\n",
        "            sql_problems.append(problem)\n",
        "\n",
        "        print(f\"‚úÖ Created {len(sql_problems)} SQL training examples\")\n",
        "        print(f\"Sample problem:\")\n",
        "        print(f\"  Instruction: {sql_problems[0]['instruction']}\")\n",
        "        print(f\"  Output: {sql_problems[0]['output'][:100]}...\")\n",
        "\n",
        "        return sql_problems\n",
        "\n",
        "    def format_training_data(self, sql_problems):\n",
        "        \"\"\"\n",
        "        Format data for training with consistent text structure\n",
        "        \"\"\"\n",
        "        print(\"\\nüîß Formatting training data...\")\n",
        "\n",
        "        formatted_data = []\n",
        "\n",
        "        for problem in sql_problems:\n",
        "            # Create instruction-following format with consistent structure\n",
        "            text = f\"### Instruction:\\n{problem['instruction']}\\n\\n### Response:\\n{problem['output']}<|endoftext|>\"\n",
        "\n",
        "            formatted_data.append({\"text\": text})\n",
        "\n",
        "        # Create dataset\n",
        "        dataset = Dataset.from_pandas(pd.DataFrame(formatted_data))\n",
        "\n",
        "        print(f\"‚úÖ Formatted {len(dataset)} training examples\")\n",
        "        print(f\"Sample formatted text (first 200 chars):\")\n",
        "        print(dataset[0]['text'][:200] + \"...\")\n",
        "\n",
        "        return dataset\n",
        "\n",
        "    def load_base_model(self, model_name=\"microsoft/DialoGPT-medium\"):\n",
        "        \"\"\"\n",
        "        Load base model and tokenizer from Hugging Face\n",
        "        \"\"\"\n",
        "        print(f\"\\n{'='*60}\")\n",
        "        print(f\"üì• LOADING BASE MODEL: {model_name}\")\n",
        "        print(f\"{'='*60}\")\n",
        "\n",
        "        # Load tokenizer\n",
        "        print(\"Loading tokenizer...\")\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "        # Add special tokens properly\n",
        "        if self.tokenizer.pad_token is None:\n",
        "            self.tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
        "\n",
        "        if self.tokenizer.eos_token is None:\n",
        "            self.tokenizer.add_special_tokens({'eos_token': '<|endoftext|>'})\n",
        "\n",
        "        # Add custom tokens for SQL training\n",
        "        special_tokens = {\n",
        "            \"additional_special_tokens\": [\"### Instruction:\", \"### Response:\", \"```sql\", \"```\"]\n",
        "        }\n",
        "        num_added = self.tokenizer.add_special_tokens(special_tokens)\n",
        "\n",
        "        print(f\"‚úÖ Tokenizer loaded. Vocab size: {len(self.tokenizer)}\")\n",
        "        print(f\"Added {num_added} special tokens\")\n",
        "        print(f\"Pad token: {self.tokenizer.pad_token} (ID: {self.tokenizer.pad_token_id})\")\n",
        "        print(f\"EOS token: {self.tokenizer.eos_token} (ID: {self.tokenizer.eos_token_id})\")\n",
        "\n",
        "        # Load model\n",
        "        print(\"Loading base model...\")\n",
        "        self.model = AutoModelForCausalLM.from_pretrained(\n",
        "            model_name,\n",
        "            torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
        "            device_map=\"auto\" if torch.cuda.is_available() else None,\n",
        "            trust_remote_code=True,\n",
        "        )\n",
        "\n",
        "        # Resize embeddings for new tokens\n",
        "        if num_added > 0:\n",
        "            self.model.resize_token_embeddings(len(self.tokenizer))\n",
        "            print(f\"Resized model embeddings to {len(self.tokenizer)}\")\n",
        "\n",
        "        print(f\"‚úÖ Base model loaded successfully\")\n",
        "        print(f\"Model parameters: {sum(p.numel() for p in self.model.parameters()):,}\")\n",
        "\n",
        "        return True\n",
        "\n",
        "    def setup_lora_config(self):\n",
        "        \"\"\"\n",
        "        Setup LoRA configuration for training\n",
        "        \"\"\"\n",
        "        print(f\"\\n{'='*60}\")\n",
        "        print(f\"‚öôÔ∏è  SETTING UP LORA CONFIGURATION\")\n",
        "        print(f\"{'='*60}\")\n",
        "\n",
        "        # Define LoRA config\n",
        "        lora_config = LoraConfig(\n",
        "            r=16,  # rank\n",
        "            lora_alpha=32,  # alpha parameter\n",
        "            target_modules=[\"c_attn\", \"c_proj\", \"c_fc\"],  # target modules for LoRA\n",
        "            lora_dropout=0.1,  # dropout probability for LoRA layers\n",
        "            bias=\"none\",  # bias type\n",
        "            task_type=TaskType.CAUSAL_LM,  # task type\n",
        "        )\n",
        "\n",
        "        # Prepare model for training\n",
        "        if torch.cuda.is_available():\n",
        "            self.model = prepare_model_for_kbit_training(self.model)\n",
        "\n",
        "        # Get PEFT model\n",
        "        self.model = get_peft_model(self.model, lora_config)\n",
        "\n",
        "        # Print trainable parameters\n",
        "        trainable_params = sum(p.numel() for p in self.model.parameters() if p.requires_grad)\n",
        "        total_params = sum(p.numel() for p in self.model.parameters())\n",
        "\n",
        "        print(f\"‚úÖ LoRA configuration applied\")\n",
        "        print(f\"LoRA rank (r): {lora_config.r}\")\n",
        "        print(f\"LoRA alpha: {lora_config.lora_alpha}\")\n",
        "        print(f\"Target modules: {lora_config.target_modules}\")\n",
        "        print(f\"Trainable parameters: {trainable_params:,}\")\n",
        "        print(f\"Total parameters: {total_params:,}\")\n",
        "        print(f\"Trainable %: {100 * trainable_params / total_params:.2f}%\")\n",
        "\n",
        "        return lora_config\n",
        "\n",
        "    def tokenize_dataset(self, dataset, max_length=256):\n",
        "        \"\"\"\n",
        "        Tokenize the dataset for training with proper padding and truncation\n",
        "        \"\"\"\n",
        "        print(f\"\\nüîß Tokenizing dataset with max_length={max_length}...\")\n",
        "\n",
        "        def tokenize_function(examples):\n",
        "            # Tokenize the text with proper padding and truncation\n",
        "            result = self.tokenizer(\n",
        "                examples[\"text\"],\n",
        "                truncation=True,\n",
        "                padding=\"max_length\",  # Pad to max_length for consistent tensor sizes\n",
        "                max_length=max_length,\n",
        "                return_overflowing_tokens=False,\n",
        "                return_attention_mask=True,\n",
        "            )\n",
        "\n",
        "            # For causal LM, labels are the same as input_ids\n",
        "            result[\"labels\"] = result[\"input_ids\"].copy()\n",
        "\n",
        "            return result\n",
        "\n",
        "        # Tokenize dataset\n",
        "        tokenized_dataset = dataset.map(\n",
        "            tokenize_function,\n",
        "            batched=True,\n",
        "            remove_columns=dataset.column_names,\n",
        "            desc=\"Tokenizing dataset\"\n",
        "        )\n",
        "\n",
        "        # Verify the tokenization\n",
        "        sample = tokenized_dataset[0]\n",
        "        print(f\"‚úÖ Dataset tokenized.\")\n",
        "        print(f\"Sample input_ids length: {len(sample['input_ids'])}\")\n",
        "        print(f\"Sample attention_mask length: {len(sample['attention_mask'])}\")\n",
        "        print(f\"Sample labels length: {len(sample['labels'])}\")\n",
        "        print(f\"All samples have max_length={max_length}: {all(len(item['input_ids']) == max_length for item in tokenized_dataset)}\")\n",
        "\n",
        "        return tokenized_dataset\n",
        "\n",
        "    def train_model(self, tokenized_dataset, output_dir=\"./sql-lora-model\"):\n",
        "        \"\"\"\n",
        "        Train the model with LoRA and TensorBoard logging\n",
        "        \"\"\"\n",
        "        print(f\"\\n{'='*60}\")\n",
        "        print(f\"üöÄ STARTING LORA TRAINING WITH TENSORBOARD\")\n",
        "        print(f\"{'='*60}\")\n",
        "\n",
        "        # Data collator\n",
        "        data_collator = DataCollatorForLanguageModeling(\n",
        "            tokenizer=self.tokenizer,\n",
        "            mlm=False,  # We're doing causal LM, not masked LM\n",
        "            pad_to_multiple_of=8,  # Pad to multiple of 8 for efficiency\n",
        "        )\n",
        "\n",
        "        # Training arguments\n",
        "        training_args = TrainingArguments(\n",
        "            output_dir=output_dir,\n",
        "            overwrite_output_dir=True,\n",
        "            num_train_epochs=20,  # Reduced for faster training\n",
        "            per_device_train_batch_size=2,  # Reduced batch size to avoid memory issues\n",
        "            gradient_accumulation_steps=8,  # Increased to maintain effective batch size\n",
        "            warmup_steps=50,\n",
        "            learning_rate=2e-4,\n",
        "            fp16=torch.cuda.is_available(),\n",
        "            logging_dir=self.log_dir,\n",
        "            logging_steps=10,\n",
        "            save_steps=200,\n",
        "            eval_steps=100,\n",
        "            do_eval=True,\n",
        "            save_total_limit=2,\n",
        "            prediction_loss_only=True,\n",
        "            remove_unused_columns=False,\n",
        "            dataloader_pin_memory=False,\n",
        "            report_to=\"tensorboard\",\n",
        "            run_name=f\"sql-lora-{int(time.time())}\",\n",
        "            dataloader_drop_last=True,  # Drop last incomplete batch\n",
        "        )\n",
        "\n",
        "        print(f\"üìä TensorBoard Configuration:\")\n",
        "        print(f\"  Logging directory: {self.log_dir}\")\n",
        "        print(f\"  Run name: {training_args.run_name}\")\n",
        "        print(f\"\\nTraining configuration:\")\n",
        "        print(f\"  Epochs: {training_args.num_train_epochs}\")\n",
        "        print(f\"  Batch size: {training_args.per_device_train_batch_size}\")\n",
        "        print(f\"  Gradient accumulation: {training_args.gradient_accumulation_steps}\")\n",
        "        print(f\"  Effective batch size: {training_args.per_device_train_batch_size * training_args.gradient_accumulation_steps}\")\n",
        "        print(f\"  Learning rate: {training_args.learning_rate}\")\n",
        "        print(f\"  FP16: {training_args.fp16}\")\n",
        "\n",
        "        # Split dataset for training and validation\n",
        "        train_test_split = tokenized_dataset.train_test_split(test_size=0.1, seed=42)\n",
        "        train_dataset = train_test_split[\"train\"]\n",
        "        eval_dataset = train_test_split[\"test\"]\n",
        "\n",
        "        print(f\"Training samples: {len(train_dataset)}\")\n",
        "        print(f\"Validation samples: {len(eval_dataset)}\")\n",
        "\n",
        "        # Initialize custom trainer - FIXED\n",
        "        trainer = CustomTrainer(\n",
        "            model=self.model,\n",
        "            args=training_args,\n",
        "            train_dataset=train_dataset,\n",
        "            eval_dataset=eval_dataset,\n",
        "            data_collator=data_collator,\n",
        "            tokenizer=self.tokenizer,\n",
        "            tb_callback=self.tb_callback,\n",
        "        )\n",
        "\n",
        "        # Print GPU memory before training\n",
        "        if torch.cuda.is_available():\n",
        "            initial_memory = torch.cuda.memory_allocated() / 1024**3\n",
        "            print(f\"GPU memory before training: {initial_memory:.1f}GB\")\n",
        "\n",
        "        # Start training\n",
        "        print(f\"\\nüî• Starting training...\")\n",
        "        print(f\"üåê To view TensorBoard, run in another cell:\")\n",
        "        print(f\"   %load_ext tensorboard\")\n",
        "        print(f\"   %tensorboard --logdir {self.log_dir}\")\n",
        "\n",
        "        start_time = time.time()\n",
        "\n",
        "        try:\n",
        "            trainer.train()\n",
        "            training_time = time.time() - start_time\n",
        "            print(f\"\\n‚úÖ Training completed in {training_time:.2f} seconds!\")\n",
        "\n",
        "            # Save the model\n",
        "            trainer.save_model()\n",
        "            self.tokenizer.save_pretrained(output_dir)\n",
        "            print(f\"‚úÖ Model saved to {output_dir}\")\n",
        "\n",
        "            # Store trained model\n",
        "            self.trained_model = self.model\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"‚ùå Training failed: {e}\")\n",
        "            import traceback\n",
        "            traceback.print_exc()\n",
        "            return None\n",
        "\n",
        "        # Print final GPU memory\n",
        "        if torch.cuda.is_available():\n",
        "            final_memory = torch.cuda.memory_allocated() / 1024**3\n",
        "            print(f\"GPU memory after training: {final_memory:.1f}GB\")\n",
        "\n",
        "        return trainer\n",
        "\n",
        "    def test_trained_model(self, test_prompts=None):\n",
        "        \"\"\"\n",
        "        Test the trained model with SQL prompts\n",
        "        \"\"\"\n",
        "        print(f\"\\n{'='*60}\")\n",
        "        print(f\"üß™ TESTING TRAINED MODEL\")\n",
        "        print(f\"{'='*60}\")\n",
        "\n",
        "        if test_prompts is None:\n",
        "            test_prompts = [\n",
        "                \"Write a SQL query to: Find all customers from California\",\n",
        "                \"Write a SQL query to: Calculate the total revenue by product category\",\n",
        "                \"Write a SQL query to: Get the top 10 best-selling products\",\n",
        "                \"Write a SQL query to: Find customers who haven't placed any orders\"\n",
        "            ]\n",
        "\n",
        "        if self.trained_model is None:\n",
        "            print(\"‚ùå No trained model available\")\n",
        "            return []\n",
        "\n",
        "        self.trained_model.eval()\n",
        "\n",
        "        results = []\n",
        "\n",
        "        for i, prompt in enumerate(test_prompts, 1):\n",
        "            print(f\"\\nüìù Test {i}/{len(test_prompts)}:\")\n",
        "            print(f\"{'‚îÄ'*50}\")\n",
        "            print(f\"üí≠ Prompt: {prompt}\")\n",
        "            print(f\"{'‚îÄ'*50}\")\n",
        "\n",
        "            try:\n",
        "                # Format prompt\n",
        "                formatted_prompt = f\"### Instruction:\\n{prompt}\\n\\n### Response:\\n\"\n",
        "\n",
        "                # Tokenize\n",
        "                inputs = self.tokenizer(\n",
        "                    formatted_prompt,\n",
        "                    return_tensors=\"pt\",\n",
        "                    truncation=True,\n",
        "                    max_length=128,\n",
        "                    padding=False\n",
        "                )\n",
        "\n",
        "                # Move to device\n",
        "                inputs = {k: v.to(self.trained_model.device) for k, v in inputs.items()}\n",
        "\n",
        "                # Generate\n",
        "                with torch.no_grad():\n",
        "                    outputs = self.trained_model.generate(\n",
        "                        **inputs,\n",
        "                        max_new_tokens=100,\n",
        "                        do_sample=True,\n",
        "                        temperature=0.7,\n",
        "                        top_p=0.9,\n",
        "                        pad_token_id=self.tokenizer.pad_token_id,\n",
        "                        eos_token_id=self.tokenizer.eos_token_id,\n",
        "                    )\n",
        "\n",
        "                # Decode\n",
        "                full_response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "                # Extract generated part\n",
        "                if \"### Response:\" in full_response:\n",
        "                    response = full_response.split(\"### Response:\")[-1].strip()\n",
        "                else:\n",
        "                    response = full_response[len(formatted_prompt):].strip()\n",
        "\n",
        "                print(f\"ü§ñ Generated SQL:\\n{response}\")\n",
        "\n",
        "                results.append({\n",
        "                    'prompt': prompt,\n",
        "                    'response': response,\n",
        "                    'success': True\n",
        "                })\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"‚ùå Error: {e}\")\n",
        "                results.append({\n",
        "                    'prompt': prompt,\n",
        "                    'response': f\"Error: {e}\",\n",
        "                    'success': False\n",
        "                })\n",
        "\n",
        "            print(f\"{'‚îÄ'*50}\")\n",
        "\n",
        "        # Summary\n",
        "        successful = sum(1 for r in results if r['success'])\n",
        "        print(f\"\\nüìä Testing Summary:\")\n",
        "        print(f\"Total tests: {len(test_prompts)}\")\n",
        "        print(f\"Successful: {successful}\")\n",
        "        print(f\"Failed: {len(test_prompts) - successful}\")\n",
        "\n",
        "        return results\n",
        "\n",
        "    def close_tensorboard(self):\n",
        "        \"\"\"Close TensorBoard writer\"\"\"\n",
        "        self.tb_callback.close()\n"
      ],
      "metadata": {
        "id": "Ck5zicwqovNJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def main():\n",
        "    \"\"\"\n",
        "    Main training pipeline with TensorBoard\n",
        "    \"\"\"\n",
        "    try:\n",
        "        print(\"üöÄ Starting SQL LoRA Training Pipeline with TensorBoard...\")\n",
        "\n",
        "        # Initialize trainer\n",
        "        trainer = SQLLoRATrainer(log_dir=\"./tensorboard_logs\")\n",
        "\n",
        "        # Create SQL dataset - smaller for testing\n",
        "        sql_problems = trainer.create_sql_dataset(num_samples=200)\n",
        "\n",
        "        # Format training data\n",
        "        dataset = trainer.format_training_data(sql_problems)\n",
        "\n",
        "        # Load base model\n",
        "        trainer.load_base_model(\"microsoft/DialoGPT-medium\")\n",
        "\n",
        "        # Setup LoRA\n",
        "        lora_config = trainer.setup_lora_config()\n",
        "\n",
        "        # Tokenize dataset with smaller max_length\n",
        "        tokenized_dataset = trainer.tokenize_dataset(dataset, max_length=256)\n",
        "\n",
        "        # Train model\n",
        "        training_result = trainer.train_model(tokenized_dataset)\n",
        "\n",
        "        if training_result is not None:\n",
        "            # Test trained model\n",
        "            test_results = trainer.test_trained_model()\n",
        "\n",
        "            print(f\"\\n‚úÖ SQL LoRA Training Pipeline Completed Successfully!\")\n",
        "        else:\n",
        "            print(f\"\\n‚ùå Training failed\")\n",
        "            test_results = []\n",
        "\n",
        "        # Close TensorBoard\n",
        "        trainer.close_tensorboard()\n",
        "\n",
        "        print(f\"\\nüìä TensorBoard Instructions:\")\n",
        "        print(f\"1. In a new cell, run: %load_ext tensorboard\")\n",
        "        print(f\"2. Then run: %tensorboard --logdir ./tensorboard_logs\")\n",
        "\n",
        "        return trainer, test_results\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error in training pipeline: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        return None, None\n"
      ],
      "metadata": {
        "id": "m_leCxkeo0T6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def quick_gpu_test():\n",
        "    \"\"\"\n",
        "    Quick test to verify GPU setup\n",
        "    \"\"\"\n",
        "    print(\"üî¨ Quick GPU Test\")\n",
        "\n",
        "    if not torch.cuda.is_available():\n",
        "        print(\"‚ùå CUDA not available. Please switch to GPU runtime.\")\n",
        "        print(\"Runtime -> Change runtime type -> GPU\")\n",
        "        return False\n",
        "\n",
        "    try:\n",
        "        # Test GPU operations\n",
        "        x = torch.randn(1000, 1000).cuda()\n",
        "        y = torch.randn(1000, 1000).cuda()\n",
        "        z = torch.matmul(x, y)\n",
        "\n",
        "        print(f\"‚úÖ GPU test passed!\")\n",
        "        print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "        print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f}GB\")\n",
        "\n",
        "        # Clean up\n",
        "        del x, y, z\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "        return True\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå GPU test failed: {e}\")\n",
        "        return False\n"
      ],
      "metadata": {
        "id": "VDHF-591o15b"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"üéØ SQL LoRA Training on GPU with TensorBoard (Fixed v3)\")\n",
        "    print(\"Make sure you're using GPU runtime in Colab!\")\n",
        "\n",
        "    # Quick GPU test\n",
        "    if quick_gpu_test():\n",
        "        # Run main training\n",
        "        trainer_obj, results = main()\n",
        "\n",
        "        if trainer_obj and results:\n",
        "            print(f\"\\nüéâ Training completed successfully!\")\n",
        "            print(f\"üìä Check TensorBoard for detailed metrics and visualizations.\")\n",
        "        else:\n",
        "            print(f\"\\n‚ùå Training failed. Check the logs above.\")\n",
        "    else:\n",
        "        print(\"Please switch to GPU runtime and try again.\")"
      ],
      "metadata": {
        "id": "r3IXm5o7o3Rb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext tensorboard"
      ],
      "metadata": {
        "id": "wmM-Vp14o3ec"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%tensorboard --logdir ./tensorboard_logs"
      ],
      "metadata": {
        "id": "9vrv9HMhqYm7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Wy2zE-uKqZ3q"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "Welcome To Colab",
      "toc_visible": true,
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}