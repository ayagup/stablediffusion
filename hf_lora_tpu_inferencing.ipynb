{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ayagup/stablediffusion/blob/main/hf_lora_tpu_inferencing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch_xla\n",
        "import torch_xla.core.xla_model as xm\n",
        "import torch_xla.runtime as xr\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForCausalLM,\n",
        "    GenerationConfig,\n",
        ")\n",
        "from peft import PeftModel, PeftConfig, get_peft_model, LoraConfig\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "import time\n",
        "\n",
        "# Check TPU availability\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"XLA version: {torch_xla.__version__}\")\n",
        "\n",
        "device = xm.xla_device()\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "try:\n",
        "    world_size = xr.world_size()\n",
        "    print(f\"Number of TPU cores: {world_size}\")\n",
        "except:\n",
        "    print(\"World size not available, but TPU is working\")\n"
      ],
      "metadata": {
        "id": "Id9maYHWgjcj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "class LoRAInferenceEngine:\n",
        "    def __init__(self):\n",
        "        self.tokenizer = None\n",
        "        self.model = None\n",
        "        self.generation_config = None\n",
        "\n",
        "    def setup_tokenizer_padding(self, tokenizer):\n",
        "        \"\"\"\n",
        "        Properly setup tokenizer padding to avoid attention mask issues\n",
        "        \"\"\"\n",
        "        # Check if pad token exists\n",
        "        if tokenizer.pad_token is None:\n",
        "            # Try to use different tokens for padding\n",
        "            if tokenizer.unk_token is not None:\n",
        "                tokenizer.pad_token = tokenizer.unk_token\n",
        "                print(f\"‚úì Using unk_token as pad_token: {tokenizer.pad_token}\")\n",
        "            elif tokenizer.bos_token is not None:\n",
        "                tokenizer.pad_token = tokenizer.bos_token\n",
        "                print(f\"‚úì Using bos_token as pad_token: {tokenizer.pad_token}\")\n",
        "            else:\n",
        "                # Add a new pad token\n",
        "                tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
        "                print(f\"‚úì Added new pad_token: {tokenizer.pad_token}\")\n",
        "        else:\n",
        "            print(f\"‚úì Using existing pad_token: {tokenizer.pad_token}\")\n",
        "\n",
        "        # Set padding side to left for generation tasks\n",
        "        tokenizer.padding_side = \"left\"\n",
        "\n",
        "        return tokenizer\n",
        "\n",
        "    def download_and_load_models(self,\n",
        "                                base_model_name=\"microsoft/DialoGPT-medium\",\n",
        "                                lora_model_name=None,\n",
        "                                create_synthetic_lora=True):\n",
        "        \"\"\"\n",
        "        Download and load base model and LoRA adapter with proper tokenizer setup\n",
        "        \"\"\"\n",
        "        print(f\"Loading base model: {base_model_name}\")\n",
        "\n",
        "        # Load tokenizer\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(base_model_name)\n",
        "\n",
        "        # Setup proper padding\n",
        "        self.tokenizer = self.setup_tokenizer_padding(self.tokenizer)\n",
        "\n",
        "        # Load base model\n",
        "        self.model = AutoModelForCausalLM.from_pretrained(\n",
        "            base_model_name,\n",
        "            torch_dtype=torch.float32,  # TPU works better with float32\n",
        "            device_map=None,  # We'll move to TPU manually\n",
        "            trust_remote_code=True\n",
        "        )\n",
        "\n",
        "        # Resize model embeddings if we added new tokens\n",
        "        self.model.resize_token_embeddings(len(self.tokenizer))\n",
        "\n",
        "        print(f\"‚úì Base model loaded successfully\")\n",
        "        print(f\"Model parameters: {sum(p.numel() for p in self.model.parameters()):,}\")\n",
        "        print(f\"Vocabulary size: {len(self.tokenizer)}\")\n",
        "\n",
        "        # Create or load LoRA adapter\n",
        "        if create_synthetic_lora:\n",
        "            print(\"Creating synthetic LoRA adapter...\")\n",
        "            self.create_synthetic_lora()\n",
        "        elif lora_model_name:\n",
        "            print(f\"Loading LoRA adapter: {lora_model_name}\")\n",
        "            self.load_lora_adapter(lora_model_name)\n",
        "\n",
        "        # Move model to TPU\n",
        "        print(\"Moving model to TPU...\")\n",
        "        self.model = self.model.to(device)\n",
        "        print(\"‚úì Model moved to TPU successfully\")\n",
        "\n",
        "        # Setup generation config with proper token IDs\n",
        "        self.generation_config = GenerationConfig(\n",
        "            max_new_tokens=100,\n",
        "            do_sample=True,\n",
        "            temperature=0.7,\n",
        "            top_p=0.9,\n",
        "            top_k=50,\n",
        "            repetition_penalty=1.1,\n",
        "            pad_token_id=self.tokenizer.pad_token_id,\n",
        "            eos_token_id=self.tokenizer.eos_token_id,\n",
        "            bos_token_id=self.tokenizer.bos_token_id if self.tokenizer.bos_token_id else self.tokenizer.eos_token_id,\n",
        "        )\n",
        "\n",
        "        print(\"‚úì Model setup completed!\")\n",
        "        print(f\"Pad token ID: {self.tokenizer.pad_token_id}\")\n",
        "        print(f\"EOS token ID: {self.tokenizer.eos_token_id}\")\n",
        "\n",
        "    def create_synthetic_lora(self):\n",
        "        \"\"\"\n",
        "        Create a synthetic LoRA adapter for demonstration\n",
        "        \"\"\"\n",
        "        # Define LoRA configuration\n",
        "        lora_config = LoraConfig(\n",
        "            r=16,  # rank\n",
        "            lora_alpha=32,\n",
        "            target_modules=[\"c_attn\", \"c_proj\"],  # DialoGPT specific modules\n",
        "            lora_dropout=0.1,\n",
        "            bias=\"none\",\n",
        "            task_type=\"CAUSAL_LM\",\n",
        "        )\n",
        "\n",
        "        # Add LoRA adapter to the model\n",
        "        self.model = get_peft_model(self.model, lora_config)\n",
        "\n",
        "        print(f\"‚úì Synthetic LoRA adapter created\")\n",
        "        print(f\"Trainable parameters: {sum(p.numel() for p in self.model.parameters() if p.requires_grad):,}\")\n",
        "\n",
        "        # Simulate some training by slightly modifying LoRA weights\n",
        "        self.simulate_lora_training()\n",
        "\n",
        "    def simulate_lora_training(self):\n",
        "        \"\"\"\n",
        "        Simulate LoRA training by adding small random modifications\n",
        "        \"\"\"\n",
        "        print(\"Simulating LoRA training...\")\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for name, param in self.model.named_parameters():\n",
        "                if 'lora_' in name and param.requires_grad:\n",
        "                    # Add small random noise to simulate training\n",
        "                    noise = torch.randn_like(param) * 0.01\n",
        "                    param.add_(noise)\n",
        "\n",
        "        print(\"‚úì LoRA simulation completed\")\n",
        "\n",
        "    def load_lora_adapter(self, lora_model_name):\n",
        "        \"\"\"\n",
        "        Load a real LoRA adapter from Hugging Face\n",
        "        \"\"\"\n",
        "        try:\n",
        "            # Load LoRA config\n",
        "            peft_config = PeftConfig.from_pretrained(lora_model_name)\n",
        "\n",
        "            # Load LoRA model\n",
        "            self.model = PeftModel.from_pretrained(\n",
        "                self.model,\n",
        "                lora_model_name,\n",
        "                torch_dtype=torch.float32\n",
        "            )\n",
        "\n",
        "            print(f\"‚úì LoRA adapter loaded from {lora_model_name}\")\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Failed to load LoRA adapter: {e}\")\n",
        "            print(\"Creating synthetic LoRA instead...\")\n",
        "            self.create_synthetic_lora()\n",
        "\n",
        "    def generate_response(self, prompt, add_context=True):\n",
        "        \"\"\"\n",
        "        Generate response for a given prompt with proper attention masks\n",
        "        \"\"\"\n",
        "        # Add conversational context if requested\n",
        "        if add_context:\n",
        "            formatted_prompt = f\"Human: {prompt} Assistant:\"\n",
        "        else:\n",
        "            formatted_prompt = prompt\n",
        "\n",
        "        # Tokenize input with proper attention mask\n",
        "        encoded = self.tokenizer(\n",
        "            formatted_prompt,\n",
        "            return_tensors=\"pt\",\n",
        "            truncation=True,\n",
        "            max_length=512,\n",
        "            padding=True,  # Enable padding\n",
        "            return_attention_mask=True  # Explicitly return attention mask\n",
        "        )\n",
        "\n",
        "        # Move to device\n",
        "        input_ids = encoded['input_ids'].to(device)\n",
        "        attention_mask = encoded['attention_mask'].to(device)\n",
        "\n",
        "        # Generate response\n",
        "        start_time = time.time()\n",
        "        with torch.no_grad():\n",
        "            outputs = self.model.generate(\n",
        "                input_ids=input_ids,\n",
        "                attention_mask=attention_mask,  # Pass attention mask\n",
        "                generation_config=self.generation_config,\n",
        "                pad_token_id=self.tokenizer.pad_token_id,\n",
        "            )\n",
        "        generation_time = time.time() - start_time\n",
        "\n",
        "        # Decode response\n",
        "        full_response = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "        # Extract only the new response\n",
        "        if add_context and \"Assistant:\" in full_response:\n",
        "            response = full_response.split(\"Assistant:\")[-1].strip()\n",
        "        else:\n",
        "            response = full_response[len(formatted_prompt):].strip()\n",
        "\n",
        "        return response, generation_time\n",
        "\n",
        "    def batch_inference(self, prompts, add_context=True):\n",
        "        \"\"\"\n",
        "        Perform inference on multiple prompts with proper batching\n",
        "        \"\"\"\n",
        "        print(f\"\\n{'='*80}\")\n",
        "        print(f\"üî• PERFORMING BATCH INFERENCE ON {len(prompts)} PROMPTS\")\n",
        "        print(f\"{'='*80}\")\n",
        "\n",
        "        results = []\n",
        "        total_time = 0\n",
        "\n",
        "        for i, prompt in enumerate(prompts, 1):\n",
        "            print(f\"\\nüìù PROMPT {i}/{len(prompts)}:\")\n",
        "            print(f\"{'‚îÄ'*60}\")\n",
        "            print(f\"üí≠ Input: {prompt}\")\n",
        "            print(f\"{'‚îÄ'*60}\")\n",
        "\n",
        "            try:\n",
        "                # Generate response\n",
        "                response, gen_time = self.generate_response(prompt, add_context)\n",
        "                total_time += gen_time\n",
        "\n",
        "                print(f\"ü§ñ Output: {response}\")\n",
        "                print(f\"‚è±Ô∏è  Generation time: {gen_time:.2f}s\")\n",
        "\n",
        "                results.append({\n",
        "                    'prompt': prompt,\n",
        "                    'response': response,\n",
        "                    'generation_time': gen_time,\n",
        "                    'status': 'success'\n",
        "                })\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"‚ùå Error generating response: {e}\")\n",
        "                results.append({\n",
        "                    'prompt': prompt,\n",
        "                    'response': f\"Error: {str(e)}\",\n",
        "                    'generation_time': 0,\n",
        "                    'status': 'error'\n",
        "                })\n",
        "\n",
        "            print(f\"{'‚îÄ'*60}\")\n",
        "\n",
        "        # Print summary\n",
        "        successful_results = [r for r in results if r['status'] == 'success']\n",
        "        print(f\"\\n{'='*80}\")\n",
        "        print(f\"üìä INFERENCE SUMMARY\")\n",
        "        print(f\"{'='*80}\")\n",
        "        print(f\"Total prompts processed: {len(prompts)}\")\n",
        "        print(f\"Successful generations: {len(successful_results)}\")\n",
        "        print(f\"Failed generations: {len(prompts) - len(successful_results)}\")\n",
        "        if successful_results:\n",
        "            print(f\"Total time: {total_time:.2f}s\")\n",
        "            print(f\"Average time per prompt: {total_time/len(successful_results):.2f}s\")\n",
        "        print(f\"{'='*80}\")\n",
        "\n",
        "        return results\n",
        "\n",
        "    def test_tokenizer_setup(self):\n",
        "        \"\"\"\n",
        "        Test tokenizer configuration to ensure proper attention mask handling\n",
        "        \"\"\"\n",
        "        print(f\"\\n{'='*60}\")\n",
        "        print(f\"üîß TESTING TOKENIZER CONFIGURATION\")\n",
        "        print(f\"{'='*60}\")\n",
        "\n",
        "        test_text = \"Hello, how are you?\"\n",
        "\n",
        "        # Test encoding\n",
        "        encoded = self.tokenizer(\n",
        "            test_text,\n",
        "            return_tensors=\"pt\",\n",
        "            padding=True,\n",
        "            return_attention_mask=True\n",
        "        )\n",
        "\n",
        "        print(f\"Test text: {test_text}\")\n",
        "        print(f\"Input IDs shape: {encoded['input_ids'].shape}\")\n",
        "        print(f\"Attention mask shape: {encoded['attention_mask'].shape}\")\n",
        "        print(f\"Input IDs: {encoded['input_ids']}\")\n",
        "        print(f\"Attention mask: {encoded['attention_mask']}\")\n",
        "        print(f\"Pad token: '{self.tokenizer.pad_token}' (ID: {self.tokenizer.pad_token_id})\")\n",
        "        print(f\"EOS token: '{self.tokenizer.eos_token}' (ID: {self.tokenizer.eos_token_id})\")\n",
        "\n",
        "        # Check if pad token is different from EOS token\n",
        "        if self.tokenizer.pad_token_id == self.tokenizer.eos_token_id:\n",
        "            print(\"‚ö†Ô∏è  WARNING: Pad token is same as EOS token!\")\n",
        "        else:\n",
        "            print(\"‚úÖ Pad token is different from EOS token\")\n",
        "\n",
        "        print(f\"{'='*60}\")\n"
      ],
      "metadata": {
        "id": "Srf85E_fgtsp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def create_sample_prompts():\n",
        "    \"\"\"\n",
        "    Create diverse sample prompts for testing\n",
        "    \"\"\"\n",
        "    prompts = [\n",
        "        # Short prompts\n",
        "        \"Hello!\",\n",
        "        \"How are you?\",\n",
        "        \"What's AI?\",\n",
        "\n",
        "        # Medium prompts\n",
        "        \"Tell me about machine learning\",\n",
        "        \"What's your favorite programming language?\",\n",
        "        \"Explain quantum computing simply\",\n",
        "\n",
        "        # Longer prompts\n",
        "        \"Can you write a short story about a robot who learns to paint?\",\n",
        "        \"What are the most important technological advances of the 21st century?\",\n",
        "        \"How do you think artificial intelligence will change education in the future?\",\n",
        "\n",
        "        # Diverse topics\n",
        "        \"What makes a good friend?\",\n",
        "        \"Describe the perfect vacation\",\n",
        "        \"What would you do with a million dollars?\",\n",
        "        \"How can we protect the environment?\",\n",
        "        \"What's the best way to learn a new skill?\",\n",
        "        \"Tell me about space exploration\"\n",
        "    ]\n",
        "\n",
        "    return prompts\n"
      ],
      "metadata": {
        "id": "1uhkz76EgyB8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def main():\n",
        "    \"\"\"\n",
        "    Main execution function with proper error handling\n",
        "    \"\"\"\n",
        "    try:\n",
        "        print(\"üöÄ Starting LoRA Inference Engine on TPU...\")\n",
        "\n",
        "        # Initialize inference engine\n",
        "        inference_engine = LoRAInferenceEngine()\n",
        "\n",
        "        # Download and load models\n",
        "        inference_engine.download_and_load_models(\n",
        "            base_model_name=\"microsoft/DialoGPT-medium\",\n",
        "            create_synthetic_lora=True\n",
        "        )\n",
        "\n",
        "        # Test tokenizer configuration\n",
        "        inference_engine.test_tokenizer_setup()\n",
        "\n",
        "        # Create sample prompts\n",
        "        sample_prompts = create_sample_prompts()\n",
        "\n",
        "        # Perform batch inference on subset of prompts\n",
        "        results = inference_engine.batch_inference(sample_prompts[:8])  # Use first 8 prompts\n",
        "\n",
        "        # Show successful results\n",
        "        successful_results = [r for r in results if r['status'] == 'success']\n",
        "        if successful_results:\n",
        "            print(f\"\\n{'='*80}\")\n",
        "            print(f\"üèÜ SUCCESSFUL GENERATIONS\")\n",
        "            print(f\"{'='*80}\")\n",
        "\n",
        "            # Sort by generation time\n",
        "            sorted_results = sorted(successful_results, key=lambda x: x['generation_time'])\n",
        "            for i, result in enumerate(sorted_results[:5], 1):\n",
        "                print(f\"\\n{i}. ‚ö° Response ({result['generation_time']:.2f}s):\")\n",
        "                print(f\"   üí≠ Prompt: {result['prompt']}\")\n",
        "                print(f\"   ü§ñ Response: {result['response'][:100]}...\")  # Truncate long responses\n",
        "\n",
        "        print(f\"\\n‚úÖ LoRA Inference completed successfully!\")\n",
        "        return inference_engine, results\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        return None, None\n"
      ],
      "metadata": {
        "id": "rLACPn1Gg0LS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def quick_test():\n",
        "    \"\"\"\n",
        "    Quick test with minimal setup and better error handling\n",
        "    \"\"\"\n",
        "    print(\"üî¨ Quick LoRA Inference Test\")\n",
        "\n",
        "    try:\n",
        "        inference_engine = LoRAInferenceEngine()\n",
        "        inference_engine.download_and_load_models(\n",
        "            base_model_name=\"microsoft/DialoGPT-small\",  # Smaller model for quick test\n",
        "            create_synthetic_lora=True\n",
        "        )\n",
        "\n",
        "        # Test tokenizer\n",
        "        inference_engine.test_tokenizer_setup()\n",
        "\n",
        "        # Test with a few simple prompts\n",
        "        test_prompts = [\n",
        "            \"Hi there!\",\n",
        "            \"How's it going?\",\n",
        "            \"Tell me something interesting\"\n",
        "        ]\n",
        "\n",
        "        results = inference_engine.batch_inference(test_prompts)\n",
        "        print(\"‚úÖ Quick test completed!\")\n",
        "        return results\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Quick test failed: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n",
        "        return None\n"
      ],
      "metadata": {
        "id": "EMaJU05tg2Px"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "if __name__ == \"__main__\":\n",
        "    print(\"üéØ LoRA Inference Engine on TPU (Fixed Version)\")\n",
        "    print(\"Choose an option:\")\n",
        "    print(\"1. Full inference demo\")\n",
        "    print(\"2. Quick test\")\n",
        "\n",
        "    # Run quick test by default for safety\n",
        "    choice = \"2\"\n",
        "\n",
        "    if choice == \"1\":\n",
        "        engine, results = main()\n",
        "    elif choice == \"2\":\n",
        "        results = quick_test()\n",
        "    else:\n",
        "        print(\"Running quick test by default...\")\n",
        "        results = quick_test()"
      ],
      "metadata": {
        "id": "ZxEz1YPMg4Va"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "Welcome To Colab",
      "toc_visible": true,
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}