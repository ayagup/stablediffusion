{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ayagup/stablediffusion/blob/main/hf_tpu_test.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch_xla\n",
        "import torch_xla.core.xla_model as xm\n",
        "import torch_xla.distributed.parallel_loader as pl\n",
        "import torch_xla.runtime as xr\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForSequenceClassification,\n",
        "    AutoConfig,\n",
        "    get_linear_schedule_with_warmup\n",
        ")\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score\n",
        "import time\n",
        "\n",
        "# Check TPU availability\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"XLA version: {torch_xla.__version__}\")\n",
        "\n",
        "device = xm.xla_device()\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "try:\n",
        "    world_size = xr.world_size()\n",
        "    print(f\"Number of TPU cores: {world_size}\")\n",
        "except:\n",
        "    print(\"World size not available, but TPU is working\")\n"
      ],
      "metadata": {
        "id": "2S2Se2DKYtsy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Custom dataset for text classification\n",
        "class TextDataset(Dataset):\n",
        "    def __init__(self, texts, labels, tokenizer, max_length=128):\n",
        "        self.texts = texts\n",
        "        self.labels = labels\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.texts)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        text = str(self.texts[idx])\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        encoding = self.tokenizer(\n",
        "            text,\n",
        "            truncation=True,\n",
        "            padding='max_length',\n",
        "            max_length=self.max_length,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "\n",
        "        return {\n",
        "            'input_ids': encoding['input_ids'].flatten(),\n",
        "            'attention_mask': encoding['attention_mask'].flatten(),\n",
        "            'label': torch.tensor(label, dtype=torch.long)\n",
        "        }\n"
      ],
      "metadata": {
        "id": "obg1F2BwY0Pq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Generate synthetic text data for training\n",
        "def create_synthetic_text_data(num_samples=1000):\n",
        "    \"\"\"Create synthetic text classification data\"\"\"\n",
        "\n",
        "    # Simple sentiment-like data\n",
        "    positive_words = ['good', 'great', 'excellent', 'amazing', 'wonderful', 'fantastic', 'love', 'perfect']\n",
        "    negative_words = ['bad', 'terrible', 'awful', 'horrible', 'hate', 'worst', 'disappointing', 'poor']\n",
        "    neutral_words = ['okay', 'fine', 'average', 'normal', 'standard', 'typical', 'regular', 'common']\n",
        "\n",
        "    texts = []\n",
        "    labels = []\n",
        "\n",
        "    for _ in range(num_samples):\n",
        "        # Generate random sentences\n",
        "        label = np.random.choice([0, 1, 2])  # 0: negative, 1: neutral, 2: positive\n",
        "\n",
        "        if label == 0:  # negative\n",
        "            words = np.random.choice(negative_words, size=np.random.randint(3, 8))\n",
        "            text = f\"This is {' '.join(words)} and not recommended.\"\n",
        "        elif label == 1:  # neutral\n",
        "            words = np.random.choice(neutral_words, size=np.random.randint(3, 8))\n",
        "            text = f\"This seems {' '.join(words)} to me.\"\n",
        "        else:  # positive\n",
        "            words = np.random.choice(positive_words, size=np.random.randint(3, 8))\n",
        "            text = f\"This is {' '.join(words)} and highly recommended!\"\n",
        "\n",
        "        texts.append(text)\n",
        "        labels.append(label)\n",
        "\n",
        "    return texts, labels\n"
      ],
      "metadata": {
        "id": "CsuWD3hgY7-S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Download and setup model\n",
        "def setup_model_and_tokenizer(model_name=\"distilbert-base-uncased\", num_labels=3):\n",
        "    \"\"\"Download and setup model and tokenizer from Hugging Face\"\"\"\n",
        "\n",
        "    print(f\"Downloading model: {model_name}\")\n",
        "\n",
        "    # Download tokenizer\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "    # Download model configuration\n",
        "    config = AutoConfig.from_pretrained(model_name)\n",
        "    config.num_labels = num_labels\n",
        "\n",
        "    # Download model\n",
        "    model = AutoModelForSequenceClassification.from_pretrained(\n",
        "        model_name,\n",
        "        config=config\n",
        "    )\n",
        "\n",
        "    print(f\"✓ Model and tokenizer downloaded successfully\")\n",
        "    print(f\"Model config: {config}\")\n",
        "\n",
        "    return model, tokenizer, config\n"
      ],
      "metadata": {
        "id": "0aXupUS-Y-jq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Training function\n",
        "def train_model():\n",
        "    \"\"\"Train the Hugging Face model on TPU\"\"\"\n",
        "\n",
        "    # Setup model and tokenizer\n",
        "    model, tokenizer, config = setup_model_and_tokenizer(\n",
        "        model_name=\"distilbert-base-uncased\",\n",
        "        num_labels=3\n",
        "    )\n",
        "\n",
        "    # Move model to TPU\n",
        "    model = model.to(device)\n",
        "    print(\"✓ Model moved to TPU\")\n",
        "\n",
        "    # Create synthetic dataset\n",
        "    print(\"Creating synthetic dataset...\")\n",
        "    train_texts, train_labels = create_synthetic_text_data(800)\n",
        "    val_texts, val_labels = create_synthetic_text_data(200)\n",
        "\n",
        "    print(f\"Training samples: {len(train_texts)}\")\n",
        "    print(f\"Validation samples: {len(val_texts)}\")\n",
        "    print(f\"Sample text: {train_texts[0]}\")\n",
        "    print(f\"Sample label: {train_labels[0]}\")\n",
        "\n",
        "    # Create datasets\n",
        "    train_dataset = TextDataset(train_texts, train_labels, tokenizer)\n",
        "    val_dataset = TextDataset(val_texts, val_labels, tokenizer)\n",
        "\n",
        "    # Create data loaders\n",
        "    batch_size = 8\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    # Create TPU-optimized data loaders\n",
        "    train_loader = pl.ParallelLoader(train_loader, [device])\n",
        "    val_loader = pl.ParallelLoader(val_loader, [device])\n",
        "\n",
        "    # Setup optimizer and scheduler\n",
        "    num_epochs = 3\n",
        "    num_training_steps = len(train_loader.per_device_loader(device)) * num_epochs\n",
        "\n",
        "    optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n",
        "    scheduler = get_linear_schedule_with_warmup(\n",
        "        optimizer,\n",
        "        num_warmup_steps=0,\n",
        "        num_training_steps=num_training_steps\n",
        "    )\n",
        "\n",
        "    print(\"Starting training...\")\n",
        "\n",
        "    # Training loop\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        total_loss = 0\n",
        "        correct_predictions = 0\n",
        "        total_predictions = 0\n",
        "\n",
        "        print(f\"\\nEpoch {epoch + 1}/{num_epochs}\")\n",
        "\n",
        "        for batch_idx, batch in enumerate(train_loader.per_device_loader(device)):\n",
        "            # Move batch to device\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels = batch['label'].to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(\n",
        "                input_ids=input_ids,\n",
        "                attention_mask=attention_mask,\n",
        "                labels=labels\n",
        "            )\n",
        "\n",
        "            loss = outputs.loss\n",
        "            logits = outputs.logits\n",
        "\n",
        "            # Backward pass\n",
        "            loss.backward()\n",
        "            xm.optimizer_step(optimizer)  # TPU-optimized step\n",
        "            scheduler.step()\n",
        "\n",
        "            # Calculate accuracy\n",
        "            predictions = torch.argmax(logits, dim=-1)\n",
        "            correct_predictions += (predictions == labels).sum().item()\n",
        "            total_predictions += labels.size(0)\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            if batch_idx % 20 == 0:\n",
        "                current_acc = correct_predictions / total_predictions if total_predictions > 0 else 0\n",
        "                print(f\"Batch {batch_idx}, Loss: {loss.item():.4f}, Accuracy: {current_acc:.4f}\")\n",
        "\n",
        "        # Epoch summary\n",
        "        avg_loss = total_loss / len(train_loader.per_device_loader(device))\n",
        "        accuracy = correct_predictions / total_predictions if total_predictions > 0 else 0\n",
        "        print(f\"Epoch {epoch + 1} - Avg Loss: {avg_loss:.4f}, Accuracy: {accuracy:.4f}\")\n",
        "\n",
        "        # Validation\n",
        "        if (epoch + 1) % 1 == 0:\n",
        "            val_accuracy = evaluate_model(model, val_loader, device)\n",
        "            print(f\"Validation Accuracy: {val_accuracy:.4f}\")\n",
        "\n",
        "    print(\"✓ Training completed!\")\n",
        "    return model, tokenizer\n"
      ],
      "metadata": {
        "id": "Kt8fDllFZA2r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Evaluation function\n",
        "def evaluate_model(model, val_loader, device):\n",
        "    \"\"\"Evaluate model on validation set\"\"\"\n",
        "    model.eval()\n",
        "    all_predictions = []\n",
        "    all_labels = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for batch in val_loader.per_device_loader(device):\n",
        "            input_ids = batch['input_ids'].to(device)\n",
        "            attention_mask = batch['attention_mask'].to(device)\n",
        "            labels = batch['label'].to(device)\n",
        "\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "            predictions = torch.argmax(outputs.logits, dim=-1)\n",
        "\n",
        "            all_predictions.extend(predictions.cpu().numpy())\n",
        "            all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "    accuracy = accuracy_score(all_labels, all_predictions)\n",
        "    return accuracy\n"
      ],
      "metadata": {
        "id": "2kSF3m5pZDur"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Inference function\n",
        "def perform_inference(model, tokenizer, texts):\n",
        "    \"\"\"Perform inference on new texts\"\"\"\n",
        "    model.eval()\n",
        "\n",
        "    label_map = {0: \"Negative\", 1: \"Neutral\", 2: \"Positive\"}\n",
        "    results = []\n",
        "\n",
        "    print(\"\\n=== Performing Inference ===\")\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for text in texts:\n",
        "            # Tokenize input\n",
        "            encoding = tokenizer(\n",
        "                text,\n",
        "                truncation=True,\n",
        "                padding='max_length',\n",
        "                max_length=128,\n",
        "                return_tensors='pt'\n",
        "            )\n",
        "\n",
        "            # Move to device\n",
        "            input_ids = encoding['input_ids'].to(device)\n",
        "            attention_mask = encoding['attention_mask'].to(device)\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(input_ids=input_ids, attention_mask=attention_mask)\n",
        "            predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
        "            predicted_class = torch.argmax(predictions, dim=-1).item()\n",
        "            confidence = predictions[0][predicted_class].item()\n",
        "\n",
        "            result = {\n",
        "                'text': text,\n",
        "                'predicted_label': label_map[predicted_class],\n",
        "                'confidence': confidence,\n",
        "                'all_scores': predictions.cpu().numpy()[0]\n",
        "            }\n",
        "            results.append(result)\n",
        "\n",
        "            print(f\"Text: '{text}'\")\n",
        "            print(f\"Prediction: {label_map[predicted_class]} (confidence: {confidence:.4f})\")\n",
        "            print(\"-\" * 50)\n",
        "\n",
        "    return results\n"
      ],
      "metadata": {
        "id": "znvAhpadZFT6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Main execution function\n",
        "def main():\n",
        "    try:\n",
        "        print(\"=== Hugging Face Model Training on TPU ===\")\n",
        "\n",
        "        # Train the model\n",
        "        trained_model, tokenizer = train_model()\n",
        "\n",
        "        # Test inference with sample texts\n",
        "        test_texts = [\n",
        "            \"This movie is absolutely wonderful and amazing!\",\n",
        "            \"The product is terrible and disappointing.\",\n",
        "            \"It's an okay product, nothing special.\",\n",
        "            \"I love this book, it's fantastic!\",\n",
        "            \"The service was bad and horrible.\",\n",
        "            \"This is a normal and average experience.\"\n",
        "        ]\n",
        "\n",
        "        # Perform inference\n",
        "        inference_results = perform_inference(trained_model, tokenizer, test_texts)\n",
        "\n",
        "        print(\"\\n=== Inference Results Summary ===\")\n",
        "        for i, result in enumerate(inference_results):\n",
        "            print(f\"{i+1}. '{result['text']}' -> {result['predicted_label']} ({result['confidence']:.3f})\")\n",
        "\n",
        "        print(\"\\n✓ Hugging Face PyTorch TPU example completed successfully!\")\n",
        "\n",
        "        return trained_model, tokenizer, inference_results\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n"
      ],
      "metadata": {
        "id": "KR4AYlpxZHAr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Test basic TPU functionality first\n",
        "def test_basic_tpu():\n",
        "    \"\"\"Test basic TPU functionality\"\"\"\n",
        "    print(\"=== Testing Basic TPU Operations ===\")\n",
        "\n",
        "    # Basic tensor operations\n",
        "    x = torch.randn(3, 3, device=device)\n",
        "    y = torch.randn(3, 3, device=device)\n",
        "    z = torch.matmul(x, y)\n",
        "\n",
        "    print(f\"✓ Basic tensor operations work on TPU\")\n",
        "    print(f\"Device: {device}\")\n",
        "    print(f\"Result shape: {z.shape}\")\n",
        "\n",
        "    return True\n"
      ],
      "metadata": {
        "id": "d3xZ9GlQZIor"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_basic_tpu()"
      ],
      "metadata": {
        "id": "8jNpY4s9ZKv_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Test TPU first\n",
        "    test_basic_tpu()\n",
        "\n",
        "    # Run main training and inference\n",
        "    model, tokenizer, results = main()"
      ],
      "metadata": {
        "id": "MYa6Ueo1ZJVb"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "Welcome To Colab",
      "toc_visible": true,
      "provenance": [],
      "gpuType": "V28",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "TPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}