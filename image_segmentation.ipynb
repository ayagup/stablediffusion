{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.13",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "none",
      "dataSources": [
        {
          "sourceId": 13427111,
          "sourceType": "datasetVersion",
          "datasetId": 8522301
        }
      ],
      "dockerImageVersionId": 31153,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": false
    },
    "colab": {
      "name": "image_segmentation",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ayagup/stablediffusion/blob/main/image_segmentation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "source": [
        "# IMPORTANT: SOME KAGGLE DATA SOURCES ARE PRIVATE\n",
        "# RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES.\n",
        "import kagglehub\n",
        "kagglehub.login()\n"
      ],
      "metadata": {
        "id": "ELp9vl0x2aMZ"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "source": [
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "\n",
        "maygup123_dataset_image_segmentation_path = kagglehub.dataset_download('maygup123/dataset-image-segmentation')\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "id": "_U06ap6h2aMa"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# This Python 3 environment comes with many helpful analytics libraries installed\n",
        "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
        "# For example, here's several helpful packages to load\n",
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "# Input data files are available in the read-only \"../input/\" directory\n",
        "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
        "\n",
        "import os\n",
        "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirname, filename))\n",
        "\n",
        "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\"\n",
        "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "id": "NnlqUfAj2aMb"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Simple Image Segmentation Pipeline\n",
        "Segment images using HuggingFace Transformers models\n",
        "\"\"\"\n",
        "\n",
        "import torch\n",
        "from transformers import AutoImageProcessor, AutoModelForSemanticSegmentation\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "import os\n",
        "import gc\n",
        "import time\n",
        "from typing import Optional\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.patches as mpatches\n",
        "\n",
        "# Suppress warnings\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
        "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n",
        "\n",
        "\n",
        "def print_header():\n",
        "    \"\"\"Print a nice header\"\"\"\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"üé® Image Segmentation Pipeline\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "\n",
        "def clear_gpu_memory():\n",
        "    \"\"\"Clear GPU memory cache\"\"\"\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "        gc.collect()\n",
        "        print(\"üßπ GPU memory cache cleared\")\n",
        "\n",
        "\n",
        "def load_image(image_path: str, max_size: Optional[int] = None) -> Image.Image:\n",
        "    \"\"\"Load and optionally resize image\"\"\"\n",
        "    print(f\"Loading image: {image_path}\")\n",
        "\n",
        "    if image_path.startswith('http://') or image_path.startswith('https://'):\n",
        "        import requests\n",
        "        from io import BytesIO\n",
        "        response = requests.get(image_path)\n",
        "        image = Image.open(BytesIO(response.content)).convert('RGB')\n",
        "    else:\n",
        "        image = Image.open(image_path).convert('RGB')\n",
        "\n",
        "    original_size = image.size\n",
        "    print(f\"Original size: {original_size[0]}x{original_size[1]}\")\n",
        "\n",
        "    # Resize if needed\n",
        "    if max_size and max(image.size) > max_size:\n",
        "        ratio = max_size / max(image.size)\n",
        "        new_size = tuple(int(dim * ratio) for dim in image.size)\n",
        "        image = image.resize(new_size, Image.LANCZOS)\n",
        "        print(f\"Resized to: {new_size[0]}x{new_size[1]}\")\n",
        "\n",
        "    return image\n",
        "\n",
        "\n",
        "def create_segmentation_visualization(\n",
        "    image: Image.Image,\n",
        "    segmentation_map: np.ndarray,\n",
        "    id2label: dict,\n",
        "    output_path: str,\n",
        "    alpha: float = 0.6\n",
        "):\n",
        "    \"\"\"Create visualization of segmentation results\"\"\"\n",
        "\n",
        "    # Create figure with subplots\n",
        "    fig, axes = plt.subplots(1, 3, figsize=(18, 6))\n",
        "\n",
        "    # Original image\n",
        "    axes[0].imshow(image)\n",
        "    axes[0].set_title('Original Image', fontsize=14, fontweight='bold')\n",
        "    axes[0].axis('off')\n",
        "\n",
        "    # Segmentation map\n",
        "    axes[1].imshow(segmentation_map, cmap='tab20')\n",
        "    axes[1].set_title('Segmentation Map', fontsize=14, fontweight='bold')\n",
        "    axes[1].axis('off')\n",
        "\n",
        "    # Overlay\n",
        "    axes[2].imshow(image)\n",
        "    axes[2].imshow(segmentation_map, cmap='tab20', alpha=alpha)\n",
        "    axes[2].set_title('Overlay', fontsize=14, fontweight='bold')\n",
        "    axes[2].axis('off')\n",
        "\n",
        "    # Create legend for unique classes\n",
        "    unique_labels = np.unique(segmentation_map)\n",
        "    colors = plt.cm.tab20(np.linspace(0, 1, 20))\n",
        "\n",
        "    patches = []\n",
        "    for label_id in unique_labels:\n",
        "        if label_id in id2label:\n",
        "            label_name = id2label[label_id]\n",
        "            color = colors[label_id % 20]\n",
        "            patches.append(mpatches.Patch(color=color, label=label_name))\n",
        "\n",
        "    # Add legend\n",
        "    if patches:\n",
        "        fig.legend(handles=patches, loc='center', bbox_to_anchor=(0.5, -0.05),\n",
        "                  ncol=min(len(patches), 5), fontsize=10)\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(output_path, bbox_inches='tight', dpi=150)\n",
        "    plt.close()\n",
        "\n",
        "    print(f\"‚úì Segmentation visualization saved: {output_path}\")\n",
        "\n",
        "\n",
        "def segment_image(\n",
        "    image_path: str,\n",
        "    model_name: str = \"nvidia/segformer-b0-finetuned-ade-512-512\",\n",
        "    output_path: str = \"segmented_image.png\",\n",
        "    max_image_size: Optional[int] = 1024,\n",
        "    overlay_alpha: float = 0.6,\n",
        ") -> tuple:\n",
        "    \"\"\"\n",
        "    Segment image using semantic segmentation model\n",
        "\n",
        "    Args:\n",
        "        image_path: Path to input image or URL\n",
        "        model_name: HuggingFace model identifier\n",
        "        output_path: Output path for visualization\n",
        "        max_image_size: Maximum image dimension\n",
        "        overlay_alpha: Transparency for overlay (0-1)\n",
        "\n",
        "    Returns:\n",
        "        Tuple of (segmentation_map, class_counts, id2label)\n",
        "    \"\"\"\n",
        "\n",
        "    print_header()\n",
        "    clear_gpu_memory()\n",
        "\n",
        "    # Device setup\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "    if device == \"cuda\":\n",
        "        gpu_name = torch.cuda.get_device_name(0)\n",
        "        gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
        "        print(f\"\\nüñ•Ô∏è  Using GPU: {gpu_name}\")\n",
        "        print(f\"   Memory: {gpu_memory:.1f} GB\\n\")\n",
        "    else:\n",
        "        print(\"\\nüíª Using CPU\\n\")\n",
        "\n",
        "    # Load image\n",
        "    image = load_image(image_path, max_image_size)\n",
        "\n",
        "    # Load model\n",
        "    print(f\"\\nLoading model: {model_name}\")\n",
        "    start_time = time.time()\n",
        "\n",
        "    processor = AutoImageProcessor.from_pretrained(model_name)\n",
        "    model = AutoModelForSemanticSegmentation.from_pretrained(model_name)\n",
        "    model = model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    load_time = time.time() - start_time\n",
        "    print(f\"‚úì Model loaded in {load_time:.2f}s\")\n",
        "\n",
        "    # Get label mapping\n",
        "    id2label = model.config.id2label\n",
        "    num_classes = len(id2label)\n",
        "    print(f\"‚úì Model supports {num_classes} classes\")\n",
        "\n",
        "    # Print segmentation parameters\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"üé¨ Segmentation Parameters\")\n",
        "    print(\"=\"*70)\n",
        "    print(f\"Input image: {image_path}\")\n",
        "    print(f\"Image size: {image.size[0]}x{image.size[1]}\")\n",
        "    print(f\"Model: {model_name}\")\n",
        "    print(f\"Device: {device}\")\n",
        "    print(f\"Number of classes: {num_classes}\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    # Prepare inputs\n",
        "    print(\"\\nüé® Segmenting image...\")\n",
        "    inputs = processor(images=image, return_tensors=\"pt\")\n",
        "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "\n",
        "    # Inference\n",
        "    inference_start = time.time()\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "\n",
        "    inference_time = time.time() - inference_start\n",
        "    print(f\"‚úì Inference completed in {inference_time:.3f}s\")\n",
        "\n",
        "    # Post-process\n",
        "    print(\"\\nüìä Processing segmentation map...\")\n",
        "\n",
        "    # Get logits and convert to segmentation map\n",
        "    logits = outputs.logits\n",
        "\n",
        "    # Resize to original image size\n",
        "    upsampled_logits = torch.nn.functional.interpolate(\n",
        "        logits,\n",
        "        size=image.size[::-1],  # (height, width)\n",
        "        mode='bilinear',\n",
        "        align_corners=False\n",
        "    )\n",
        "\n",
        "    # Get predictions\n",
        "    segmentation_map = upsampled_logits.argmax(dim=1)[0].cpu().numpy()\n",
        "\n",
        "    # Count classes\n",
        "    unique_labels, counts = np.unique(segmentation_map, return_counts=True)\n",
        "    class_counts = {}\n",
        "    total_pixels = segmentation_map.size\n",
        "\n",
        "    for label_id, count in zip(unique_labels, counts):\n",
        "        if label_id in id2label:\n",
        "            class_name = id2label[label_id]\n",
        "            percentage = (count / total_pixels) * 100\n",
        "            class_counts[class_name] = {\n",
        "                'pixels': int(count),\n",
        "                'percentage': percentage\n",
        "            }\n",
        "\n",
        "    # Print results\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"üèÜ Segmentation Results\")\n",
        "    print(\"=\"*70)\n",
        "    print(f\"\\nTotal classes detected: {len(unique_labels)}\")\n",
        "    print(f\"Image resolution: {image.size[0]}x{image.size[1]} ({total_pixels:,} pixels)\")\n",
        "    print(\"\\nClass breakdown:\")\n",
        "\n",
        "    # Sort by percentage\n",
        "    sorted_classes = sorted(class_counts.items(), key=lambda x: x[1]['percentage'], reverse=True)\n",
        "\n",
        "    for class_name, stats in sorted_classes:\n",
        "        print(f\"  {class_name:20s}: {stats['percentage']:5.2f}% ({stats['pixels']:,} pixels)\")\n",
        "\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "\n",
        "    # Create visualization\n",
        "    print(\"\\nüé® Creating visualization...\")\n",
        "    create_segmentation_visualization(\n",
        "        image, segmentation_map, id2label, output_path, overlay_alpha\n",
        "    )\n",
        "\n",
        "    # Summary\n",
        "    total_time = time.time() - start_time + load_time\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"‚úÖ Segmentation Complete!\")\n",
        "    print(\"=\"*70)\n",
        "    print(f\"Total time: {total_time:.2f}s\")\n",
        "    print(f\"  - Model loading: {load_time:.2f}s\")\n",
        "    print(f\"  - Inference: {inference_time:.3f}s\")\n",
        "    print(f\"  - Post-processing: {total_time - load_time - inference_time:.2f}s\")\n",
        "    print(f\"\\nOutput saved: {output_path}\")\n",
        "    print(f\"Classes found: {len(unique_labels)}\")\n",
        "    print(\"=\"*70 + \"\\n\")\n",
        "\n",
        "    clear_gpu_memory()\n",
        "\n",
        "    return segmentation_map, class_counts, id2label\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-18T19:21:55.841684Z",
          "iopub.execute_input": "2025-10-18T19:21:55.842051Z",
          "iopub.status.idle": "2025-10-18T19:22:27.169488Z",
          "shell.execute_reply.started": "2025-10-18T19:21:55.842013Z",
          "shell.execute_reply": "2025-10-18T19:22:27.168424Z"
        },
        "id": "_erp8Lc-2aMb",
        "outputId": "1ca9d089-a4a2-418b-fab6-98d76ffc983f"
      },
      "outputs": [
        {
          "name": "stderr",
          "text": "2025-10-18 19:22:09.047009: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1760815329.263538      37 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1760815329.341480      37 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# if __name__ == \"__main__\":\n",
        "#     import argparse\n",
        "\n",
        "#     parser = argparse.ArgumentParser(\n",
        "#         description='Simple Image Segmentation',\n",
        "#         formatter_class=argparse.RawDescriptionHelpFormatter,\n",
        "#         epilog=\"\"\"\n",
        "# Examples:\n",
        "#   # Basic segmentation with SegFormer\n",
        "#   python simple_image_segmentation.py --image photo.jpg\n",
        "\n",
        "#   # Use different model\n",
        "#   python simple_image_segmentation.py --image photo.jpg --model nvidia/segformer-b2-finetuned-ade-512-512\n",
        "\n",
        "#   # Segment from URL\n",
        "#   python simple_image_segmentation.py --image https://example.com/photo.jpg\n",
        "\n",
        "#   # Adjust overlay transparency\n",
        "#   python simple_image_segmentation.py --image photo.jpg --alpha 0.4\n",
        "#         \"\"\"\n",
        "#     )\n",
        "\n",
        "#     parser.add_argument('--image', type=str, required=True,\n",
        "#                         help='Path to input image or URL')\n",
        "#     parser.add_argument('--model', type=str,\n",
        "#                         default='nvidia/segformer-b0-finetuned-ade-512-512',\n",
        "#                         help='HuggingFace model name')\n",
        "#     parser.add_argument('--output', type=str, default='segmented_image.png',\n",
        "#                         help='Output path for visualization')\n",
        "#     parser.add_argument('--max-size', type=int, default=1024,\n",
        "#                         help='Maximum image dimension')\n",
        "#     parser.add_argument('--alpha', type=float, default=0.6,\n",
        "#                         help='Overlay transparency (0-1)')\n",
        "\n",
        "#     args = parser.parse_args()\n",
        "\n",
        "try:\n",
        "    segment_image(\n",
        "        image_path='/kaggle/input/dataset-image-segmentation/detected_objects (1).jpg',\n",
        "        model_name='nvidia/segformer-b0-finetuned-ade-512-512',\n",
        "        output_path='/kaggle/working/segmented_image.png',\n",
        "        max_image_size=1024,\n",
        "        overlay_alpha=0.6,\n",
        "    )\n",
        "except Exception as e:\n",
        "    print(f\"\\n‚ùå Error: {e}\")\n",
        "    import traceback\n",
        "    traceback.print_exc()\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-18T19:42:49.908315Z",
          "iopub.execute_input": "2025-10-18T19:42:49.909237Z",
          "iopub.status.idle": "2025-10-18T19:42:56.131437Z",
          "shell.execute_reply.started": "2025-10-18T19:42:49.909208Z",
          "shell.execute_reply": "2025-10-18T19:42:56.130629Z"
        },
        "id": "u-VkJDVE2aMc",
        "outputId": "289797ed-00e7-4f8c-8e41-dc9849469f22",
        "colab": {
          "referenced_widgets": [
            "df69cf06e51640d0b8152a449327abd1",
            "b7eb40e0d7f94626a935f85a685669cb",
            "9c7a6e0aef2d4cc783032dea76bb167d"
          ]
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "\n======================================================================\nüé® Image Segmentation Pipeline\n======================================================================\n\nüíª Using CPU\n\nLoading image: /kaggle/input/dataset-image-segmentation/detected_objects (1).jpg\nOriginal size: 5656x4244\nResized to: 1024x768\n\nLoading model: nvidia/segformer-b0-finetuned-ade-512-512\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "preprocessor_config.json:   0%|          | 0.00/271 [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "df69cf06e51640d0b8152a449327abd1"
            }
          },
          "metadata": {}
        },
        {
          "name": "stderr",
          "text": "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "config.json: 0.00B [00:00, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b7eb40e0d7f94626a935f85a685669cb"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "model.safetensors:   0%|          | 0.00/15.0M [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9c7a6e0aef2d4cc783032dea76bb167d"
            }
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "text": "‚úì Model loaded in 2.21s\n‚úì Model supports 150 classes\n\n======================================================================\nüé¨ Segmentation Parameters\n======================================================================\nInput image: /kaggle/input/dataset-image-segmentation/detected_objects (1).jpg\nImage size: 1024x768\nModel: nvidia/segformer-b0-finetuned-ade-512-512\nDevice: cpu\nNumber of classes: 150\n======================================================================\n\nüé® Segmenting image...\n‚úì Inference completed in 0.724s\n\nüìä Processing segmentation map...\n\n======================================================================\nüèÜ Segmentation Results\n======================================================================\n\nTotal classes detected: 11\nImage resolution: 1024x768 (786,432 pixels)\n\nClass breakdown:\n  wall                : 82.62% (649,750 pixels)\n  chair               :  8.64% (67,918 pixels)\n  painting            :  5.59% (43,931 pixels)\n  basket              :  2.49% (19,614 pixels)\n  plate               :  0.28% (2,203 pixels)\n  lamp                :  0.13% (995 pixels)\n  cushion             :  0.11% (838 pixels)\n  floor               :  0.06% (478 pixels)\n  table               :  0.05% (380 pixels)\n  box                 :  0.02% (169 pixels)\n  book                :  0.02% (156 pixels)\n\n======================================================================\n\nüé® Creating visualization...\n‚úì Segmentation visualization saved: /kaggle/working/segmented_image.png\n\n======================================================================\n‚úÖ Segmentation Complete!\n======================================================================\nTotal time: 7.69s\n  - Model loading: 2.21s\n  - Inference: 0.724s\n  - Post-processing: 4.76s\n\nOutput saved: /kaggle/working/segmented_image.png\nClasses found: 11\n======================================================================\n\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "vAa-gc8l2aMd"
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}