{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.13",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [
        {
          "sourceId": 13427359,
          "sourceType": "datasetVersion",
          "datasetId": 8522439
        }
      ],
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "name": "image_to_text",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ayagup/stablediffusion/blob/main/image_to_text.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "source": [
        "# IMPORTANT: SOME KAGGLE DATA SOURCES ARE PRIVATE\n",
        "# RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES.\n",
        "import kagglehub\n",
        "kagglehub.login()\n"
      ],
      "metadata": {
        "id": "Sol8WCPLHtr1"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "source": [
        "# IMPORTANT: RUN THIS CELL IN ORDER TO IMPORT YOUR KAGGLE DATA SOURCES,\n",
        "# THEN FEEL FREE TO DELETE THIS CELL.\n",
        "# NOTE: THIS NOTEBOOK ENVIRONMENT DIFFERS FROM KAGGLE'S PYTHON\n",
        "# ENVIRONMENT SO THERE MAY BE MISSING LIBRARIES USED BY YOUR\n",
        "# NOTEBOOK.\n",
        "\n",
        "maygup123_dataset1_path = kagglehub.dataset_download('maygup123/dataset1')\n",
        "\n",
        "print('Data source import complete.')\n"
      ],
      "metadata": {
        "id": "4y5J4F9wHtr4"
      },
      "cell_type": "code",
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# This Python 3 environment comes with many helpful analytics libraries installed\n",
        "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
        "# For example, here's several helpful packages to load\n",
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "# Input data files are available in the read-only \"../input/\" directory\n",
        "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
        "\n",
        "import os\n",
        "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirname, filename))\n",
        "\n",
        "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\"\n",
        "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "id": "F6f7siouHtr4"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install \"sentencepiece>=0.1.99\""
      ],
      "metadata": {
        "trusted": true,
        "id": "1EVqnpFyHtr5"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Simple Image-to-Text (Image Captioning) Pipeline\n",
        "Generate text captions from images using HuggingFace Transformers models\n",
        "\"\"\"\n",
        "import transformers.utils.hub\n",
        "transformers.utils.hub.list_repo_templates = lambda *args, **kwargs: []\n",
        "\n",
        "import torch\n",
        "from transformers import AutoProcessor, AutoModelForVision2Seq, BlipProcessor, BlipForConditionalGeneration\n",
        "from PIL import Image\n",
        "import os\n",
        "import gc\n",
        "import time\n",
        "from typing import Optional, List\n",
        "\n",
        "# Suppress warnings\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
        "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n",
        "\n",
        "# Monkey patch to fix transformers chat_template 404 error\n",
        "# Must be done at module level before any model loading\n",
        "import transformers.utils.hub as hub_utils\n",
        "_original_list_repo_templates = hub_utils.list_repo_templates\n",
        "\n",
        "def _patched_list_repo_templates(*args, **kwargs):\n",
        "    \"\"\"Return empty list to avoid 404 errors on chat templates\"\"\"\n",
        "    return []\n",
        "\n",
        "# Apply the patch globally\n",
        "hub_utils.list_repo_templates = _patched_list_repo_templates\n",
        "\n",
        "\n",
        "def print_header():\n",
        "    \"\"\"Print a nice header\"\"\"\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"üìù Image-to-Text (Captioning) Pipeline\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "\n",
        "def clear_gpu_memory():\n",
        "    \"\"\"Clear GPU memory cache\"\"\"\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "        gc.collect()\n",
        "        print(\"üßπ GPU memory cache cleared\")\n",
        "\n",
        "\n",
        "def load_image(image_path: str, max_size: Optional[int] = None) -> Image.Image:\n",
        "    \"\"\"Load and optionally resize image\"\"\"\n",
        "    print(f\"Loading image: {image_path}\")\n",
        "\n",
        "    if image_path.startswith('http://') or image_path.startswith('https://'):\n",
        "        import requests\n",
        "        from io import BytesIO\n",
        "        response = requests.get(image_path)\n",
        "        image = Image.open(BytesIO(response.content)).convert('RGB')\n",
        "    else:\n",
        "        image = Image.open(image_path).convert('RGB')\n",
        "\n",
        "    original_size = image.size\n",
        "    print(f\"Original size: {original_size[0]}x{original_size[1]}\")\n",
        "\n",
        "    # Resize if needed\n",
        "    if max_size and max(image.size) > max_size:\n",
        "        ratio = max_size / max(image.size)\n",
        "        new_size = tuple(int(dim * ratio) for dim in image.size)\n",
        "        image = image.resize(new_size, Image.LANCZOS)\n",
        "        print(f\"Resized to: {new_size[0]}x{new_size[1]}\")\n",
        "\n",
        "    return image\n",
        "\n",
        "\n",
        "def generate_caption(\n",
        "    image_path: str,\n",
        "    model_name: str = \"Salesforce/blip-image-captioning-large\",\n",
        "    max_length: int = 50,\n",
        "    num_beams: int = 5,\n",
        "    max_image_size: Optional[int] = 1024,\n",
        "    generate_multiple: bool = False,\n",
        "    num_captions: int = 3,\n",
        ") -> List[str]:\n",
        "    \"\"\"\n",
        "    Generate text caption from image\n",
        "\n",
        "    Args:\n",
        "        image_path: Path to input image or URL\n",
        "        model_name: HuggingFace model identifier\n",
        "        max_length: Maximum caption length\n",
        "        num_beams: Number of beams for beam search\n",
        "        max_image_size: Maximum image dimension\n",
        "        generate_multiple: Generate multiple captions\n",
        "        num_captions: Number of captions to generate (if generate_multiple=True)\n",
        "\n",
        "    Returns:\n",
        "        List of generated captions\n",
        "    \"\"\"\n",
        "\n",
        "    print_header()\n",
        "    clear_gpu_memory()\n",
        "\n",
        "    # Device setup\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "    if device == \"cuda\":\n",
        "        gpu_name = torch.cuda.get_device_name(0)\n",
        "        gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
        "        print(f\"\\nüñ•Ô∏è  Using GPU: {gpu_name}\")\n",
        "        print(f\"   Memory: {gpu_memory:.1f} GB\\n\")\n",
        "    else:\n",
        "        print(\"\\nüíª Using CPU\\n\")\n",
        "\n",
        "    # Load image\n",
        "    image = load_image(image_path, max_image_size)\n",
        "\n",
        "    # Load model\n",
        "    print(f\"\\nLoading model: {model_name}\")\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Use BLIP models with their specific processor\n",
        "    if \"blip\" in model_name.lower():\n",
        "        processor = BlipProcessor.from_pretrained(model_name)\n",
        "        model = BlipForConditionalGeneration.from_pretrained(\n",
        "            model_name,\n",
        "            trust_remote_code=True\n",
        "        )\n",
        "    else:\n",
        "        processor = AutoProcessor.from_pretrained(\n",
        "            model_name,\n",
        "            use_fast=True,\n",
        "            trust_remote_code=True\n",
        "        )\n",
        "        model = AutoModelForVision2Seq.from_pretrained(\n",
        "            model_name,\n",
        "            trust_remote_code=True\n",
        "        )\n",
        "\n",
        "    model = model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    load_time = time.time() - start_time\n",
        "    print(f\"‚úì Model loaded in {load_time:.2f}s\")\n",
        "\n",
        "    # Print generation parameters\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"üé¨ Caption Generation Parameters\")\n",
        "    print(\"=\"*70)\n",
        "    print(f\"Input image: {image_path}\")\n",
        "    print(f\"Image size: {image.size[0]}x{image.size[1]}\")\n",
        "    print(f\"Model: {model_name}\")\n",
        "    print(f\"Device: {device}\")\n",
        "    print(f\"Max caption length: {max_length} tokens\")\n",
        "    print(f\"Beam search: {num_beams} beams\")\n",
        "    if generate_multiple:\n",
        "        print(f\"Number of captions: {num_captions}\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    # Prepare inputs\n",
        "    print(\"\\nüìù Generating caption(s)...\")\n",
        "    inputs = processor(images=image, return_tensors=\"pt\")\n",
        "    inputs = {k: v.to(device) for k, v in inputs.items()}\n",
        "\n",
        "    # Generate caption(s)\n",
        "    generation_start = time.time()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        if generate_multiple:\n",
        "            # Generate multiple captions with sampling\n",
        "            outputs = model.generate(\n",
        "                **inputs,\n",
        "                max_length=max_length,\n",
        "                num_beams=num_beams,\n",
        "                num_return_sequences=num_captions,\n",
        "                do_sample=True,\n",
        "                top_k=50,\n",
        "                top_p=0.95,\n",
        "                temperature=0.7,\n",
        "            )\n",
        "        else:\n",
        "            # Generate single best caption\n",
        "            outputs = model.generate(\n",
        "                **inputs,\n",
        "                max_length=max_length,\n",
        "                num_beams=num_beams,\n",
        "            )\n",
        "\n",
        "    generation_time = time.time() - generation_start\n",
        "    print(f\"‚úì Generation completed in {generation_time:.3f}s\")\n",
        "\n",
        "    # Decode captions\n",
        "    captions = processor.batch_decode(outputs, skip_special_tokens=True)\n",
        "\n",
        "    # Clean up captions\n",
        "    captions = [caption.strip() for caption in captions]\n",
        "\n",
        "    # Print results\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"üìã Generated Caption(s)\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    if len(captions) == 1:\n",
        "        print(f\"\\n{captions[0]}\")\n",
        "    else:\n",
        "        for idx, caption in enumerate(captions, 1):\n",
        "            print(f\"\\n{idx}. {caption}\")\n",
        "\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "\n",
        "    # Summary\n",
        "    total_time = time.time() - start_time + load_time\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"‚úÖ Captioning Complete!\")\n",
        "    print(\"=\"*70)\n",
        "    print(f\"Total time: {total_time:.2f}s\")\n",
        "    print(f\"  - Model loading: {load_time:.2f}s\")\n",
        "    print(f\"  - Caption generation: {generation_time:.3f}s\")\n",
        "    print(f\"\\nCaptions generated: {len(captions)}\")\n",
        "    if captions:\n",
        "        print(f\"Caption length: {len(captions[0].split())} words\")\n",
        "    print(\"=\"*70 + \"\\n\")\n",
        "\n",
        "    clear_gpu_memory()\n",
        "\n",
        "    return captions\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-19T06:17:37.683163Z",
          "iopub.execute_input": "2025-10-19T06:17:37.683475Z",
          "iopub.status.idle": "2025-10-19T06:17:46.827966Z",
          "shell.execute_reply.started": "2025-10-19T06:17:37.68345Z",
          "shell.execute_reply": "2025-10-19T06:17:46.827263Z"
        },
        "id": "JaOxyBypHtr5",
        "outputId": "6d1fdc1c-a1e7-4692-af4d-39ba8fc23043"
      },
      "outputs": [
        {
          "name": "stderr",
          "text": "2025-10-19 06:17:42.126792: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1760854662.150184     269 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1760854662.157385     269 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# if __name__ == \"__main__\":\n",
        "#     import argparse\n",
        "\n",
        "#     parser = argparse.ArgumentParser(\n",
        "#         description='Simple Image-to-Text Captioning',\n",
        "#         formatter_class=argparse.RawDescriptionHelpFormatter,\n",
        "#         epilog=\"\"\"\n",
        "# Examples:\n",
        "#   # Basic captioning\n",
        "#   python simple_image_to_text.py --image photo.jpg\n",
        "\n",
        "#   # Generate multiple captions\n",
        "#   python simple_image_to_text.py --image photo.jpg --multiple --num-captions 5\n",
        "\n",
        "#   # Use different model\n",
        "#   python simple_image_to_text.py --image photo.jpg --model Salesforce/blip-image-captioning-base\n",
        "\n",
        "#   # From URL\n",
        "#   python simple_image_to_text.py --image https://example.com/photo.jpg\n",
        "\n",
        "#   # Longer captions\n",
        "#   python simple_image_to_text.py --image photo.jpg --max-length 100\n",
        "#         \"\"\"\n",
        "#     )\n",
        "\n",
        "#     parser.add_argument('--image', type=str, required=True,\n",
        "#                         help='Path to input image or URL')\n",
        "#     parser.add_argument('--model', type=str,\n",
        "#                         default='Salesforce/blip-image-captioning-large',\n",
        "#                         help='HuggingFace model name')\n",
        "#     parser.add_argument('--max-length', type=int, default=50,\n",
        "#                         help='Maximum caption length in tokens')\n",
        "#     parser.add_argument('--num-beams', type=int, default=5,\n",
        "#                         help='Number of beams for beam search')\n",
        "#     parser.add_argument('--max-size', type=int, default=1024,\n",
        "#                         help='Maximum image dimension')\n",
        "#     parser.add_argument('--multiple', action='store_true',\n",
        "#                         help='Generate multiple captions')\n",
        "#     parser.add_argument('--num-captions', type=int, default=3,\n",
        "#                         help='Number of captions to generate (with --multiple)')\n",
        "\n",
        "#     args = parser.parse_args()\n",
        "\n",
        "# Fix for transformers chat_template 404 error\n",
        "\n",
        "\n",
        "# Now import and use the function\n",
        "# from simple_image_to_text import generate_caption\n",
        "\n",
        "# captions = generate_caption(\n",
        "#     image_path=\"/kaggle/input/dataset1/google-imagen-lead-image.jpeg\",\n",
        "#     model_name=\"Salesforce/blip-image-captioning-large\"\n",
        "# )\n",
        "\n",
        "try:\n",
        "    captions = generate_caption(\n",
        "        image_path='/kaggle/input/dataset1/google-imagen-lead-image.jpeg',\n",
        "        model_name='Salesforce/blip-image-captioning-large',\n",
        "        max_length=50,\n",
        "        num_beams=5,\n",
        "        max_image_size=1024,\n",
        "        generate_multiple=True,\n",
        "        num_captions=3,\n",
        "    )\n",
        "\n",
        "    # Also print to stdout for easy scripting\n",
        "    if len(captions) == 1:\n",
        "        print(f\"\\nCaption: {captions[0]}\")\n",
        "    else:\n",
        "        print(f\"\\nCaptions:\")\n",
        "        for idx, caption in enumerate(captions, 1):\n",
        "            print(f\"  {idx}. {caption}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"\\n‚ùå Error: {e}\")\n",
        "    import traceback\n",
        "    traceback.print_exc()\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-19T06:17:46.828997Z",
          "iopub.execute_input": "2025-10-19T06:17:46.829497Z",
          "iopub.status.idle": "2025-10-19T06:17:59.419413Z",
          "shell.execute_reply.started": "2025-10-19T06:17:46.829477Z",
          "shell.execute_reply": "2025-10-19T06:17:59.418729Z"
        },
        "id": "_fF8Uo0qHtr6",
        "outputId": "46103cad-4042-4e9f-c0a8-b1f3df5771f2",
        "colab": {
          "referenced_widgets": [
            "6f703b50357f4ccfbc4a72202777e489",
            "d2341e286dea4ee491d2426a7f3382ca",
            "38469928fb3746f4b313fc6d1e735c12",
            "8ef14f59a1894aa88d6afda8cc5905c2",
            "727d364092e445b3a65ba7922ae104c4"
          ]
        }
      },
      "outputs": [
        {
          "name": "stderr",
          "text": "Using a slow image processor as `use_fast` is unset and a slow processor was saved with this model. `use_fast=True` will be the default behavior in v4.52, even if the model was saved with a slow processor. This will result in minor differences in outputs. You'll still be able to use a slow processor with `use_fast=False`.\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\n======================================================================\nüìù Image-to-Text (Captioning) Pipeline\n======================================================================\nüßπ GPU memory cache cleared\n\nüñ•Ô∏è  Using GPU: Tesla P100-PCIE-16GB\n   Memory: 15.9 GB\n\nLoading image: /kaggle/input/dataset1/google-imagen-lead-image.jpeg\nOriginal size: 1180x885\nResized to: 1024x768\n\nLoading model: Salesforce/blip-image-captioning-large\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "vocab.txt: 0.00B [00:00, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6f703b50357f4ccfbc4a72202777e489"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "tokenizer.json: 0.00B [00:00, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d2341e286dea4ee491d2426a7f3382ca"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "special_tokens_map.json:   0%|          | 0.00/125 [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "38469928fb3746f4b313fc6d1e735c12"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "config.json: 0.00B [00:00, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8ef14f59a1894aa88d6afda8cc5905c2"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "model.safetensors:   0%|          | 0.00/1.88G [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "727d364092e445b3a65ba7922ae104c4"
            }
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "text": "‚úì Model loaded in 9.68s\n\n======================================================================\nüé¨ Caption Generation Parameters\n======================================================================\nInput image: /kaggle/input/dataset1/google-imagen-lead-image.jpeg\nImage size: 1024x768\nModel: Salesforce/blip-image-captioning-large\nDevice: cuda\nMax caption length: 50 tokens\nBeam search: 5 beams\nNumber of captions: 3\n======================================================================\n\nüìù Generating caption(s)...\n‚úì Generation completed in 2.073s\n\n======================================================================\nüìã Generated Caption(s)\n======================================================================\n\n1. blue bird sitting on top of a pile of macaroons on top of a table\n\n2. blue bird sitting on top of a pile of macaroons on top of a plate\n\n3. blue bird sitting on top of a plate of macarons with colorful macarons\n\n======================================================================\n\n======================================================================\n‚úÖ Captioning Complete!\n======================================================================\nTotal time: 21.50s\n  - Model loading: 9.68s\n  - Caption generation: 2.073s\n\nCaptions generated: 3\nCaption length: 15 words\n======================================================================\n\nüßπ GPU memory cache cleared\n\nCaptions:\n  1. blue bird sitting on top of a pile of macaroons on top of a table\n  2. blue bird sitting on top of a pile of macaroons on top of a plate\n  3. blue bird sitting on top of a plate of macarons with colorful macarons\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install \"httpx==0.27.2\""
      ],
      "metadata": {
        "trusted": true,
        "id": "JQORWpjQHtr7"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "%pip freeze"
      ],
      "metadata": {
        "trusted": true,
        "id": "5MTE008gHtr7"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install 'httpx>=0.27.0,<0.28.0' 'huggingface-hub>=0.30.0' --quiet --upgrade\n",
        "\n",
        "print(\"\\n‚úÖ Installation complete!\")\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"‚ö†Ô∏è  CRITICAL: You MUST restart your kernel now!\")\n",
        "print(\"=\"*70)\n",
        "print(\"\\nAfter restarting, your vit-gpt2 model should work.\")\n",
        "print(\"\\nIf this still doesn't work, the issue is Kaggle-specific.\")\n",
        "print(\"In that case, please use BLIP model instead:\")\n",
        "print(\"  model_name='Salesforce/blip-image-captioning-large'\")\n",
        "print(\"=\"*70)\n"
      ],
      "metadata": {
        "trusted": true,
        "id": "5OnnOX7IHtr8"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "JuLHaZHmHtr8"
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}