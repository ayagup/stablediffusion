{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ayagup/stablediffusion/blob/main/llm%20inferencing%20via%20vllm.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JyG45Qk3qQLS"
      },
      "source": [
        "# Cells\n",
        "A notebook is a list of cells. Cells contain either explanatory text or executable code and its output. Click a cell to select it."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KR921S_OQSHG"
      },
      "source": [
        "## Code cells\n",
        "Below is a **code cell**. Once the toolbar button indicates CONNECTED, click in the cell to select it and execute the contents in the following ways:\n",
        "\n",
        "* Click the **Play icon** in the left gutter of the cell;\n",
        "* Type **Cmd/Ctrl+Enter** to run the cell in place;\n",
        "* Type **Shift+Enter** to run the cell and move focus to the next cell (adding one if none exists); or\n",
        "* Type **Alt+Enter** to run the cell and insert a new code cell immediately below it.\n",
        "\n",
        "There are additional options for running some or all cells in the **Runtime** menu.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "cellView": "both",
        "id": "WUtu4316QSHL"
      },
      "outputs": [],
      "source": [
        "# a = 10\n",
        "# a"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Id6tDF1HQSHD"
      },
      "source": [
        "## Text cells\n",
        "This is a **text cell**. You can **double-click** to edit this cell. Text cells\n",
        "use markdown syntax. To learn more, see our [markdown\n",
        "guide](/notebooks/markdown_guide.ipynb).\n",
        "\n",
        "You can also add math to text cells using [LaTeX](http://www.latex-project.org/)\n",
        "to be rendered by [MathJax](https://www.mathjax.org). Just place the statement\n",
        "within a pair of **\\$** signs. For example `$\\sqrt{3x-1}+(1+x)^2$` becomes\n",
        "$\\sqrt{3x-1}+(1+x)^2.$\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7bqjkGKwQSHW"
      },
      "source": [
        "## Adding and moving cells\n",
        "You can add new cells by using the **+ CODE** and **+ TEXT** buttons that show when you hover between cells. These buttons are also in the toolbar above the notebook where they can be used to add a cell below the currently selected cell.\n",
        "\n",
        "You can move a cell by selecting it and clicking **Cell Up** or **Cell Down** in the top toolbar.\n",
        "\n",
        "Consecutive cells can be selected by \"lasso selection\" by dragging from outside one cell and through the group.  Non-adjacent cells can be selected concurrently by clicking one and then holding down Ctrl while clicking another.  Similarly, using Shift instead of Ctrl will select all intermediate cells."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GOwlZRXEQSHZ"
      },
      "source": [
        "# Working with python\n",
        "Colaboratory is built on top of [Jupyter Notebook](https://jupyter.org/). Below are some examples of convenience functions provided."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RVuqWUXPQSHa"
      },
      "source": [
        "Long running python processes can be interrupted. Run the following cell and select **Runtime -> Interrupt execution** (*hotkey: Cmd/Ctrl-M I*) to stop execution."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "cellView": "both",
        "id": "d-S-3nYLQSHb"
      },
      "outputs": [],
      "source": [
        "# import time\n",
        "# print(\"Sleeping\")\n",
        "# time.sleep(30) # sleep for a while; interrupt me!\n",
        "# print(\"Done Sleeping\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wej_mEyXQSHc"
      },
      "source": [
        "## System aliases\n",
        "\n",
        "Jupyter includes shortcuts for common operations, such as ls:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "cellView": "both",
        "id": "5OCYEvK5QSHf"
      },
      "outputs": [],
      "source": [
        "# !ls /bin"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y8Da6JWKQSHh"
      },
      "source": [
        "That `!ls` probably generated a large output. You can select the cell and clear the output by either:\n",
        "\n",
        "1. Clicking on the clear output button (x) in the toolbar above the cell; or\n",
        "2. Right clicking the left gutter of the output area and selecting \"Clear output\" from the context menu.\n",
        "\n",
        "Execute any other process using `!` with string interpolation from python variables, and note the result can be assigned to a variable:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "cellView": "both",
        "id": "zqGrv0blQSHj"
      },
      "outputs": [],
      "source": [
        "# # In https://github.com/ipython/ipython/pull/10545, single quote strings are ignored\n",
        "# message = 'Colaboratory is great!'\n",
        "# foo = !unset message && echo -e '{message}\\n{message}\\n'$message\"\\n$message\"\n",
        "# foo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qM4myQGfQboQ"
      },
      "source": [
        "## Magics\n",
        "Colaboratory shares the notion of magics from Jupyter. There are shorthand annotations that change how a cell's text is executed. To learn more, see [Jupyter's magics page](http://nbviewer.jupyter.org/github/ipython/ipython/blob/1.x/examples/notebooks/Cell%20Magics.ipynb).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "cellView": "both",
        "id": "odfM-_GxWbCy"
      },
      "outputs": [],
      "source": [
        "# %%html\n",
        "# <marquee style='width: 30%; color: blue;'><b>Whee!</b></marquee>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "_YrTcK7k22Fp"
      },
      "outputs": [],
      "source": [
        "# %%html\n",
        "# <svg xmlns=\"http://www.w3.org/2000/svg\" viewBox=\"0 0 450 400\" width=\"200\" height=\"200\">\n",
        "#   <rect x=\"80\" y=\"60\" width=\"250\" height=\"250\" rx=\"20\" style=\"fill:red; stroke:black; fill-opacity:0.7\" />\n",
        "#   <rect x=\"180\" y=\"110\" width=\"250\" height=\"250\" rx=\"40\" style=\"fill:blue; stroke:black; fill-opacity:0.5;\" />\n",
        "# </svg>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d4L9TOP9QSHn"
      },
      "source": [
        "## Automatic completions and exploring code\n",
        "\n",
        "Colab provides automatic completions to explore attributes of Python objects, as well as to quickly view documentation strings. As an example, first run the following cell to import the  [`numpy`](http://www.numpy.org) module."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "cellView": "both",
        "id": "Q0JKWcmtQSHp"
      },
      "outputs": [],
      "source": [
        "# import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1M890-bXeyYp"
      },
      "source": [
        "If you now insert your cursor after `np` and press **Period**(`.`), you will see the list of available completions within the `np` module. Completions can be opened again by using **Ctrl+Space**."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "cellView": "both",
        "id": "j6QRIfUHQSHq"
      },
      "outputs": [],
      "source": [
        "# np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g6MfomFhQSHs"
      },
      "source": [
        "If you type an open parenthesis after any function or class in the module, you will see a pop-up of its documentation string:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "cellView": "both",
        "id": "SD0XnrVhQSHt"
      },
      "outputs": [],
      "source": [
        "# np.ndarray"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GVIVDgdaRjPQ"
      },
      "source": [
        "The documentation can be opened again using **Ctrl+Shift+Space** or you can view the documentation for method by mouse hovering over the method name.\n",
        "\n",
        "When hovering over the method name the `Open in tab` link will open the documentation in a persistent pane. The `View source` link will navigate to the source code for the method."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TYTBdJXxfqiJ"
      },
      "source": [
        "## Exception Formatting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4bqAVK-aQSHx"
      },
      "source": [
        "Exceptions are formatted nicely in Colab outputs:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "cellView": "both",
        "id": "CrJf1PEmQSHx"
      },
      "outputs": [],
      "source": [
        "# x = 1\n",
        "# y = 4\n",
        "# z = y/(1-x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7cRnhv_7N4Pa"
      },
      "source": [
        "## Rich, interactive outputs\n",
        "Until now all of the generated outputs have been text, but they can be more interesting, like the chart below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "JVXnTqyE9RET"
      },
      "outputs": [],
      "source": [
        "# import numpy as np\n",
        "# from matplotlib import pyplot as plt\n",
        "\n",
        "# ys = 200 + np.random.randn(100)\n",
        "# x = [x for x in range(len(ys))]\n",
        "\n",
        "# plt.plot(x, ys, '-')\n",
        "# plt.fill_between(x, ys, 195, where=(ys > 195), facecolor='g', alpha=0.6)\n",
        "\n",
        "# plt.title(\"Fills and Alpha Example\")\n",
        "# plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aro-UJgUQSH1"
      },
      "source": [
        "# Integration with Drive\n",
        "\n",
        "Colaboratory is integrated with Google Drive. It allows you to share, comment, and collaborate on the same document with multiple people:\n",
        "\n",
        "* The **SHARE** button (top-right of the toolbar) allows you to share the notebook and control permissions set on it.\n",
        "\n",
        "* **File->Make a Copy** creates a copy of the notebook in Drive.\n",
        "\n",
        "* **File->Save** saves the File to Drive. **File->Save and checkpoint** pins the version so it doesn't get deleted from the revision history.\n",
        "\n",
        "* **File->Revision history** shows the notebook's revision history."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4hfV37gxpP_c"
      },
      "source": [
        "## Commenting on a cell\n",
        "You can comment on a Colaboratory notebook like you would on a Google Document. Comments are attached to cells, and are displayed next to the cell they refer to. If you have **comment-only** permissions, you will see a comment button on the top right of the cell when you hover over it.\n",
        "\n",
        "If you have edit or comment permissions you can comment on a cell in one of three ways:\n",
        "\n",
        "1. Select a cell and click the comment button in the toolbar above the top-right corner of the cell.\n",
        "1. Right click a text cell and select **Add a comment** from the context menu.\n",
        "3. Use the shortcut **Ctrl+Shift+M** to add a comment to the currently selected cell.\n",
        "\n",
        "You can resolve and reply to comments, and you can target comments to specific collaborators by typing *+[email address]* (e.g., `+user@domain.com`). Addressed collaborators will be emailed.\n",
        "\n",
        "The Comment button in the top-right corner of the page shows all comments attached to the notebook."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import auth\n",
        "\n",
        "# Authenticate if using Cloud TPU resources outside the local Colab instance\n",
        "auth.authenticate_user()"
      ],
      "metadata": {
        "id": "4cKwPJ8u7_1e"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Install the specialized vLLM build for TPUs\n",
        "# !pip install vllm-tpu"
      ],
      "metadata": {
        "id": "jls3GpYl8LNi"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "help(LLM.__init__)"
      ],
      "metadata": {
        "id": "695b8bYY8xXF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!export MAX_MODEL_LEN=2048\n",
        "!export TP=1 # number of chips"
      ],
      "metadata": {
        "id": "E7qbI8GnJzJd"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# !VLLM_LOGGING_LEVEL=DEBUG VLLM_TRACE_FUNCTION=1 vllm serve meta-llama/Llama-3.1-8B-Instruct \\\n",
        "!vllm serve meta-llama/Llama-3.1-8B-Instruct \\\n",
        "    --seed 42 \\\n",
        "    --disable-log-requests \\\n",
        "    --no-enable-prefix-caching \\\n",
        "    --async-scheduling \\\n",
        "    --gpu-memory-utilization 0.98 \\\n",
        "    --max-num-batched-tokens 4096 \\\n",
        "    --max-num-seqs 128 \\\n",
        "    --tensor-parallel-size 1 \\\n",
        "    --max-model-len 4096"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6tnqHRbaMg4n",
        "outputId": "cbd97281-5b63-41ca-f024-27cc13f0e6b8"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR\u001b[0m \u001b[90m01-29 15:08:36\u001b[0m \u001b[90m[tpu_info.py:40]\u001b[0m Unable to poll TPU GCE Metadata. Got status code: 404 and content: \n",
            "\u001b[32mINFO\u001b[0m \u001b[90m01-29 15:08:36\u001b[0m \u001b[90m[__init__.py:59]\u001b[0m TPU info: node_name=None | tpu_type=v6e-1 | worker_id=0 | num_chips=1 | num_cores_per_chip=1\n",
            "/usr/local/lib/python3.12/dist-packages/jax/_src/cloud_tpu_init.py:93: UserWarning: Transparent hugepages are not enabled. TPU runtime startup and shutdown time should be significantly improved on TPU v5e and newer. If not already set, you may need to enable transparent hugepages in your VM image (sudo sh -c \"echo always > /sys/kernel/mm/transparent_hugepage/enabled\")\n",
            "  warnings.warn(\n",
            "WARNING:absl:Tensorflow library not found, tensorflow.io.gfile operations will use native shim calls. GCS paths (i.e. 'gs://...') cannot be accessed.\n",
            "\u001b[32mINFO\u001b[0m \u001b[90m01-29 15:08:39\u001b[0m \u001b[90m[importing.py:44]\u001b[0m Triton is installed but 0 active driver(s) found (expected 1). Disabling Triton to prevent runtime errors.\n",
            "\u001b[32mINFO\u001b[0m \u001b[90m01-29 15:08:39\u001b[0m \u001b[90m[importing.py:68]\u001b[0m Triton not installed or not compatible; certain GPU-related functions will not be available.\n",
            "\u001b[33mWARNING\u001b[0m \u001b[90m01-29 15:08:39\u001b[0m \u001b[90m[interface.py:221]\u001b[0m Failed to import from vllm._C: ModuleNotFoundError(\"No module named 'vllm._C'\")\n",
            "\u001b[32mINFO\u001b[0m \u001b[90m01-29 15:08:39\u001b[0m \u001b[90m[tpu_platform.py:81]\u001b[0m Automatically using fp8_e5m2 for FP8 KV cache on TPU v6e.\n",
            "\u001b[32mINFO\u001b[0m \u001b[90m01-29 15:08:39\u001b[0m \u001b[90m[tpu_platform.py:81]\u001b[0m Automatically using fp8_e5m2 for FP8 KV cache on TPU v6e.\n",
            "\u001b[32mINFO\u001b[0m \u001b[90m01-29 15:08:39\u001b[0m \u001b[90m[tpu_platform.py:81]\u001b[0m Automatically using fp8_e5m2 for FP8 KV cache on TPU v6e.\n",
            "\u001b[0;36m(APIServer pid=40714)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-29 15:08:40\u001b[0m \u001b[90m[api_server.py:1351]\u001b[0m vLLM API server version 0.13.3\n",
            "\u001b[0;36m(APIServer pid=40714)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-29 15:08:40\u001b[0m \u001b[90m[utils.py:253]\u001b[0m non-default args: {'model_tag': 'meta-llama/Llama-3.1-8B-Instruct', 'model': 'meta-llama/Llama-3.1-8B-Instruct', 'seed': 42, 'max_model_len': 4096, 'gpu_memory_utilization': 0.98, 'enable_prefix_caching': False, 'max_num_batched_tokens': 4096, 'max_num_seqs': 128, 'async_scheduling': True}\n",
            "\u001b[0;36m(APIServer pid=40714)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-29 15:08:41\u001b[0m \u001b[90m[model.py:514]\u001b[0m Resolved architecture: LlamaForCausalLM\n",
            "\u001b[0;36m(APIServer pid=40714)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-29 15:08:41\u001b[0m \u001b[90m[model.py:1661]\u001b[0m Using max model len 4096\n",
            "\u001b[0;36m(APIServer pid=40714)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-29 15:08:42\u001b[0m \u001b[90m[scheduler.py:230]\u001b[0m Chunked prefill is enabled with max_num_batched_tokens=4096.\n",
            "\u001b[0;36m(APIServer pid=40714)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-29 15:08:42\u001b[0m \u001b[90m[vllm.py:598]\u001b[0m Disabling NCCL for DP synchronization when using async scheduling.\n",
            "\u001b[0;36m(APIServer pid=40714)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-29 15:08:42\u001b[0m \u001b[90m[tpu_platform.py:119]\u001b[0m Initialized sharding configuration: ShardingConfigManager(total_devices=1, sharding_strategy=ShardingStrategy(tensor_parallelism=1, expert_parallelism=1, sequence_parallelism=1, data_parallelism=1, attention_data_parallelism=1), device_indexes=None)\n",
            "\u001b[0;36m(APIServer pid=40714)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-29 15:08:42\u001b[0m \u001b[90m[tpu_platform.py:171]\u001b[0m Force using UniProcExecutor for JAX on single host without pipeline parallelism.\n",
            "\u001b[0;36m(APIServer pid=40714)\u001b[0;0m \u001b[33mWARNING\u001b[0m \u001b[90m01-29 15:08:43\u001b[0m \u001b[90m[tpu_platform.py:214]\u001b[0m Pin memory is not supported on TPU.\n",
            "\u001b[31mERROR\u001b[0m \u001b[90m01-29 15:08:46\u001b[0m \u001b[90m[tpu_info.py:40]\u001b[0m Unable to poll TPU GCE Metadata. Got status code: 404 and content: \n",
            "\u001b[32mINFO\u001b[0m \u001b[90m01-29 15:08:46\u001b[0m \u001b[90m[__init__.py:59]\u001b[0m TPU info: node_name=None | tpu_type=v6e-1 | worker_id=0 | num_chips=1 | num_cores_per_chip=1\n",
            "/usr/local/lib/python3.12/dist-packages/jax/_src/cloud_tpu_init.py:93: UserWarning: Transparent hugepages are not enabled. TPU runtime startup and shutdown time should be significantly improved on TPU v5e and newer. If not already set, you may need to enable transparent hugepages in your VM image (sudo sh -c \"echo always > /sys/kernel/mm/transparent_hugepage/enabled\")\n",
            "  warnings.warn(\n",
            "WARNING:absl:Tensorflow library not found, tensorflow.io.gfile operations will use native shim calls. GCS paths (i.e. 'gs://...') cannot be accessed.\n",
            "\u001b[32mINFO\u001b[0m \u001b[90m01-29 15:08:48\u001b[0m \u001b[90m[importing.py:44]\u001b[0m Triton is installed but 0 active driver(s) found (expected 1). Disabling Triton to prevent runtime errors.\n",
            "\u001b[32mINFO\u001b[0m \u001b[90m01-29 15:08:48\u001b[0m \u001b[90m[importing.py:68]\u001b[0m Triton not installed or not compatible; certain GPU-related functions will not be available.\n",
            "\u001b[33mWARNING\u001b[0m \u001b[90m01-29 15:08:48\u001b[0m \u001b[90m[interface.py:221]\u001b[0m Failed to import from vllm._C: ModuleNotFoundError(\"No module named 'vllm._C'\")\n",
            "\u001b[32mINFO\u001b[0m \u001b[90m01-29 15:08:48\u001b[0m \u001b[90m[tpu_platform.py:81]\u001b[0m Automatically using fp8_e5m2 for FP8 KV cache on TPU v6e.\n",
            "\u001b[32mINFO\u001b[0m \u001b[90m01-29 15:08:48\u001b[0m \u001b[90m[tpu_platform.py:81]\u001b[0m Automatically using fp8_e5m2 for FP8 KV cache on TPU v6e.\n",
            "\u001b[32mINFO\u001b[0m \u001b[90m01-29 15:08:48\u001b[0m \u001b[90m[tpu_platform.py:81]\u001b[0m Automatically using fp8_e5m2 for FP8 KV cache on TPU v6e.\n",
            "\u001b[0;36m(EngineCore_DP0 pid=40853)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-29 15:08:50\u001b[0m \u001b[90m[core.py:93]\u001b[0m Initializing a V1 LLM engine (v0.13.3) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=None, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False), seed=42, served_model_name=meta-llama/Llama-3.1-8B-Instruct, enable_prefix_caching=False, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.DYNAMO_TRACE_ONCE: 2>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'openxla', 'custom_ops': ['all'], 'splitting_ops': [], 'compile_mm_encoder': False, 'compile_sizes': None, 'compile_ranges_split_points': [4096], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.NONE: 0>, 'cudagraph_num_of_warmups': 0, 'cudagraph_capture_sizes': None, 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': True, 'fuse_act_quant': True, 'fuse_attn_quant': False, 'eliminate_noops': True, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': None, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False}, 'local_cache_dir': None}\n",
            "\u001b[0;36m(EngineCore_DP0 pid=40853)\u001b[0;0m \u001b[33mWARNING\u001b[0m \u001b[90m01-29 15:08:50\u001b[0m \u001b[90m[tpu_platform.py:214]\u001b[0m Pin memory is not supported on TPU.\n",
            "\u001b[0;36m(EngineCore_DP0 pid=40853)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-29 15:08:52\u001b[0m \u001b[90m[parallel_state.py:1203]\u001b[0m world_size=1 rank=0 local_rank=0 distributed_init_method=file:///tmp/tmpb3due0c5 backend=gloo\n",
            "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
            "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
            "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
            "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
            "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
            "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
            "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
            "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
            "\u001b[0;36m(EngineCore_DP0 pid=40853)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-29 15:08:52\u001b[0m \u001b[90m[parallel_state.py:1411]\u001b[0m rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank 0\n",
            "\u001b[0;36m(EngineCore_DP0 pid=40853)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-29 15:08:53\u001b[0m \u001b[90m[tpu_runner.py:304]\u001b[0m Init mesh | mesh=Mesh('data': 1, 'model': 1, axis_types=(Auto, Auto))\n",
            "\u001b[0;36m(EngineCore_DP0 pid=40853)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-29 15:08:53\u001b[0m \u001b[90m[utils.py:94]\u001b[0m Prepared token paddings: [16, 32, 64, 128, 256, 512, 1024, 2048, 4096]\n",
            "\u001b[0;36m(EngineCore_DP0 pid=40853)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-29 15:08:53\u001b[0m \u001b[90m[utils.py:60]\u001b[0m Prepared request paddings: [8, 16, 32, 64, 128]\n",
            "\u001b[0;36m(EngineCore_DP0 pid=40853)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-29 15:08:53\u001b[0m \u001b[90m[compilation_manager.py:52]\u001b[0m Enabling JAX compile cache.\n",
            "\u001b[0;36m(EngineCore_DP0 pid=40853)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-29 15:08:53\u001b[0m \u001b[90m[tpu_worker.py:249]\u001b[0m Init worker | rank=0 | is_first_rank=True | is_last_rank=True | topology_order_id=0 | is_driver_worker=True | hbm=[(0.0, 31.25)]GiB |self.devices=[TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)] | total devices=[TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)] | local_devices=[TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)]\n",
            "\u001b[0;36m(EngineCore_DP0 pid=40853)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-29 15:08:53\u001b[0m \u001b[90m[model_loader.py:364]\u001b[0m Loading model with MODEL_IMPL_TYPE=auto\n",
            "\u001b[0;36m(EngineCore_DP0 pid=40853)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-29 15:08:53\u001b[0m \u001b[90m[model_loader.py:375]\u001b[0m Resolved MODEL_IMPL_TYPE 'auto' to 'flax_nnx'\n",
            "\u001b[0;36m(EngineCore_DP0 pid=40853)\u001b[0;0m WARNING:root:Duplicate op registration for aten.__and__\n",
            "\u001b[0;36m(EngineCore_DP0 pid=40853)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-29 15:08:54\u001b[0m \u001b[90m[weight_utils.py:140]\u001b[0m Downloading weights from HF meta-llama/Llama-3.1-8B-Instruct\n",
            "\u001b[0;36m(EngineCore_DP0 pid=40853)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-29 15:08:54\u001b[0m \u001b[90m[weight_utils.py:162]\u001b[0m Loading weights from /root/.cache/huggingface/hub/models--meta-llama--Llama-3.1-8B-Instruct/snapshots/0e9e39f249a16976918f6564b8830bc894c89659/model-00001-of-00004.safetensors\n",
            "\u001b[0;36m(EngineCore_DP0 pid=40853)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-29 15:08:54\u001b[0m \u001b[90m[weight_utils.py:162]\u001b[0m Loading weights from /root/.cache/huggingface/hub/models--meta-llama--Llama-3.1-8B-Instruct/snapshots/0e9e39f249a16976918f6564b8830bc894c89659/model-00002-of-00004.safetensors\n",
            "\u001b[0;36m(EngineCore_DP0 pid=40853)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-29 15:08:54\u001b[0m \u001b[90m[weight_utils.py:162]\u001b[0m Loading weights from /root/.cache/huggingface/hub/models--meta-llama--Llama-3.1-8B-Instruct/snapshots/0e9e39f249a16976918f6564b8830bc894c89659/model-00003-of-00004.safetensors\n",
            "\u001b[0;36m(EngineCore_DP0 pid=40853)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-29 15:08:54\u001b[0m \u001b[90m[weight_utils.py:162]\u001b[0m Loading weights from /root/.cache/huggingface/hub/models--meta-llama--Llama-3.1-8B-Instruct/snapshots/0e9e39f249a16976918f6564b8830bc894c89659/model-00004-of-00004.safetensors\n",
            "\u001b[0;36m(EngineCore_DP0 pid=40853)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-29 15:09:08\u001b[0m \u001b[90m[tpu_runner.py:536]\u001b[0m Init model | hbm=[(14.96, 31.25)]GiB\n",
            "\u001b[0;36m(EngineCore_DP0 pid=40853)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-29 15:09:08\u001b[0m \u001b[90m[tpu_worker.py:283]\u001b[0m Memory statistics | total_hbm_limit_gb=31.25GiB | total_hbm_limit_cap_gb=30.62GiB | total_hbm_used_gb=14.96GiB | total_hbm_avail_gb=15.66GiB\n",
            "\u001b[0;36m(EngineCore_DP0 pid=40853)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-29 15:09:08\u001b[0m \u001b[90m[kv_cache_utils.py:1291]\u001b[0m GPU KV cache size: 128,256 tokens\n",
            "\u001b[0;36m(EngineCore_DP0 pid=40853)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-29 15:09:08\u001b[0m \u001b[90m[kv_cache_utils.py:1296]\u001b[0m Maximum concurrency for 4,096 tokens per request: 31.31x\n",
            "\u001b[0;36m(EngineCore_DP0 pid=40853)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-29 15:09:08\u001b[0m \u001b[90m[compilation_manager.py:484]\u001b[0m Compiling sampling with different input shapes.\n",
            "\u001b[0;36m(EngineCore_DP0 pid=40853)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-29 15:09:08\u001b[0m \u001b[90m[compilation_manager.py:75]\u001b[0m Precompile worker0 sample --> {'num_reqs': 8, 'do_sampling': True}\n",
            "\u001b[0;36m(EngineCore_DP0 pid=40853)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-29 15:09:08\u001b[0m \u001b[90m[compilation_manager.py:85]\u001b[0m Compilation finished in 0.09 [secs].\n",
            "\u001b[0;36m(EngineCore_DP0 pid=40853)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-29 15:09:08\u001b[0m \u001b[90m[compilation_manager.py:75]\u001b[0m Precompile worker0 sample --> {'num_reqs': 8, 'do_sampling': True}\n",
            "\u001b[0;36m(EngineCore_DP0 pid=40853)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-29 15:09:08\u001b[0m \u001b[90m[compilation_manager.py:85]\u001b[0m Compilation finished in 0.07 [secs].\n",
            "\u001b[0;36m(EngineCore_DP0 pid=40853)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-29 15:09:08\u001b[0m \u001b[90m[compilation_manager.py:75]\u001b[0m Precompile worker0 sample --> {'num_reqs': 8, 'do_sampling': False}\n",
            "\u001b[0;36m(EngineCore_DP0 pid=40853)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-29 15:09:08\u001b[0m \u001b[90m[compilation_manager.py:85]\u001b[0m Compilation finished in 0.33 [secs].\n",
            "\u001b[0;36m(EngineCore_DP0 pid=40853)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-29 15:09:08\u001b[0m \u001b[90m[compilation_manager.py:75]\u001b[0m Precompile worker0 sample --> {'num_reqs': 8, 'do_sampling': False}\n",
            "\u001b[0;36m(EngineCore_DP0 pid=40853)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-29 15:09:08\u001b[0m \u001b[90m[compilation_manager.py:85]\u001b[0m Compilation finished in 0.32 [secs].\n",
            "\u001b[0;36m(EngineCore_DP0 pid=40853)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-29 15:09:08\u001b[0m \u001b[90m[compilation_manager.py:75]\u001b[0m Precompile worker0 sample --> {'num_reqs': 16, 'do_sampling': True}\n",
            "\u001b[0;36m(EngineCore_DP0 pid=40853)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-29 15:09:09\u001b[0m \u001b[90m[compilation_manager.py:85]\u001b[0m Compilation finished in 0.10 [secs].\n",
            "\u001b[0;36m(EngineCore_DP0 pid=40853)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-29 15:09:09\u001b[0m \u001b[90m[compilation_manager.py:75]\u001b[0m Precompile worker0 sample --> {'num_reqs': 16, 'do_sampling': True}\n",
            "\u001b[0;36m(EngineCore_DP0 pid=40853)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-29 15:09:09\u001b[0m \u001b[90m[compilation_manager.py:85]\u001b[0m Compilation finished in 0.07 [secs].\n",
            "\u001b[0;36m(EngineCore_DP0 pid=40853)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-29 15:09:09\u001b[0m \u001b[90m[compilation_manager.py:75]\u001b[0m Precompile worker0 sample --> {'num_reqs': 16, 'do_sampling': False}\n",
            "\u001b[0;36m(EngineCore_DP0 pid=40853)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-29 15:09:09\u001b[0m \u001b[90m[compilation_manager.py:85]\u001b[0m Compilation finished in 0.76 [secs].\n",
            "\u001b[0;36m(EngineCore_DP0 pid=40853)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-29 15:09:09\u001b[0m \u001b[90m[compilation_manager.py:75]\u001b[0m Precompile worker0 sample --> {'num_reqs': 16, 'do_sampling': False}\n",
            "\u001b[0;36m(EngineCore_DP0 pid=40853)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-29 15:09:10\u001b[0m \u001b[90m[compilation_manager.py:85]\u001b[0m Compilation finished in 0.76 [secs].\n",
            "\u001b[0;36m(EngineCore_DP0 pid=40853)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-29 15:09:10\u001b[0m \u001b[90m[compilation_manager.py:75]\u001b[0m Precompile worker0 sample --> {'num_reqs': 32, 'do_sampling': True}\n",
            "\u001b[0;36m(EngineCore_DP0 pid=40853)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-29 15:09:10\u001b[0m \u001b[90m[compilation_manager.py:85]\u001b[0m Compilation finished in 0.09 [secs].\n",
            "\u001b[0;36m(EngineCore_DP0 pid=40853)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-29 15:09:10\u001b[0m \u001b[90m[compilation_manager.py:75]\u001b[0m Precompile worker0 sample --> {'num_reqs': 32, 'do_sampling': True}\n",
            "\u001b[0;36m(EngineCore_DP0 pid=40853)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-29 15:09:10\u001b[0m \u001b[90m[compilation_manager.py:85]\u001b[0m Compilation finished in 0.07 [secs].\n",
            "\u001b[0;36m(EngineCore_DP0 pid=40853)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-29 15:09:10\u001b[0m \u001b[90m[compilation_manager.py:75]\u001b[0m Precompile worker0 sample --> {'num_reqs': 32, 'do_sampling': False}\n",
            "\u001b[0;36m(EngineCore_DP0 pid=40853)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-29 15:09:10\u001b[0m \u001b[90m[compilation_manager.py:85]\u001b[0m Compilation finished in 0.01 [secs].\n",
            "\u001b[0;36m(EngineCore_DP0 pid=40853)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-29 15:09:10\u001b[0m \u001b[90m[compilation_manager.py:75]\u001b[0m Precompile worker0 sample --> {'num_reqs': 32, 'do_sampling': False}\n",
            "\u001b[0;36m(EngineCore_DP0 pid=40853)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-29 15:09:10\u001b[0m \u001b[90m[compilation_manager.py:85]\u001b[0m Compilation finished in 0.01 [secs].\n",
            "\u001b[0;36m(EngineCore_DP0 pid=40853)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-29 15:09:10\u001b[0m \u001b[90m[compilation_manager.py:75]\u001b[0m Precompile worker0 sample --> {'num_reqs': 64, 'do_sampling': True}\n",
            "\u001b[0;36m(EngineCore_DP0 pid=40853)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-29 15:09:11\u001b[0m \u001b[90m[compilation_manager.py:85]\u001b[0m Compilation finished in 0.09 [secs].\n",
            "\u001b[0;36m(EngineCore_DP0 pid=40853)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-29 15:09:11\u001b[0m \u001b[90m[compilation_manager.py:75]\u001b[0m Precompile worker0 sample --> {'num_reqs': 64, 'do_sampling': True}\n",
            "\u001b[0;36m(EngineCore_DP0 pid=40853)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-29 15:09:11\u001b[0m \u001b[90m[compilation_manager.py:85]\u001b[0m Compilation finished in 0.07 [secs].\n",
            "\u001b[0;36m(EngineCore_DP0 pid=40853)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-29 15:09:11\u001b[0m \u001b[90m[compilation_manager.py:75]\u001b[0m Precompile worker0 sample --> {'num_reqs': 64, 'do_sampling': False}\n",
            "\u001b[0;36m(EngineCore_DP0 pid=40853)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-29 15:09:11\u001b[0m \u001b[90m[compilation_manager.py:85]\u001b[0m Compilation finished in 0.37 [secs].\n",
            "\u001b[0;36m(EngineCore_DP0 pid=40853)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-29 15:09:11\u001b[0m \u001b[90m[compilation_manager.py:75]\u001b[0m Precompile worker0 sample --> {'num_reqs': 64, 'do_sampling': False}\n",
            "\u001b[0;36m(EngineCore_DP0 pid=40853)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-29 15:09:11\u001b[0m \u001b[90m[compilation_manager.py:85]\u001b[0m Compilation finished in 0.37 [secs].\n",
            "\u001b[0;36m(EngineCore_DP0 pid=40853)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-29 15:09:11\u001b[0m \u001b[90m[compilation_manager.py:75]\u001b[0m Precompile worker0 sample --> {'num_reqs': 128, 'do_sampling': True}\n",
            "\u001b[0;36m(EngineCore_DP0 pid=40853)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-29 15:09:12\u001b[0m \u001b[90m[compilation_manager.py:85]\u001b[0m Compilation finished in 0.10 [secs].\n",
            "\u001b[0;36m(EngineCore_DP0 pid=40853)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-29 15:09:12\u001b[0m \u001b[90m[compilation_manager.py:75]\u001b[0m Precompile worker0 sample --> {'num_reqs': 128, 'do_sampling': True}\n",
            "\u001b[0;36m(EngineCore_DP0 pid=40853)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-29 15:09:12\u001b[0m \u001b[90m[compilation_manager.py:85]\u001b[0m Compilation finished in 0.08 [secs].\n",
            "\u001b[0;36m(EngineCore_DP0 pid=40853)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-29 15:09:12\u001b[0m \u001b[90m[compilation_manager.py:75]\u001b[0m Precompile worker0 sample --> {'num_reqs': 128, 'do_sampling': False}\n",
            "\u001b[0;36m(EngineCore_DP0 pid=40853)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-29 15:09:12\u001b[0m \u001b[90m[compilation_manager.py:85]\u001b[0m Compilation finished in 0.35 [secs].\n",
            "\u001b[0;36m(EngineCore_DP0 pid=40853)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-29 15:09:12\u001b[0m \u001b[90m[compilation_manager.py:75]\u001b[0m Precompile worker0 sample --> {'num_reqs': 128, 'do_sampling': False}\n",
            "\u001b[0;36m(EngineCore_DP0 pid=40853)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-29 15:09:12\u001b[0m \u001b[90m[compilation_manager.py:85]\u001b[0m Compilation finished in 0.35 [secs].\n",
            "\u001b[0;36m(EngineCore_DP0 pid=40853)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-29 15:09:12\u001b[0m \u001b[90m[compilation_manager.py:557]\u001b[0m Compiling gather_logprobs with different input shapes.\n",
            "\u001b[0;36m(EngineCore_DP0 pid=40853)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-29 15:09:12\u001b[0m \u001b[90m[compilation_manager.py:75]\u001b[0m Precompile worker0 gather_logprobs --> {'num_reqs': 8}\n",
            "\u001b[0;36m(EngineCore_DP0 pid=40853)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-29 15:09:13\u001b[0m \u001b[90m[compilation_manager.py:85]\u001b[0m Compilation finished in 0.46 [secs].\n",
            "\u001b[0;36m(EngineCore_DP0 pid=40853)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-29 15:09:13\u001b[0m \u001b[90m[compilation_manager.py:75]\u001b[0m Precompile worker0 gather_logprobs --> {'num_reqs': 16}\n",
            "\u001b[0;36m(EngineCore_DP0 pid=40853)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-29 15:09:13\u001b[0m \u001b[90m[compilation_manager.py:85]\u001b[0m Compilation finished in 0.48 [secs].\n",
            "\u001b[0;36m(EngineCore_DP0 pid=40853)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-29 15:09:13\u001b[0m \u001b[90m[compilation_manager.py:75]\u001b[0m Precompile worker0 gather_logprobs --> {'num_reqs': 32}\n",
            "\u001b[0;36m(EngineCore_DP0 pid=40853)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-29 15:09:13\u001b[0m \u001b[90m[compilation_manager.py:85]\u001b[0m Compilation finished in 0.03 [secs].\n",
            "\u001b[0;36m(EngineCore_DP0 pid=40853)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-29 15:09:13\u001b[0m \u001b[90m[compilation_manager.py:75]\u001b[0m Precompile worker0 gather_logprobs --> {'num_reqs': 64}\n",
            "\u001b[0;36m(EngineCore_DP0 pid=40853)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-29 15:09:13\u001b[0m \u001b[90m[compilation_manager.py:85]\u001b[0m Compilation finished in 0.03 [secs].\n",
            "\u001b[0;36m(EngineCore_DP0 pid=40853)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-29 15:09:13\u001b[0m \u001b[90m[compilation_manager.py:75]\u001b[0m Precompile worker0 gather_logprobs --> {'num_reqs': 128}\n",
            "\u001b[0;36m(EngineCore_DP0 pid=40853)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-29 15:09:13\u001b[0m \u001b[90m[compilation_manager.py:85]\u001b[0m Compilation finished in 0.03 [secs].\n",
            "\u001b[0;36m(EngineCore_DP0 pid=40853)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-29 15:09:15\u001b[0m \u001b[90m[kv_cache_manager.py:251]\u001b[0m Init kv-cache | num_layers=32 | shape=(num_blocks, (256, 8, 2, 128)) | num_blocks=[501, 501, 501, 501, 501, 501, 501, 501, 501, 501, 501, 501, 501, 501, 501, 501, 501, 501, 501, 501, 501, 501, 501, 501, 501, 501, 501, 501, 501, 501, 501, 501] | sharding=NamedSharding(mesh=Mesh('data': 1, 'model': 1, axis_types=(Auto, Auto)), spec=PartitionSpec('data', None, 'model'), memory_kind=device) | dtype=bfloat16 | hbm=[(30.64, 31.25)]Gb\n",
            "\u001b[0;36m(EngineCore_DP0 pid=40853)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-29 15:09:15\u001b[0m \u001b[90m[compilation_manager.py:90]\u001b[0m Precompile all the subgraphs with possible input shapes.\n",
            "\u001b[0;36m(EngineCore_DP0 pid=40853)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-29 15:09:15\u001b[0m \u001b[90m[compilation_manager.py:75]\u001b[0m Precompile worker0 backbone --> {'num_tokens': 16}\n",
            "\u001b[0;36m(EngineCore_DP0 pid=40853)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-29 15:09:15\u001b[0m \u001b[90m[tuned_block_sizes.py:4386]\u001b[0m RPA v3 kernel tuned block sizes for ('TPU v6e', 256, 'q_bfloat16_kv_bfloat16', 'q_head-32_kv_head-8_head-128', 'max_model_len-4096-sw-None'): bkv_p=8, bq=64\n",
            "\u001b[0;36m(EngineCore_DP0 pid=40853)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-29 15:09:17\u001b[0m \u001b[90m[compilation_manager.py:85]\u001b[0m Compilation finished in 1.98 [secs].\n",
            "\u001b[0;36m(EngineCore_DP0 pid=40853)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-29 15:09:17\u001b[0m \u001b[90m[compilation_manager.py:75]\u001b[0m Precompile worker0 backbone --> {'num_tokens': 32}\n",
            "\u001b[0;36m(EngineCore_DP0 pid=40853)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-29 15:09:19\u001b[0m \u001b[90m[compilation_manager.py:85]\u001b[0m Compilation finished in 1.47 [secs].\n",
            "\u001b[0;36m(EngineCore_DP0 pid=40853)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-29 15:09:19\u001b[0m \u001b[90m[compilation_manager.py:75]\u001b[0m Precompile worker0 backbone --> {'num_tokens': 64}\n",
            "\u001b[0;36m(EngineCore_DP0 pid=40853)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-29 15:09:21\u001b[0m \u001b[90m[compilation_manager.py:85]\u001b[0m Compilation finished in 1.94 [secs].\n",
            "\u001b[0;36m(EngineCore_DP0 pid=40853)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-29 15:09:21\u001b[0m \u001b[90m[compilation_manager.py:75]\u001b[0m Precompile worker0 backbone --> {'num_tokens': 128}\n",
            "\u001b[0;36m(EngineCore_DP0 pid=40853)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-29 15:09:22\u001b[0m \u001b[90m[compilation_manager.py:85]\u001b[0m Compilation finished in 1.47 [secs].\n",
            "\u001b[0;36m(EngineCore_DP0 pid=40853)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-29 15:09:22\u001b[0m \u001b[90m[compilation_manager.py:75]\u001b[0m Precompile worker0 backbone --> {'num_tokens': 256}\n",
            "\u001b[0;36m(EngineCore_DP0 pid=40853)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-29 15:09:24\u001b[0m \u001b[90m[compilation_manager.py:85]\u001b[0m Compilation finished in 1.46 [secs].\n",
            "\u001b[0;36m(EngineCore_DP0 pid=40853)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-29 15:09:24\u001b[0m \u001b[90m[compilation_manager.py:75]\u001b[0m Precompile worker0 backbone --> {'num_tokens': 512}\n",
            "\u001b[0;36m(EngineCore_DP0 pid=40853)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-29 15:09:26\u001b[0m \u001b[90m[compilation_manager.py:85]\u001b[0m Compilation finished in 2.04 [secs].\n",
            "\u001b[0;36m(EngineCore_DP0 pid=40853)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-29 15:09:26\u001b[0m \u001b[90m[compilation_manager.py:75]\u001b[0m Precompile worker0 backbone --> {'num_tokens': 1024}\n",
            "\u001b[0;36m(EngineCore_DP0 pid=40853)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-29 15:09:27\u001b[0m \u001b[90m[compilation_manager.py:85]\u001b[0m Compilation finished in 1.50 [secs].\n",
            "\u001b[0;36m(EngineCore_DP0 pid=40853)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-29 15:09:27\u001b[0m \u001b[90m[compilation_manager.py:75]\u001b[0m Precompile worker0 backbone --> {'num_tokens': 2048}\n",
            "\u001b[0;36m(EngineCore_DP0 pid=40853)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-29 15:09:29\u001b[0m \u001b[90m[compilation_manager.py:85]\u001b[0m Compilation finished in 1.55 [secs].\n",
            "\u001b[0;36m(EngineCore_DP0 pid=40853)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-29 15:09:29\u001b[0m \u001b[90m[compilation_manager.py:75]\u001b[0m Precompile worker0 backbone --> {'num_tokens': 4096}\n",
            "\u001b[0;36m(EngineCore_DP0 pid=40853)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-29 15:09:31\u001b[0m \u001b[90m[compilation_manager.py:85]\u001b[0m Compilation finished in 2.16 [secs].\n",
            "\u001b[0;36m(EngineCore_DP0 pid=40853)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-29 15:09:31\u001b[0m \u001b[90m[compilation_manager.py:75]\u001b[0m Precompile _substitute_placeholder_token_fn --> {'num_tokens': 16, 'num_reqs': 8}\n",
            "\u001b[0;36m(EngineCore_DP0 pid=40853)\u001b[0;0m /usr/local/lib/python3.12/dist-packages/jax/_src/interpreters/mlir.py:1276: UserWarning: Some donated buffers were not usable: int32[16], int32[16].\n",
            "\u001b[0;36m(EngineCore_DP0 pid=40853)\u001b[0;0m See an explanation at https://docs.jax.dev/en/latest/faq.html#buffer-donation.\n",
            "\u001b[0;36m(EngineCore_DP0 pid=40853)\u001b[0;0m   warnings.warn(\"Some donated buffers were not usable:\"\n",
            "\u001b[0;36m(EngineCore_DP0 pid=40853)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-29 15:09:31\u001b[0m \u001b[90m[compilation_manager.py:85]\u001b[0m Compilation finished in 0.05 [secs].\n",
            "\u001b[0;36m(EngineCore_DP0 pid=40853)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-29 15:09:31\u001b[0m \u001b[90m[compilation_manager.py:75]\u001b[0m Precompile _substitute_placeholder_token_fn --> {'num_tokens': 16, 'num_reqs': 16}\n",
            "\u001b[0;36m(EngineCore_DP0 pid=40853)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-29 15:09:31\u001b[0m \u001b[90m[compilation_manager.py:85]\u001b[0m Compilation finished in 0.05 [secs].\n",
            "\u001b[0;36m(EngineCore_DP0 pid=40853)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-29 15:09:31\u001b[0m \u001b[90m[compilation_manager.py:75]\u001b[0m Precompile _substitute_placeholder_token_fn --> {'num_tokens': 16, 'num_reqs': 32}\n",
            "\u001b[0;36m(EngineCore_DP0 pid=40853)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-29 15:09:31\u001b[0m \u001b[90m[compilation_manager.py:85]\u001b[0m Compilation finished in 0.05 [secs].\n",
            "\u001b[0;36m(EngineCore_DP0 pid=40853)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-29 15:09:31\u001b[0m \u001b[90m[compilation_manager.py:75]\u001b[0m Precompile _substitute_placeholder_token_fn --> {'num_tokens': 16, 'num_reqs': 64}\n",
            "\u001b[0;36m(EngineCore_DP0 pid=40853)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-29 15:09:31\u001b[0m \u001b[90m[compilation_manager.py:85]\u001b[0m Compilation finished in 0.05 [secs].\n",
            "\u001b[0;36m(EngineCore_DP0 pid=40853)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-29 15:09:31\u001b[0m \u001b[90m[compilation_manager.py:75]\u001b[0m Precompile _substitute_placeholder_token_fn --> {'num_tokens': 16, 'num_reqs': 128}\n",
            "\u001b[0;36m(EngineCore_DP0 pid=40853)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-29 15:09:31\u001b[0m \u001b[90m[compilation_manager.py:85]\u001b[0m Compilation finished in 0.05 [secs].\n",
            "\u001b[0;36m(EngineCore_DP0 pid=40853)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-29 15:09:31\u001b[0m \u001b[90m[compilation_manager.py:75]\u001b[0m Precompile _substitute_placeholder_token_fn --> {'num_tokens': 32, 'num_reqs': 8}\n",
            "\u001b[0;36m(EngineCore_DP0 pid=40853)\u001b[0;0m /usr/local/lib/python3.12/dist-packages/jax/_src/interpreters/mlir.py:1276: UserWarning: Some donated buffers were not usable: int32[32], int32[32].\n",
            "\u001b[0;36m(EngineCore_DP0 pid=40853)\u001b[0;0m See an explanation at https://docs.jax.dev/en/latest/faq.html#buffer-donation.\n",
            "\u001b[0;36m(EngineCore_DP0 pid=40853)\u001b[0;0m   warnings.warn(\"Some donated buffers were not usable:\"\n",
            "\u001b[0;36m(EngineCore_DP0 pid=40853)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-29 15:09:31\u001b[0m \u001b[90m[compilation_manager.py:85]\u001b[0m Compilation finished in 0.06 [secs].\n",
            "\u001b[0;36m(EngineCore_DP0 pid=40853)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-29 15:09:31\u001b[0m \u001b[90m[compilation_manager.py:75]\u001b[0m Precompile _substitute_placeholder_token_fn --> {'num_tokens': 32, 'num_reqs': 16}\n",
            "\u001b[0;36m(EngineCore_DP0 pid=40853)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-29 15:09:31\u001b[0m \u001b[90m[compilation_manager.py:85]\u001b[0m Compilation finished in 0.06 [secs].\n",
            "\u001b[0;36m(EngineCore_DP0 pid=40853)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-29 15:09:31\u001b[0m \u001b[90m[compilation_manager.py:75]\u001b[0m Precompile _substitute_placeholder_token_fn --> {'num_tokens': 32, 'num_reqs': 32}\n",
            "\u001b[0;36m(EngineCore_DP0 pid=40853)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-29 15:09:31\u001b[0m \u001b[90m[compilation_manager.py:85]\u001b[0m Compilation finished in 0.05 [secs].\n",
            "\u001b[0;36m(EngineCore_DP0 pid=40853)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-29 15:09:31\u001b[0m \u001b[90m[compilation_manager.py:75]\u001b[0m Precompile _substitute_placeholder_token_fn --> {'num_tokens': 32, 'num_reqs': 64}\n",
            "\u001b[0;36m(EngineCore_DP0 pid=40853)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-29 15:09:32\u001b[0m \u001b[90m[compilation_manager.py:85]\u001b[0m Compilation finished in 0.06 [secs].\n",
            "\u001b[0;36m(EngineCore_DP0 pid=40853)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-29 15:09:32\u001b[0m \u001b[90m[compilation_manager.py:75]\u001b[0m Precompile _substitute_placeholder_token_fn --> {'num_tokens': 32, 'num_reqs': 128}\n",
            "\u001b[0;36m(EngineCore_DP0 pid=40853)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-29 15:09:32\u001b[0m \u001b[90m[compilation_manager.py:85]\u001b[0m Compilation finished in 0.06 [secs].\n",
            "\u001b[0;36m(EngineCore_DP0 pid=40853)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-29 15:09:32\u001b[0m \u001b[90m[compilation_manager.py:75]\u001b[0m Precompile _substitute_placeholder_token_fn --> {'num_tokens': 64, 'num_reqs': 8}\n",
            "\u001b[0;36m(EngineCore_DP0 pid=40853)\u001b[0;0m /usr/local/lib/python3.12/dist-packages/jax/_src/interpreters/mlir.py:1276: UserWarning: Some donated buffers were not usable: int32[64], int32[64].\n",
            "\u001b[0;36m(EngineCore_DP0 pid=40853)\u001b[0;0m See an explanation at https://docs.jax.dev/en/latest/faq.html#buffer-donation.\n",
            "\u001b[0;36m(EngineCore_DP0 pid=40853)\u001b[0;0m   warnings.warn(\"Some donated buffers were not usable:\"\n",
            "\u001b[0;36m(EngineCore_DP0 pid=40853)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-29 15:09:32\u001b[0m \u001b[90m[compilation_manager.py:85]\u001b[0m Compilation finished in 0.08 [secs].\n",
            "\u001b[0;36m(EngineCore_DP0 pid=40853)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-29 15:09:32\u001b[0m \u001b[90m[compilation_manager.py:75]\u001b[0m Precompile _substitute_placeholder_token_fn --> {'num_tokens': 64, 'num_reqs': 16}\n",
            "\u001b[0;36m(EngineCore_DP0 pid=40853)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-29 15:09:32\u001b[0m \u001b[90m[compilation_manager.py:85]\u001b[0m Compilation finished in 0.08 [secs].\n",
            "\u001b[0;36m(EngineCore_DP0 pid=40853)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-29 15:09:32\u001b[0m \u001b[90m[compilation_manager.py:75]\u001b[0m Precompile _substitute_placeholder_token_fn --> {'num_tokens': 64, 'num_reqs': 32}\n",
            "\u001b[0;36m(EngineCore_DP0 pid=40853)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-29 15:09:32\u001b[0m \u001b[90m[compilation_manager.py:85]\u001b[0m Compilation finished in 0.07 [secs].\n",
            "\u001b[0;36m(EngineCore_DP0 pid=40853)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-29 15:09:32\u001b[0m \u001b[90m[compilation_manager.py:75]\u001b[0m Precompile _substitute_placeholder_token_fn --> {'num_tokens': 64, 'num_reqs': 64}\n",
            "\u001b[0;36m(EngineCore_DP0 pid=40853)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-29 15:09:32\u001b[0m \u001b[90m[compilation_manager.py:85]\u001b[0m Compilation finished in 0.07 [secs].\n",
            "\u001b[0;36m(EngineCore_DP0 pid=40853)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-29 15:09:32\u001b[0m \u001b[90m[compilation_manager.py:75]\u001b[0m Precompile _substitute_placeholder_token_fn --> {'num_tokens': 64, 'num_reqs': 128}\n",
            "\u001b[0;36m(EngineCore_DP0 pid=40853)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-29 15:09:32\u001b[0m \u001b[90m[compilation_manager.py:85]\u001b[0m Compilation finished in 0.07 [secs].\n",
            "\u001b[0;36m(EngineCore_DP0 pid=40853)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-29 15:09:32\u001b[0m \u001b[90m[compilation_manager.py:75]\u001b[0m Precompile _substitute_placeholder_token_fn --> {'num_tokens': 128, 'num_reqs': 8}\n",
            "\u001b[0;36m(EngineCore_DP0 pid=40853)\u001b[0;0m /usr/local/lib/python3.12/dist-packages/jax/_src/interpreters/mlir.py:1276: UserWarning: Some donated buffers were not usable: int32[128], int32[128].\n",
            "\u001b[0;36m(EngineCore_DP0 pid=40853)\u001b[0;0m See an explanation at https://docs.jax.dev/en/latest/faq.html#buffer-donation.\n",
            "\u001b[0;36m(EngineCore_DP0 pid=40853)\u001b[0;0m   warnings.warn(\"Some donated buffers were not usable:\"\n",
            "\u001b[0;36m(EngineCore_DP0 pid=40853)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-29 15:09:32\u001b[0m \u001b[90m[compilation_manager.py:85]\u001b[0m Compilation finished in 0.09 [secs].\n",
            "\u001b[0;36m(EngineCore_DP0 pid=40853)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-29 15:09:32\u001b[0m \u001b[90m[compilation_manager.py:75]\u001b[0m Precompile _substitute_placeholder_token_fn --> {'num_tokens': 128, 'num_reqs': 16}\n",
            "\u001b[0;36m(EngineCore_DP0 pid=40853)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-29 15:09:32\u001b[0m \u001b[90m[compilation_manager.py:85]\u001b[0m Compilation finished in 0.10 [secs].\n",
            "\u001b[0;36m(EngineCore_DP0 pid=40853)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-29 15:09:32\u001b[0m \u001b[90m[compilation_manager.py:75]\u001b[0m Precompile _substitute_placeholder_token_fn --> {'num_tokens': 128, 'num_reqs': 32}\n",
            "\u001b[0;36m(EngineCore_DP0 pid=40853)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-29 15:09:32\u001b[0m \u001b[90m[compilation_manager.py:85]\u001b[0m Compilation finished in 0.08 [secs].\n",
            "\u001b[0;36m(EngineCore_DP0 pid=40853)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-29 15:09:32\u001b[0m \u001b[90m[compilation_manager.py:75]\u001b[0m Precompile _substitute_placeholder_token_fn --> {'num_tokens': 128, 'num_reqs': 64}\n",
            "\u001b[0;36m(EngineCore_DP0 pid=40853)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-29 15:09:32\u001b[0m \u001b[90m[compilation_manager.py:85]\u001b[0m Compilation finished in 0.09 [secs].\n",
            "\u001b[0;36m(EngineCore_DP0 pid=40853)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-29 15:09:32\u001b[0m \u001b[90m[compilation_manager.py:75]\u001b[0m Precompile _substitute_placeholder_token_fn --> {'num_tokens': 128, 'num_reqs': 128}\n",
            "\u001b[0;36m(EngineCore_DP0 pid=40853)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-29 15:09:32\u001b[0m \u001b[90m[compilation_manager.py:85]\u001b[0m Compilation finished in 0.08 [secs].\n",
            "\u001b[0;36m(EngineCore_DP0 pid=40853)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-29 15:09:32\u001b[0m \u001b[90m[compilation_manager.py:75]\u001b[0m Precompile _substitute_placeholder_token_fn --> {'num_tokens': 256, 'num_reqs': 8}\n",
            "\u001b[0;36m(EngineCore_DP0 pid=40853)\u001b[0;0m /usr/local/lib/python3.12/dist-packages/jax/_src/interpreters/mlir.py:1276: UserWarning: Some donated buffers were not usable: int32[256], int32[256].\n",
            "\u001b[0;36m(EngineCore_DP0 pid=40853)\u001b[0;0m See an explanation at https://docs.jax.dev/en/latest/faq.html#buffer-donation.\n",
            "\u001b[0;36m(EngineCore_DP0 pid=40853)\u001b[0;0m   warnings.warn(\"Some donated buffers were not usable:\"\n",
            "\u001b[0;36m(EngineCore_DP0 pid=40853)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-29 15:09:33\u001b[0m \u001b[90m[compilation_manager.py:85]\u001b[0m Compilation finished in 0.10 [secs].\n",
            "\u001b[0;36m(EngineCore_DP0 pid=40853)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-29 15:09:33\u001b[0m \u001b[90m[compilation_manager.py:75]\u001b[0m Precompile _substitute_placeholder_token_fn --> {'num_tokens': 256, 'num_reqs': 16}\n",
            "\u001b[0;36m(EngineCore_DP0 pid=40853)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-29 15:09:33\u001b[0m \u001b[90m[compilation_manager.py:85]\u001b[0m Compilation finished in 0.11 [secs].\n",
            "\u001b[0;36m(EngineCore_DP0 pid=40853)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-29 15:09:33\u001b[0m \u001b[90m[compilation_manager.py:75]\u001b[0m Precompile _substitute_placeholder_token_fn --> {'num_tokens': 256, 'num_reqs': 32}\n",
            "\u001b[0;36m(EngineCore_DP0 pid=40853)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-29 15:09:33\u001b[0m \u001b[90m[compilation_manager.py:85]\u001b[0m Compilation finished in 0.14 [secs].\n",
            "\u001b[0;36m(EngineCore_DP0 pid=40853)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-29 15:09:33\u001b[0m \u001b[90m[compilation_manager.py:75]\u001b[0m Precompile _substitute_placeholder_token_fn --> {'num_tokens': 256, 'num_reqs': 64}\n",
            "\u001b[0;36m(EngineCore_DP0 pid=40853)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-29 15:09:33\u001b[0m \u001b[90m[compilation_manager.py:85]\u001b[0m Compilation finished in 0.09 [secs].\n",
            "\u001b[0;36m(EngineCore_DP0 pid=40853)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-29 15:09:33\u001b[0m \u001b[90m[compilation_manager.py:75]\u001b[0m Precompile _substitute_placeholder_token_fn --> {'num_tokens': 256, 'num_reqs': 128}\n",
            "\u001b[0;36m(EngineCore_DP0 pid=40853)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-29 15:09:33\u001b[0m \u001b[90m[compilation_manager.py:85]\u001b[0m Compilation finished in 0.09 [secs].\n",
            "\u001b[0;36m(EngineCore_DP0 pid=40853)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-29 15:09:33\u001b[0m \u001b[90m[compilation_manager.py:75]\u001b[0m Precompile _substitute_placeholder_token_fn --> {'num_tokens': 512, 'num_reqs': 8}\n",
            "\u001b[0;36m(EngineCore_DP0 pid=40853)\u001b[0;0m /usr/local/lib/python3.12/dist-packages/jax/_src/interpreters/mlir.py:1276: UserWarning: Some donated buffers were not usable: int32[512], int32[512].\n",
            "\u001b[0;36m(EngineCore_DP0 pid=40853)\u001b[0;0m See an explanation at https://docs.jax.dev/en/latest/faq.html#buffer-donation.\n",
            "\u001b[0;36m(EngineCore_DP0 pid=40853)\u001b[0;0m   warnings.warn(\"Some donated buffers were not usable:\"\n",
            "\u001b[0;36m(EngineCore_DP0 pid=40853)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-29 15:09:33\u001b[0m \u001b[90m[compilation_manager.py:85]\u001b[0m Compilation finished in 0.10 [secs].\n",
            "\u001b[0;36m(EngineCore_DP0 pid=40853)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-29 15:09:33\u001b[0m \u001b[90m[compilation_manager.py:75]\u001b[0m Precompile _substitute_placeholder_token_fn --> {'num_tokens': 512, 'num_reqs': 16}\n",
            "\u001b[0;36m(EngineCore_DP0 pid=40853)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-29 15:09:33\u001b[0m \u001b[90m[compilation_manager.py:85]\u001b[0m Compilation finished in 0.11 [secs].\n",
            "\u001b[0;36m(EngineCore_DP0 pid=40853)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-29 15:09:33\u001b[0m \u001b[90m[compilation_manager.py:75]\u001b[0m Precompile _substitute_placeholder_token_fn --> {'num_tokens': 512, 'num_reqs': 32}\n",
            "\u001b[0;36m(EngineCore_DP0 pid=40853)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-29 15:09:33\u001b[0m \u001b[90m[compilation_manager.py:85]\u001b[0m Compilation finished in 0.13 [secs].\n",
            "\u001b[0;36m(EngineCore_DP0 pid=40853)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-29 15:09:33\u001b[0m \u001b[90m[compilation_manager.py:75]\u001b[0m Precompile _substitute_placeholder_token_fn --> {'num_tokens': 512, 'num_reqs': 64}\n",
            "\u001b[0;36m(EngineCore_DP0 pid=40853)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-29 15:09:34\u001b[0m \u001b[90m[compilation_manager.py:85]\u001b[0m Compilation finished in 0.20 [secs].\n",
            "\u001b[0;36m(EngineCore_DP0 pid=40853)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-29 15:09:34\u001b[0m \u001b[90m[compilation_manager.py:75]\u001b[0m Precompile _substitute_placeholder_token_fn --> {'num_tokens': 512, 'num_reqs': 128}\n",
            "\u001b[0;36m(EngineCore_DP0 pid=40853)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-29 15:09:34\u001b[0m \u001b[90m[compilation_manager.py:85]\u001b[0m Compilation finished in 0.10 [secs].\n",
            "\u001b[0;36m(EngineCore_DP0 pid=40853)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-29 15:09:34\u001b[0m \u001b[90m[compilation_manager.py:75]\u001b[0m Precompile _substitute_placeholder_token_fn --> {'num_tokens': 1024, 'num_reqs': 8}\n",
            "\u001b[0;36m(EngineCore_DP0 pid=40853)\u001b[0;0m /usr/local/lib/python3.12/dist-packages/jax/_src/interpreters/mlir.py:1276: UserWarning: Some donated buffers were not usable: int32[1024], int32[1024].\n",
            "\u001b[0;36m(EngineCore_DP0 pid=40853)\u001b[0;0m See an explanation at https://docs.jax.dev/en/latest/faq.html#buffer-donation.\n",
            "\u001b[0;36m(EngineCore_DP0 pid=40853)\u001b[0;0m   warnings.warn(\"Some donated buffers were not usable:\"\n",
            "\u001b[0;36m(EngineCore_DP0 pid=40853)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-29 15:09:34\u001b[0m \u001b[90m[compilation_manager.py:85]\u001b[0m Compilation finished in 0.10 [secs].\n",
            "\u001b[0;36m(EngineCore_DP0 pid=40853)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-29 15:09:34\u001b[0m \u001b[90m[compilation_manager.py:75]\u001b[0m Precompile _substitute_placeholder_token_fn --> {'num_tokens': 1024, 'num_reqs': 16}\n",
            "\u001b[0;36m(EngineCore_DP0 pid=40853)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-29 15:09:34\u001b[0m \u001b[90m[compilation_manager.py:85]\u001b[0m Compilation finished in 0.11 [secs].\n",
            "\u001b[0;36m(EngineCore_DP0 pid=40853)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-29 15:09:34\u001b[0m \u001b[90m[compilation_manager.py:75]\u001b[0m Precompile _substitute_placeholder_token_fn --> {'num_tokens': 1024, 'num_reqs': 32}\n",
            "\u001b[0;36m(EngineCore_DP0 pid=40853)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-29 15:09:34\u001b[0m \u001b[90m[compilation_manager.py:85]\u001b[0m Compilation finished in 0.13 [secs].\n",
            "\u001b[0;36m(EngineCore_DP0 pid=40853)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-29 15:09:34\u001b[0m \u001b[90m[compilation_manager.py:75]\u001b[0m Precompile _substitute_placeholder_token_fn --> {'num_tokens': 1024, 'num_reqs': 64}\n",
            "\u001b[0;36m(EngineCore_DP0 pid=40853)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-29 15:09:34\u001b[0m \u001b[90m[compilation_manager.py:85]\u001b[0m Compilation finished in 0.21 [secs].\n",
            "\u001b[0;36m(EngineCore_DP0 pid=40853)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-29 15:09:34\u001b[0m \u001b[90m[compilation_manager.py:75]\u001b[0m Precompile _substitute_placeholder_token_fn --> {'num_tokens': 1024, 'num_reqs': 128}\n",
            "\u001b[0;36m(EngineCore_DP0 pid=40853)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-29 15:09:34\u001b[0m \u001b[90m[compilation_manager.py:85]\u001b[0m Compilation finished in 0.09 [secs].\n",
            "\u001b[0;36m(EngineCore_DP0 pid=40853)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-29 15:09:34\u001b[0m \u001b[90m[compilation_manager.py:75]\u001b[0m Precompile _substitute_placeholder_token_fn --> {'num_tokens': 2048, 'num_reqs': 8}\n",
            "\u001b[0;36m(EngineCore_DP0 pid=40853)\u001b[0;0m /usr/local/lib/python3.12/dist-packages/jax/_src/interpreters/mlir.py:1276: UserWarning: Some donated buffers were not usable: int32[2048], int32[2048].\n",
            "\u001b[0;36m(EngineCore_DP0 pid=40853)\u001b[0;0m See an explanation at https://docs.jax.dev/en/latest/faq.html#buffer-donation.\n",
            "\u001b[0;36m(EngineCore_DP0 pid=40853)\u001b[0;0m   warnings.warn(\"Some donated buffers were not usable:\"\n",
            "\u001b[0;36m(EngineCore_DP0 pid=40853)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-29 15:09:34\u001b[0m \u001b[90m[compilation_manager.py:85]\u001b[0m Compilation finished in 0.12 [secs].\n",
            "\u001b[0;36m(EngineCore_DP0 pid=40853)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-29 15:09:34\u001b[0m \u001b[90m[compilation_manager.py:75]\u001b[0m Precompile _substitute_placeholder_token_fn --> {'num_tokens': 2048, 'num_reqs': 16}\n",
            "\u001b[0;36m(EngineCore_DP0 pid=40853)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-29 15:09:34\u001b[0m \u001b[90m[compilation_manager.py:85]\u001b[0m Compilation finished in 0.11 [secs].\n",
            "\u001b[0;36m(EngineCore_DP0 pid=40853)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-29 15:09:34\u001b[0m \u001b[90m[compilation_manager.py:75]\u001b[0m Precompile _substitute_placeholder_token_fn --> {'num_tokens': 2048, 'num_reqs': 32}\n",
            "\u001b[0;36m(EngineCore_DP0 pid=40853)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-29 15:09:35\u001b[0m \u001b[90m[compilation_manager.py:85]\u001b[0m Compilation finished in 0.13 [secs].\n",
            "\u001b[0;36m(EngineCore_DP0 pid=40853)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-29 15:09:35\u001b[0m \u001b[90m[compilation_manager.py:75]\u001b[0m Precompile _substitute_placeholder_token_fn --> {'num_tokens': 2048, 'num_reqs': 64}\n",
            "\u001b[0;36m(EngineCore_DP0 pid=40853)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-29 15:09:35\u001b[0m \u001b[90m[compilation_manager.py:85]\u001b[0m Compilation finished in 0.20 [secs].\n",
            "\u001b[0;36m(EngineCore_DP0 pid=40853)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-29 15:09:35\u001b[0m \u001b[90m[compilation_manager.py:75]\u001b[0m Precompile _substitute_placeholder_token_fn --> {'num_tokens': 2048, 'num_reqs': 128}\n",
            "\u001b[0;36m(EngineCore_DP0 pid=40853)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-29 15:09:35\u001b[0m \u001b[90m[compilation_manager.py:85]\u001b[0m Compilation finished in 0.10 [secs].\n",
            "\u001b[0;36m(EngineCore_DP0 pid=40853)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-29 15:09:35\u001b[0m \u001b[90m[compilation_manager.py:75]\u001b[0m Precompile _substitute_placeholder_token_fn --> {'num_tokens': 4096, 'num_reqs': 8}\n",
            "\u001b[0;36m(EngineCore_DP0 pid=40853)\u001b[0;0m /usr/local/lib/python3.12/dist-packages/jax/_src/interpreters/mlir.py:1276: UserWarning: Some donated buffers were not usable: int32[4096], int32[4096].\n",
            "\u001b[0;36m(EngineCore_DP0 pid=40853)\u001b[0;0m See an explanation at https://docs.jax.dev/en/latest/faq.html#buffer-donation.\n",
            "\u001b[0;36m(EngineCore_DP0 pid=40853)\u001b[0;0m   warnings.warn(\"Some donated buffers were not usable:\"\n",
            "\u001b[0;36m(EngineCore_DP0 pid=40853)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-29 15:09:35\u001b[0m \u001b[90m[compilation_manager.py:85]\u001b[0m Compilation finished in 0.10 [secs].\n",
            "\u001b[0;36m(EngineCore_DP0 pid=40853)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-29 15:09:35\u001b[0m \u001b[90m[compilation_manager.py:75]\u001b[0m Precompile _substitute_placeholder_token_fn --> {'num_tokens': 4096, 'num_reqs': 16}\n",
            "\u001b[0;36m(EngineCore_DP0 pid=40853)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-29 15:09:35\u001b[0m \u001b[90m[compilation_manager.py:85]\u001b[0m Compilation finished in 0.11 [secs].\n",
            "\u001b[0;36m(EngineCore_DP0 pid=40853)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-29 15:09:35\u001b[0m \u001b[90m[compilation_manager.py:75]\u001b[0m Precompile _substitute_placeholder_token_fn --> {'num_tokens': 4096, 'num_reqs': 32}\n",
            "\u001b[0;36m(EngineCore_DP0 pid=40853)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-29 15:09:35\u001b[0m \u001b[90m[compilation_manager.py:85]\u001b[0m Compilation finished in 0.13 [secs].\n",
            "\u001b[0;36m(EngineCore_DP0 pid=40853)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-29 15:09:35\u001b[0m \u001b[90m[compilation_manager.py:75]\u001b[0m Precompile _substitute_placeholder_token_fn --> {'num_tokens': 4096, 'num_reqs': 64}\n",
            "\u001b[0;36m(EngineCore_DP0 pid=40853)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-29 15:09:35\u001b[0m \u001b[90m[compilation_manager.py:85]\u001b[0m Compilation finished in 0.20 [secs].\n",
            "\u001b[0;36m(EngineCore_DP0 pid=40853)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-29 15:09:35\u001b[0m \u001b[90m[compilation_manager.py:75]\u001b[0m Precompile _substitute_placeholder_token_fn --> {'num_tokens': 4096, 'num_reqs': 128}\n",
            "\u001b[0;36m(EngineCore_DP0 pid=40853)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-29 15:09:36\u001b[0m \u001b[90m[compilation_manager.py:85]\u001b[0m Compilation finished in 0.11 [secs].\n",
            "\u001b[0;36m(EngineCore_DP0 pid=40853)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-29 15:09:36\u001b[0m \u001b[90m[compilation_manager.py:418]\u001b[0m Compiling select_from_array with different input shapes.\n",
            "\u001b[0;36m(EngineCore_DP0 pid=40853)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-29 15:09:36\u001b[0m \u001b[90m[compilation_manager.py:397]\u001b[0m Compiling select_from_array for worker0 select all logits.\n",
            "\u001b[0;36m(EngineCore_DP0 pid=40853)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-29 15:09:36\u001b[0m \u001b[90m[compilation_manager.py:75]\u001b[0m Precompile select_from_array [worker0 select all logits] --> {'array_size': 16, 'index_size': 8}\n",
            "\u001b[0;36m(EngineCore_DP0 pid=40853)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-29 15:09:36\u001b[0m \u001b[90m[compilation_manager.py:85]\u001b[0m Compilation finished in 0.05 [secs].\n",
            "\u001b[0;36m(EngineCore_DP0 pid=40853)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-29 15:09:36\u001b[0m \u001b[90m[compilation_manager.py:75]\u001b[0m Precompile select_from_array [worker0 select all logits] --> {'array_size': 16, 'index_size': 16}\n",
            "\u001b[0;36m(EngineCore_DP0 pid=40853)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-29 15:09:36\u001b[0m \u001b[90m[compilation_manager.py:85]\u001b[0m Compilation finished in 0.07 [secs].\n",
            "\u001b[0;36m(EngineCore_DP0 pid=40853)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-29 15:09:36\u001b[0m \u001b[90m[compilation_manager.py:75]\u001b[0m Precompile select_from_array [worker0 select all logits] --> {'array_size': 32, 'index_size': 8}\n",
            "\u001b[0;36m(EngineCore_DP0 pid=40853)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-29 15:09:36\u001b[0m \u001b[90m[compilation_manager.py:85]\u001b[0m Compilation finished in 0.05 [secs].\n",
            "\u001b[0;36m(EngineCore_DP0 pid=40853)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-29 15:09:36\u001b[0m \u001b[90m[compilation_manager.py:75]\u001b[0m Precompile select_from_array [worker0 select all logits] --> {'array_size': 32, 'index_size': 16}\n",
            "\u001b[0;36m(EngineCore_DP0 pid=40853)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-29 15:09:36\u001b[0m \u001b[90m[compilation_manager.py:85]\u001b[0m Compilation finished in 0.07 [secs].\n",
            "\u001b[0;36m(EngineCore_DP0 pid=40853)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-29 15:09:36\u001b[0m \u001b[90m[compilation_manager.py:75]\u001b[0m Precompile select_from_array [worker0 select all logits] --> {'array_size': 32, 'index_size': 32}\n",
            "\u001b[0;36m(EngineCore_DP0 pid=40853)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-29 15:09:36\u001b[0m \u001b[90m[compilation_manager.py:85]\u001b[0m Compilation finished in 0.11 [secs].\n",
            "\u001b[0;36m(EngineCore_DP0 pid=40853)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-29 15:09:36\u001b[0m \u001b[90m[compilation_manager.py:75]\u001b[0m Precompile select_from_array [worker0 select all logits] --> {'array_size': 64, 'index_size': 8}\n",
            "\u001b[0;36m(EngineCore_DP0 pid=40853)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-29 15:09:36\u001b[0m \u001b[90m[compilation_manager.py:85]\u001b[0m Compilation finished in 0.06 [secs].\n",
            "\u001b[0;36m(EngineCore_DP0 pid=40853)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-29 15:09:36\u001b[0m \u001b[90m[compilation_manager.py:75]\u001b[0m Precompile select_from_array [worker0 select all logits] --> {'array_size': 64, 'index_size': 16}\n",
            "\u001b[0;36m(EngineCore_DP0 pid=40853)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-29 15:09:36\u001b[0m \u001b[90m[compilation_manager.py:85]\u001b[0m Compilation finished in 0.08 [secs].\n",
            "\u001b[0;36m(EngineCore_DP0 pid=40853)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-29 15:09:36\u001b[0m \u001b[90m[compilation_manager.py:75]\u001b[0m Precompile select_from_array [worker0 select all logits] --> {'array_size': 64, 'index_size': 32}\n",
            "\u001b[0;36m(EngineCore_DP0 pid=40853)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-29 15:09:36\u001b[0m \u001b[90m[compilation_manager.py:85]\u001b[0m Compilation finished in 0.12 [secs].\n",
            "\u001b[0;36m(EngineCore_DP0 pid=40853)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-29 15:09:36\u001b[0m \u001b[90m[compilation_manager.py:75]\u001b[0m Precompile select_from_array [worker0 select all logits] --> {'array_size': 64, 'index_size': 64}\n",
            "\u001b[0;36m(EngineCore_DP0 pid=40853)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-29 15:09:36\u001b[0m \u001b[90m[compilation_manager.py:85]\u001b[0m Compilation finished in 0.20 [secs].\n",
            "\u001b[0;36m(EngineCore_DP0 pid=40853)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-29 15:09:37\u001b[0m \u001b[90m[compilation_manager.py:75]\u001b[0m Precompile select_from_array [worker0 select all logits] --> {'array_size': 128, 'index_size': 8}\n",
            "\u001b[0;36m(EngineCore_DP0 pid=40853)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-29 15:09:37\u001b[0m \u001b[90m[compilation_manager.py:85]\u001b[0m Compilation finished in 0.08 [secs].\n",
            "\u001b[0;36m(EngineCore_DP0 pid=40853)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-29 15:09:37\u001b[0m \u001b[90m[compilation_manager.py:75]\u001b[0m Precompile select_from_array [worker0 select all logits] --> {'array_size': 128, 'index_size': 16}\n",
            "\u001b[0;36m(EngineCore_DP0 pid=40853)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-29 15:09:37\u001b[0m \u001b[90m[compilation_manager.py:85]\u001b[0m Compilation finished in 0.10 [secs].\n",
            "\u001b[0;36m(EngineCore_DP0 pid=40853)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-29 15:09:37\u001b[0m \u001b[90m[compilation_manager.py:75]\u001b[0m Precompile select_from_array [worker0 select all logits] --> {'array_size': 128, 'index_size': 32}\n",
            "\u001b[0;36m(EngineCore_DP0 pid=40853)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-29 15:09:37\u001b[0m \u001b[90m[compilation_manager.py:85]\u001b[0m Compilation finished in 0.14 [secs].\n",
            "\u001b[0;36m(EngineCore_DP0 pid=40853)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-29 15:09:37\u001b[0m \u001b[90m[compilation_manager.py:75]\u001b[0m Precompile select_from_array [worker0 select all logits] --> {'array_size': 128, 'index_size': 64}\n",
            "\u001b[0;36m(EngineCore_DP0 pid=40853)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-29 15:09:37\u001b[0m \u001b[90m[compilation_manager.py:85]\u001b[0m Compilation finished in 0.23 [secs].\n",
            "\u001b[0;36m(EngineCore_DP0 pid=40853)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-29 15:09:37\u001b[0m \u001b[90m[compilation_manager.py:75]\u001b[0m Precompile select_from_array [worker0 select all logits] --> {'array_size': 128, 'index_size': 128}\n",
            "\u001b[0;36m(EngineCore_DP0 pid=40853)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-29 15:09:37\u001b[0m \u001b[90m[compilation_manager.py:85]\u001b[0m Compilation finished in 0.39 [secs].\n",
            "\u001b[0;36m(EngineCore_DP0 pid=40853)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-29 15:09:37\u001b[0m \u001b[90m[compilation_manager.py:75]\u001b[0m Precompile select_from_array [worker0 select all logits] --> {'array_size': 256, 'index_size': 8}\n",
            "\u001b[0;36m(EngineCore_DP0 pid=40853)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-29 15:09:38\u001b[0m \u001b[90m[compilation_manager.py:85]\u001b[0m Compilation finished in 0.11 [secs].\n",
            "\u001b[0;36m(EngineCore_DP0 pid=40853)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-29 15:09:38\u001b[0m \u001b[90m[compilation_manager.py:75]\u001b[0m Precompile select_from_array [worker0 select all logits] --> {'array_size': 256, 'index_size': 16}\n",
            "\u001b[0;36m(EngineCore_DP0 pid=40853)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-29 15:09:38\u001b[0m \u001b[90m[compilation_manager.py:85]\u001b[0m Compilation finished in 0.13 [secs].\n",
            "\u001b[0;36m(EngineCore_DP0 pid=40853)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-29 15:09:38\u001b[0m \u001b[90m[compilation_manager.py:75]\u001b[0m Precompile select_from_array [worker0 select all logits] --> {'array_size': 256, 'index_size': 32}\n",
            "\u001b[0;36m(EngineCore_DP0 pid=40853)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-29 15:09:38\u001b[0m \u001b[90m[compilation_manager.py:85]\u001b[0m Compilation finished in 0.17 [secs].\n",
            "\u001b[0;36m(EngineCore_DP0 pid=40853)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-29 15:09:38\u001b[0m \u001b[90m[compilation_manager.py:75]\u001b[0m Precompile select_from_array [worker0 select all logits] --> {'array_size': 256, 'index_size': 64}\n",
            "\u001b[0;36m(EngineCore_DP0 pid=40853)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-29 15:09:38\u001b[0m \u001b[90m[compilation_manager.py:85]\u001b[0m Compilation finished in 0.25 [secs].\n",
            "\u001b[0;36m(EngineCore_DP0 pid=40853)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-29 15:09:38\u001b[0m \u001b[90m[compilation_manager.py:75]\u001b[0m Precompile select_from_array [worker0 select all logits] --> {'array_size': 256, 'index_size': 128}\n",
            "\u001b[0;36m(EngineCore_DP0 pid=40853)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-29 15:09:39\u001b[0m \u001b[90m[compilation_manager.py:85]\u001b[0m Compilation finished in 0.42 [secs].\n",
            "\u001b[0;36m(EngineCore_DP0 pid=40853)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-29 15:09:39\u001b[0m \u001b[90m[compilation_manager.py:75]\u001b[0m Precompile select_from_array [worker0 select all logits] --> {'array_size': 512, 'index_size': 8}\n",
            "\u001b[0;36m(EngineCore_DP0 pid=40853)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-29 15:09:39\u001b[0m \u001b[90m[compilation_manager.py:85]\u001b[0m Compilation finished in 0.18 [secs].\n",
            "\u001b[0;36m(EngineCore_DP0 pid=40853)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-29 15:09:39\u001b[0m \u001b[90m[compilation_manager.py:75]\u001b[0m Precompile select_from_array [worker0 select all logits] --> {'array_size': 512, 'index_size': 16}\n",
            "\u001b[0;36m(EngineCore_DP0 pid=40853)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-29 15:09:39\u001b[0m \u001b[90m[compilation_manager.py:85]\u001b[0m Compilation finished in 0.20 [secs].\n",
            "\u001b[0;36m(EngineCore_DP0 pid=40853)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-29 15:09:39\u001b[0m \u001b[90m[compilation_manager.py:75]\u001b[0m Precompile select_from_array [worker0 select all logits] --> {'array_size': 512, 'index_size': 32}\n",
            "\u001b[0;36m(EngineCore_DP0 pid=40853)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-29 15:09:39\u001b[0m \u001b[90m[compilation_manager.py:85]\u001b[0m Compilation finished in 0.24 [secs].\n",
            "\u001b[0;36m(EngineCore_DP0 pid=40853)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-29 15:09:39\u001b[0m \u001b[90m[compilation_manager.py:75]\u001b[0m Precompile select_from_array [worker0 select all logits] --> {'array_size': 512, 'index_size': 64}\n",
            "\u001b[0;36m(EngineCore_DP0 pid=40853)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-29 15:09:40\u001b[0m \u001b[90m[compilation_manager.py:85]\u001b[0m Compilation finished in 0.32 [secs].\n",
            "\u001b[0;36m(EngineCore_DP0 pid=40853)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-29 15:09:40\u001b[0m \u001b[90m[compilation_manager.py:75]\u001b[0m Precompile select_from_array [worker0 select all logits] --> {'array_size': 512, 'index_size': 128}\n",
            "\u001b[0;36m(EngineCore_DP0 pid=40853)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-29 15:09:40\u001b[0m \u001b[90m[compilation_manager.py:85]\u001b[0m Compilation finished in 0.52 [secs].\n",
            "\u001b[0;36m(EngineCore_DP0 pid=40853)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-29 15:09:40\u001b[0m \u001b[90m[compilation_manager.py:75]\u001b[0m Precompile select_from_array [worker0 select all logits] --> {'array_size': 1024, 'index_size': 8}\n",
            "\u001b[0;36m(EngineCore_DP0 pid=40853)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-29 15:09:40\u001b[0m \u001b[90m[compilation_manager.py:85]\u001b[0m Compilation finished in 0.32 [secs].\n",
            "\u001b[0;36m(EngineCore_DP0 pid=40853)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-29 15:09:40\u001b[0m \u001b[90m[compilation_manager.py:75]\u001b[0m Precompile select_from_array [worker0 select all logits] --> {'array_size': 1024, 'index_size': 16}\n",
            "\u001b[0;36m(EngineCore_DP0 pid=40853)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-29 15:09:41\u001b[0m \u001b[90m[compilation_manager.py:85]\u001b[0m Compilation finished in 0.34 [secs].\n",
            "\u001b[0;36m(EngineCore_DP0 pid=40853)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-29 15:09:41\u001b[0m \u001b[90m[compilation_manager.py:75]\u001b[0m Precompile select_from_array [worker0 select all logits] --> {'array_size': 1024, 'index_size': 32}\n",
            "\u001b[0;36m(EngineCore_DP0 pid=40853)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-29 15:09:41\u001b[0m \u001b[90m[compilation_manager.py:85]\u001b[0m Compilation finished in 0.38 [secs].\n",
            "\u001b[0;36m(EngineCore_DP0 pid=40853)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-29 15:09:41\u001b[0m \u001b[90m[compilation_manager.py:75]\u001b[0m Precompile select_from_array [worker0 select all logits] --> {'array_size': 1024, 'index_size': 64}\n",
            "\u001b[0;36m(EngineCore_DP0 pid=40853)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-29 15:09:42\u001b[0m \u001b[90m[compilation_manager.py:85]\u001b[0m Compilation finished in 0.47 [secs].\n",
            "\u001b[0;36m(EngineCore_DP0 pid=40853)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-29 15:09:42\u001b[0m \u001b[90m[compilation_manager.py:75]\u001b[0m Precompile select_from_array [worker0 select all logits] --> {'array_size': 1024, 'index_size': 128}\n",
            "\u001b[0;36m(EngineCore_DP0 pid=40853)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-29 15:09:42\u001b[0m \u001b[90m[compilation_manager.py:85]\u001b[0m Compilation finished in 0.65 [secs].\n",
            "\u001b[0;36m(EngineCore_DP0 pid=40853)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-29 15:09:42\u001b[0m \u001b[90m[compilation_manager.py:75]\u001b[0m Precompile select_from_array [worker0 select all logits] --> {'array_size': 2048, 'index_size': 8}\n",
            "\u001b[0;36m(EngineCore_DP0 pid=40853)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-29 15:09:42\u001b[0m \u001b[90m[compilation_manager.py:85]\u001b[0m Compilation finished in 0.03 [secs].\n",
            "\u001b[0;36m(EngineCore_DP0 pid=40853)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-29 15:09:42\u001b[0m \u001b[90m[compilation_manager.py:75]\u001b[0m Precompile select_from_array [worker0 select all logits] --> {'array_size': 2048, 'index_size': 16}\n",
            "\u001b[0;36m(EngineCore_DP0 pid=40853)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-29 15:09:42\u001b[0m \u001b[90m[compilation_manager.py:85]\u001b[0m Compilation finished in 0.04 [secs].\n",
            "\u001b[0;36m(EngineCore_DP0 pid=40853)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-29 15:09:42\u001b[0m \u001b[90m[compilation_manager.py:75]\u001b[0m Precompile select_from_array [worker0 select all logits] --> {'array_size': 2048, 'index_size': 32}\n",
            "\u001b[0;36m(EngineCore_DP0 pid=40853)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-29 15:09:42\u001b[0m \u001b[90m[compilation_manager.py:85]\u001b[0m Compilation finished in 0.05 [secs].\n",
            "\u001b[0;36m(EngineCore_DP0 pid=40853)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-29 15:09:42\u001b[0m \u001b[90m[compilation_manager.py:75]\u001b[0m Precompile select_from_array [worker0 select all logits] --> {'array_size': 2048, 'index_size': 64}\n",
            "\u001b[0;36m(EngineCore_DP0 pid=40853)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-29 15:09:42\u001b[0m \u001b[90m[compilation_manager.py:85]\u001b[0m Compilation finished in 0.07 [secs].\n",
            "\u001b[0;36m(EngineCore_DP0 pid=40853)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-29 15:09:42\u001b[0m \u001b[90m[compilation_manager.py:75]\u001b[0m Precompile select_from_array [worker0 select all logits] --> {'array_size': 2048, 'index_size': 128}\n",
            "\u001b[0;36m(EngineCore_DP0 pid=40853)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-29 15:09:43\u001b[0m \u001b[90m[compilation_manager.py:85]\u001b[0m Compilation finished in 0.12 [secs].\n",
            "\u001b[0;36m(EngineCore_DP0 pid=40853)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-29 15:09:43\u001b[0m \u001b[90m[compilation_manager.py:75]\u001b[0m Precompile select_from_array [worker0 select all logits] --> {'array_size': 4096, 'index_size': 8}\n",
            "\u001b[0;36m(EngineCore_DP0 pid=40853)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-29 15:09:43\u001b[0m \u001b[90m[compilation_manager.py:85]\u001b[0m Compilation finished in 0.03 [secs].\n",
            "\u001b[0;36m(EngineCore_DP0 pid=40853)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-29 15:09:43\u001b[0m \u001b[90m[compilation_manager.py:75]\u001b[0m Precompile select_from_array [worker0 select all logits] --> {'array_size': 4096, 'index_size': 16}\n",
            "\u001b[0;36m(EngineCore_DP0 pid=40853)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-29 15:09:43\u001b[0m \u001b[90m[compilation_manager.py:85]\u001b[0m Compilation finished in 0.04 [secs].\n",
            "\u001b[0;36m(EngineCore_DP0 pid=40853)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-29 15:09:43\u001b[0m \u001b[90m[compilation_manager.py:75]\u001b[0m Precompile select_from_array [worker0 select all logits] --> {'array_size': 4096, 'index_size': 32}\n",
            "\u001b[0;36m(EngineCore_DP0 pid=40853)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-29 15:09:43\u001b[0m \u001b[90m[compilation_manager.py:85]\u001b[0m Compilation finished in 0.05 [secs].\n",
            "\u001b[0;36m(EngineCore_DP0 pid=40853)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-29 15:09:43\u001b[0m \u001b[90m[compilation_manager.py:75]\u001b[0m Precompile select_from_array [worker0 select all logits] --> {'array_size': 4096, 'index_size': 64}\n",
            "\u001b[0;36m(EngineCore_DP0 pid=40853)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-29 15:09:43\u001b[0m \u001b[90m[compilation_manager.py:85]\u001b[0m Compilation finished in 0.07 [secs].\n",
            "\u001b[0;36m(EngineCore_DP0 pid=40853)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-29 15:09:43\u001b[0m \u001b[90m[compilation_manager.py:75]\u001b[0m Precompile select_from_array [worker0 select all logits] --> {'array_size': 4096, 'index_size': 128}\n",
            "\u001b[0;36m(EngineCore_DP0 pid=40853)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-29 15:09:43\u001b[0m \u001b[90m[compilation_manager.py:85]\u001b[0m Compilation finished in 0.12 [secs].\n",
            "\u001b[0;36m(EngineCore_DP0 pid=40853)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-29 15:09:43\u001b[0m \u001b[90m[compilation_manager.py:462]\u001b[0m Compiling compute_logits with different input shapes.\n",
            "\u001b[0;36m(EngineCore_DP0 pid=40853)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-29 15:09:43\u001b[0m \u001b[90m[compilation_manager.py:75]\u001b[0m Precompile worker0 compute_logits --> {'num_reqs': 8}\n",
            "\u001b[0;36m(EngineCore_DP0 pid=40853)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-29 15:09:43\u001b[0m \u001b[90m[compilation_manager.py:85]\u001b[0m Compilation finished in 0.25 [secs].\n",
            "\u001b[0;36m(EngineCore_DP0 pid=40853)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-29 15:09:43\u001b[0m \u001b[90m[compilation_manager.py:75]\u001b[0m Precompile worker0 compute_logits --> {'num_reqs': 16}\n",
            "\u001b[0;36m(EngineCore_DP0 pid=40853)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-29 15:09:43\u001b[0m \u001b[90m[compilation_manager.py:85]\u001b[0m Compilation finished in 0.28 [secs].\n",
            "\u001b[0;36m(EngineCore_DP0 pid=40853)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-29 15:09:43\u001b[0m \u001b[90m[compilation_manager.py:75]\u001b[0m Precompile worker0 compute_logits --> {'num_reqs': 32}\n",
            "\u001b[0;36m(EngineCore_DP0 pid=40853)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-29 15:09:44\u001b[0m \u001b[90m[compilation_manager.py:85]\u001b[0m Compilation finished in 0.37 [secs].\n",
            "\u001b[0;36m(EngineCore_DP0 pid=40853)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-29 15:09:44\u001b[0m \u001b[90m[compilation_manager.py:75]\u001b[0m Precompile worker0 compute_logits --> {'num_reqs': 64}\n",
            "\u001b[0;36m(EngineCore_DP0 pid=40853)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-29 15:09:44\u001b[0m \u001b[90m[compilation_manager.py:85]\u001b[0m Compilation finished in 0.35 [secs].\n",
            "\u001b[0;36m(EngineCore_DP0 pid=40853)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-29 15:09:44\u001b[0m \u001b[90m[compilation_manager.py:75]\u001b[0m Precompile worker0 compute_logits --> {'num_reqs': 128}\n",
            "\u001b[0;36m(EngineCore_DP0 pid=40853)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-29 15:09:45\u001b[0m \u001b[90m[compilation_manager.py:85]\u001b[0m Compilation finished in 0.46 [secs].\n",
            "\u001b[0;36m(EngineCore_DP0 pid=40853)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-29 15:09:45\u001b[0m \u001b[90m[compilation_manager.py:872]\u001b[0m Compiling structured_decoding with different input shapes.\n",
            "\u001b[0;36m(EngineCore_DP0 pid=40853)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-29 15:09:45\u001b[0m \u001b[90m[compilation_manager.py:75]\u001b[0m Precompile structured_decode --> {'num_reqs': 8}\n",
            "\u001b[0;36m(EngineCore_DP0 pid=40853)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-29 15:09:45\u001b[0m \u001b[90m[compilation_manager.py:85]\u001b[0m Compilation finished in 0.62 [secs].\n",
            "\u001b[0;36m(EngineCore_DP0 pid=40853)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-29 15:09:45\u001b[0m \u001b[90m[compilation_manager.py:75]\u001b[0m Precompile structured_decode --> {'num_reqs': 16}\n",
            "\u001b[0;36m(EngineCore_DP0 pid=40853)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-29 15:09:46\u001b[0m \u001b[90m[compilation_manager.py:85]\u001b[0m Compilation finished in 0.58 [secs].\n",
            "\u001b[0;36m(EngineCore_DP0 pid=40853)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-29 15:09:46\u001b[0m \u001b[90m[compilation_manager.py:75]\u001b[0m Precompile structured_decode --> {'num_reqs': 32}\n",
            "\u001b[0;36m(EngineCore_DP0 pid=40853)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-29 15:09:46\u001b[0m \u001b[90m[compilation_manager.py:85]\u001b[0m Compilation finished in 0.59 [secs].\n",
            "\u001b[0;36m(EngineCore_DP0 pid=40853)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-29 15:09:46\u001b[0m \u001b[90m[compilation_manager.py:75]\u001b[0m Precompile structured_decode --> {'num_reqs': 64}\n",
            "\u001b[0;36m(EngineCore_DP0 pid=40853)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-29 15:09:47\u001b[0m \u001b[90m[compilation_manager.py:85]\u001b[0m Compilation finished in 0.13 [secs].\n",
            "\u001b[0;36m(EngineCore_DP0 pid=40853)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-29 15:09:47\u001b[0m \u001b[90m[compilation_manager.py:75]\u001b[0m Precompile structured_decode --> {'num_reqs': 128}\n",
            "\u001b[0;36m(EngineCore_DP0 pid=40853)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-29 15:09:47\u001b[0m \u001b[90m[compilation_manager.py:85]\u001b[0m Compilation finished in 0.12 [secs].\n",
            "\u001b[0;36m(EngineCore_DP0 pid=40853)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-29 15:09:47\u001b[0m \u001b[90m[core.py:259]\u001b[0m init engine (profile, create kv cache, warmup model) took 39.13 seconds\n",
            "\u001b[0;36m(EngineCore_DP0 pid=40853)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-29 15:09:48\u001b[0m \u001b[90m[core.py:182]\u001b[0m Batch queue is enabled with size 2\n",
            "\u001b[0;36m(EngineCore_DP0 pid=40853)\u001b[0;0m \u001b[33mWARNING\u001b[0m \u001b[90m01-29 15:09:48\u001b[0m \u001b[90m[vllm.py:629]\u001b[0m Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.\n",
            "\u001b[0;36m(EngineCore_DP0 pid=40853)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-29 15:09:48\u001b[0m \u001b[90m[tpu_platform.py:119]\u001b[0m Initialized sharding configuration: ShardingConfigManager(total_devices=1, sharding_strategy=ShardingStrategy(tensor_parallelism=1, expert_parallelism=1, sequence_parallelism=1, data_parallelism=1, attention_data_parallelism=1), device_indexes=None)\n",
            "\u001b[0;36m(EngineCore_DP0 pid=40853)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-29 15:09:48\u001b[0m \u001b[90m[tpu_platform.py:171]\u001b[0m Force using UniProcExecutor for JAX on single host without pipeline parallelism.\n",
            "\u001b[0;36m(APIServer pid=40714)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-29 15:09:49\u001b[0m \u001b[90m[api_server.py:1099]\u001b[0m Supported tasks: ['generate']\n",
            "\u001b[0;36m(APIServer pid=40714)\u001b[0;0m \u001b[33mWARNING\u001b[0m \u001b[90m01-29 15:09:49\u001b[0m \u001b[90m[model.py:1487]\u001b[0m Default sampling parameters have been overridden by the model's Hugging Face generation config recommended from the model creator. If this is not intended, please relaunch vLLM instance with `--generation-config vllm`.\n",
            "\u001b[0;36m(APIServer pid=40714)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-29 15:09:49\u001b[0m \u001b[90m[serving_responses.py:201]\u001b[0m Using default chat sampling params from model: {'temperature': 0.6, 'top_p': 0.9}\n",
            "\u001b[0;36m(APIServer pid=40714)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-29 15:09:49\u001b[0m \u001b[90m[serving_chat.py:137]\u001b[0m Using default chat sampling params from model: {'temperature': 0.6, 'top_p': 0.9}\n",
            "\u001b[0;36m(APIServer pid=40714)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-29 15:09:49\u001b[0m \u001b[90m[serving_completion.py:77]\u001b[0m Using default completion sampling params from model: {'temperature': 0.6, 'top_p': 0.9}\n",
            "\u001b[0;36m(APIServer pid=40714)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-29 15:09:49\u001b[0m \u001b[90m[serving_chat.py:137]\u001b[0m Using default chat sampling params from model: {'temperature': 0.6, 'top_p': 0.9}\n",
            "\u001b[0;36m(APIServer pid=40714)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-29 15:09:49\u001b[0m \u001b[90m[api_server.py:1425]\u001b[0m Starting vLLM API server 0 on http://0.0.0.0:8000\n",
            "\u001b[0;36m(APIServer pid=40714)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-29 15:09:49\u001b[0m \u001b[90m[launcher.py:38]\u001b[0m Available routes are:\n",
            "\u001b[0;36m(APIServer pid=40714)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-29 15:09:49\u001b[0m \u001b[90m[launcher.py:46]\u001b[0m Route: /openapi.json, Methods: HEAD, GET\n",
            "\u001b[0;36m(APIServer pid=40714)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-29 15:09:49\u001b[0m \u001b[90m[launcher.py:46]\u001b[0m Route: /docs, Methods: HEAD, GET\n",
            "\u001b[0;36m(APIServer pid=40714)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-29 15:09:49\u001b[0m \u001b[90m[launcher.py:46]\u001b[0m Route: /docs/oauth2-redirect, Methods: HEAD, GET\n",
            "\u001b[0;36m(APIServer pid=40714)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-29 15:09:49\u001b[0m \u001b[90m[launcher.py:46]\u001b[0m Route: /redoc, Methods: HEAD, GET\n",
            "\u001b[0;36m(APIServer pid=40714)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-29 15:09:49\u001b[0m \u001b[90m[launcher.py:46]\u001b[0m Route: /scale_elastic_ep, Methods: POST\n",
            "\u001b[0;36m(APIServer pid=40714)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-29 15:09:49\u001b[0m \u001b[90m[launcher.py:46]\u001b[0m Route: /is_scaling_elastic_ep, Methods: POST\n",
            "\u001b[0;36m(APIServer pid=40714)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-29 15:09:49\u001b[0m \u001b[90m[launcher.py:46]\u001b[0m Route: /tokenize, Methods: POST\n",
            "\u001b[0;36m(APIServer pid=40714)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-29 15:09:49\u001b[0m \u001b[90m[launcher.py:46]\u001b[0m Route: /detokenize, Methods: POST\n",
            "\u001b[0;36m(APIServer pid=40714)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-29 15:09:49\u001b[0m \u001b[90m[launcher.py:46]\u001b[0m Route: /inference/v1/generate, Methods: POST\n",
            "\u001b[0;36m(APIServer pid=40714)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-29 15:09:49\u001b[0m \u001b[90m[launcher.py:46]\u001b[0m Route: /pause, Methods: POST\n",
            "\u001b[0;36m(APIServer pid=40714)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-29 15:09:49\u001b[0m \u001b[90m[launcher.py:46]\u001b[0m Route: /resume, Methods: POST\n",
            "\u001b[0;36m(APIServer pid=40714)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-29 15:09:49\u001b[0m \u001b[90m[launcher.py:46]\u001b[0m Route: /is_paused, Methods: GET\n",
            "\u001b[0;36m(APIServer pid=40714)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-29 15:09:49\u001b[0m \u001b[90m[launcher.py:46]\u001b[0m Route: /metrics, Methods: GET\n",
            "\u001b[0;36m(APIServer pid=40714)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-29 15:09:49\u001b[0m \u001b[90m[launcher.py:46]\u001b[0m Route: /health, Methods: GET\n",
            "\u001b[0;36m(APIServer pid=40714)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-29 15:09:49\u001b[0m \u001b[90m[launcher.py:46]\u001b[0m Route: /load, Methods: GET\n",
            "\u001b[0;36m(APIServer pid=40714)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-29 15:09:49\u001b[0m \u001b[90m[launcher.py:46]\u001b[0m Route: /v1/models, Methods: GET\n",
            "\u001b[0;36m(APIServer pid=40714)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-29 15:09:49\u001b[0m \u001b[90m[launcher.py:46]\u001b[0m Route: /version, Methods: GET\n",
            "\u001b[0;36m(APIServer pid=40714)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-29 15:09:49\u001b[0m \u001b[90m[launcher.py:46]\u001b[0m Route: /v1/responses, Methods: POST\n",
            "\u001b[0;36m(APIServer pid=40714)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-29 15:09:49\u001b[0m \u001b[90m[launcher.py:46]\u001b[0m Route: /v1/responses/{response_id}, Methods: GET\n",
            "\u001b[0;36m(APIServer pid=40714)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-29 15:09:49\u001b[0m \u001b[90m[launcher.py:46]\u001b[0m Route: /v1/responses/{response_id}/cancel, Methods: POST\n",
            "\u001b[0;36m(APIServer pid=40714)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-29 15:09:49\u001b[0m \u001b[90m[launcher.py:46]\u001b[0m Route: /v1/messages, Methods: POST\n",
            "\u001b[0;36m(APIServer pid=40714)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-29 15:09:49\u001b[0m \u001b[90m[launcher.py:46]\u001b[0m Route: /v1/chat/completions, Methods: POST\n",
            "\u001b[0;36m(APIServer pid=40714)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-29 15:09:49\u001b[0m \u001b[90m[launcher.py:46]\u001b[0m Route: /v1/completions, Methods: POST\n",
            "\u001b[0;36m(APIServer pid=40714)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-29 15:09:49\u001b[0m \u001b[90m[launcher.py:46]\u001b[0m Route: /v1/audio/transcriptions, Methods: POST\n",
            "\u001b[0;36m(APIServer pid=40714)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-29 15:09:49\u001b[0m \u001b[90m[launcher.py:46]\u001b[0m Route: /v1/audio/translations, Methods: POST\n",
            "\u001b[0;36m(APIServer pid=40714)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-29 15:09:49\u001b[0m \u001b[90m[launcher.py:46]\u001b[0m Route: /ping, Methods: GET\n",
            "\u001b[0;36m(APIServer pid=40714)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-29 15:09:49\u001b[0m \u001b[90m[launcher.py:46]\u001b[0m Route: /ping, Methods: POST\n",
            "\u001b[0;36m(APIServer pid=40714)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-29 15:09:49\u001b[0m \u001b[90m[launcher.py:46]\u001b[0m Route: /invocations, Methods: POST\n",
            "\u001b[0;36m(APIServer pid=40714)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-29 15:09:49\u001b[0m \u001b[90m[launcher.py:46]\u001b[0m Route: /classify, Methods: POST\n",
            "\u001b[0;36m(APIServer pid=40714)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-29 15:09:49\u001b[0m \u001b[90m[launcher.py:46]\u001b[0m Route: /v1/embeddings, Methods: POST\n",
            "\u001b[0;36m(APIServer pid=40714)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-29 15:09:49\u001b[0m \u001b[90m[launcher.py:46]\u001b[0m Route: /score, Methods: POST\n",
            "\u001b[0;36m(APIServer pid=40714)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-29 15:09:49\u001b[0m \u001b[90m[launcher.py:46]\u001b[0m Route: /v1/score, Methods: POST\n",
            "\u001b[0;36m(APIServer pid=40714)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-29 15:09:49\u001b[0m \u001b[90m[launcher.py:46]\u001b[0m Route: /rerank, Methods: POST\n",
            "\u001b[0;36m(APIServer pid=40714)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-29 15:09:49\u001b[0m \u001b[90m[launcher.py:46]\u001b[0m Route: /v1/rerank, Methods: POST\n",
            "\u001b[0;36m(APIServer pid=40714)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-29 15:09:49\u001b[0m \u001b[90m[launcher.py:46]\u001b[0m Route: /v2/rerank, Methods: POST\n",
            "\u001b[0;36m(APIServer pid=40714)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-29 15:09:49\u001b[0m \u001b[90m[launcher.py:46]\u001b[0m Route: /pooling, Methods: POST\n",
            "\u001b[0;36m(APIServer pid=40714)\u001b[0;0m \u001b[32mINFO\u001b[0m:     Started server process [\u001b[36m40714\u001b[0m]\n",
            "\u001b[0;36m(APIServer pid=40714)\u001b[0;0m \u001b[32mINFO\u001b[0m:     Waiting for application startup.\n",
            "\u001b[0;36m(APIServer pid=40714)\u001b[0;0m \u001b[32mINFO\u001b[0m:     Application startup complete.\n",
            "\u001b[0;36m(EngineCore_DP0 pid=40853)\u001b[0;0m \u001b[33mWARNING\u001b[0m \u001b[90m01-29 15:10:46\u001b[0m \u001b[90m[tpu_runner.py:709]\u001b[0m Should not schedule a request that does nothing!\n",
            "\u001b[0;36m(APIServer pid=40714)\u001b[0;0m \u001b[32mINFO\u001b[0m:     127.0.0.1:37702 - \"\u001b[1mPOST /v1/completions HTTP/1.1\u001b[0m\" \u001b[32m200 OK\u001b[0m\n",
            "\u001b[0;36m(APIServer pid=40714)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-29 15:10:49\u001b[0m \u001b[90m[loggers.py:248]\u001b[0m Engine 000: Avg prompt throughput: 36.0 tokens/s, Avg generation throughput: 5.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%\n",
            "\u001b[0;36m(APIServer pid=40714)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-29 15:10:59\u001b[0m \u001b[90m[loggers.py:248]\u001b[0m Engine 000: Avg prompt throughput: 0.0 tokens/s, Avg generation throughput: 0.0 tokens/s, Running: 0 reqs, Waiting: 0 reqs, GPU KV cache usage: 0.0%, Prefix cache hit rate: 0.0%\n",
            "\u001b[0;36m(APIServer pid=40714)\u001b[0;0m \u001b[32mINFO\u001b[0m \u001b[90m01-29 15:12:42\u001b[0m \u001b[90m[launcher.py:110]\u001b[0m Shutting down FastAPI HTTP server.\n",
            "\u001b[0;36m(APIServer pid=40714)\u001b[0;0m \u001b[32mINFO\u001b[0m:     Shutting down\n",
            "\u001b[0;36m(APIServer pid=40714)\u001b[0;0m \u001b[32mINFO\u001b[0m:     Waiting for application shutdown.\n",
            "\u001b[0;36m(APIServer pid=40714)\u001b[0;0m \u001b[32mINFO\u001b[0m:     Application shutdown complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import login\n",
        "from google.colab import userdata\n",
        "\n",
        "# Automatically retrieves the token from your Colab Secrets\n",
        "hf_token = userdata.get('HF_TOKEN')\n",
        "login(hf_token)"
      ],
      "metadata": {
        "id": "907ZzpRYMhuF"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!curl http://127.0.0.1:8000/ping"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "loamJTgVOAJ5",
        "outputId": "51c834f0-098e-4c3d-a22e-01535b87dead"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "curl: (7) Failed to connect to 127.0.0.1 port 8000 after 0 ms: Connection refused\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import sys\n",
        "import tempfile\n",
        "from vllm import LLM, SamplingParams\n",
        "\n",
        "# Set environment variables\n",
        "os.environ['VLLM_LOGGING_LEVEL'] = 'INFO'\n",
        "\n",
        "# Workaround for Jupyter/Colab: Create a wrapper for stdout that supports fileno()\n",
        "class StdoutWrapper:\n",
        "    \"\"\"Wrapper for sys.stdout that provides a fileno() method for Jupyter compatibility\"\"\"\n",
        "    def __init__(self, original_stdout):\n",
        "        self.original_stdout = original_stdout\n",
        "        self._temp_file = tempfile.TemporaryFile(mode='w+', buffering=1)\n",
        "\n",
        "    def fileno(self):\n",
        "        return self._temp_file.fileno()\n",
        "\n",
        "    def __getattr__(self, name):\n",
        "        return getattr(self.original_stdout, name)\n",
        "\n",
        "    def __del__(self):\n",
        "        try:\n",
        "            self._temp_file.close()\n",
        "        except:\n",
        "            pass\n",
        "\n",
        "# Patch stdout/stderr before vLLM initialization\n",
        "print(\" Applying Jupyter compatibility patches...\")\n",
        "original_stdout = sys.stdout\n",
        "original_stderr = sys.stderr\n",
        "\n",
        "sys.stdout = StdoutWrapper(original_stdout)\n",
        "sys.stderr = StdoutWrapper(original_stderr)\n",
        "\n",
        "try:\n",
        "    print(\"=\" * 80)\n",
        "    print(\"Initializing vLLM Engine\")\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "    print(f\"\\n Engine Configuration:\")\n",
        "    print(f\"  - Model: meta-llama/Llama-3.1-8B-Instruct\")\n",
        "    print(f\"  - Max Model Length: 4096 tokens\")\n",
        "    print(f\"  - Max Sequences: 128\")\n",
        "    print(f\"  - GPU Memory: 98%\")\n",
        "    print(\"\\n  Initializing engine (this may take 10-15 minutes for model compilation)...\\n\")\n",
        "\n",
        "    # Create the LLM engine\n",
        "    llm = LLM(\n",
        "        model=\"meta-llama/Llama-3.1-8B-Instruct\",\n",
        "        seed=42,\n",
        "        gpu_memory_utilization=0.98,\n",
        "        max_model_len=4096,\n",
        "        max_num_batched_tokens=4096,\n",
        "        max_num_seqs=128,\n",
        "        tensor_parallel_size=1,\n",
        "    )\n",
        "\n",
        "    print(\"\\n Engine initialized successfully!\")\n",
        "    print(\"\\n\" + \"=\" * 80)\n",
        "    print(\"Engine ready for inference!\")\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "finally:\n",
        "    # Restore original stdout/stderr\n",
        "    sys.stdout = original_stdout\n",
        "    sys.stderr = original_stderr\n",
        "\n",
        "# Example usage function\n",
        "def generate_text(prompt: str, temperature: float = 0.7, max_tokens: int = 2048):\n",
        "    \"\"\"Generate text using the vLLM engine\"\"\"\n",
        "    sampling_params = SamplingParams(\n",
        "        temperature=temperature,\n",
        "        max_tokens=max_tokens,\n",
        "    )\n",
        "\n",
        "    outputs = llm.generate([prompt], sampling_params)\n",
        "    return outputs[0].outputs[0].text\n",
        "\n",
        "print(\"\\n Example Usage:\")\n",
        "print(\"\"\"\n",
        "# Generate text directly\n",
        "prompt = \"Explain quantum computing in simple terms:\"\n",
        "result = generate_text(prompt, temperature=0.7, max_tokens=512)\n",
        "print(result)\n",
        "\"\"\")\n",
        "print(\"\\n Engine is ready! Use the generate_text() function above to generate text.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_jhWZFYNURsL",
        "outputId": "2fd5cf5e-cbd7-4275-f81b-ea9b85a4ea5a"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Applying Jupyter compatibility patches...\n",
            "================================================================================\n",
            "Initializing vLLM Engine\n",
            "================================================================================\n",
            "\n",
            " Engine Configuration:\n",
            "  - Model: meta-llama/Llama-3.1-8B-Instruct\n",
            "  - Max Model Length: 4096 tokens\n",
            "  - Max Sequences: 128\n",
            "  - GPU Memory: 98%\n",
            "\n",
            "  Initializing engine (this may take 10-15 minutes for model compilation)...\n",
            "\n",
            "INFO 01-29 16:04:44 [utils.py:253] non-default args: {'seed': 42, 'max_model_len': 4096, 'gpu_memory_utilization': 0.98, 'max_num_batched_tokens': 4096, 'max_num_seqs': 128, 'disable_log_stats': True, 'model': 'meta-llama/Llama-3.1-8B-Instruct'}\n",
            "INFO 01-29 16:04:44 [model.py:514] Resolved architecture: LlamaForCausalLM\n",
            "INFO 01-29 16:04:44 [model.py:1661] Using max model len 4096\n",
            "INFO 01-29 16:04:44 [scheduler.py:230] Chunked prefill is enabled with max_num_batched_tokens=4096.\n",
            "INFO 01-29 16:04:44 [tpu_platform.py:119] Initialized sharding configuration: ShardingConfigManager(total_devices=1, sharding_strategy=ShardingStrategy(tensor_parallelism=1, expert_parallelism=1, sequence_parallelism=1, data_parallelism=1, attention_data_parallelism=1), device_indexes=None)\n",
            "INFO 01-29 16:04:44 [tpu_platform.py:171] Force using UniProcExecutor for JAX on single host without pipeline parallelism.\n",
            "INFO 01-29 16:04:44 [core.py:93] Initializing a V1 LLM engine (v0.13.3) with config: model='meta-llama/Llama-3.1-8B-Instruct', speculative_config=None, tokenizer='meta-llama/Llama-3.1-8B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=4096, download_dir=None, load_format=auto, tensor_parallel_size=1, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=None, structured_outputs_config=StructuredOutputsConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_parser='', reasoning_parser_plugin='', enable_in_reasoning=False), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None, kv_cache_metrics=False, kv_cache_metrics_sample=0.01, cudagraph_metrics=False, enable_layerwise_nvtx_tracing=False), seed=42, served_model_name=meta-llama/Llama-3.1-8B-Instruct, enable_prefix_caching=True, enable_chunked_prefill=True, pooler_config=None, compilation_config={'level': None, 'mode': <CompilationMode.DYNAMO_TRACE_ONCE: 2>, 'debug_dump_path': None, 'cache_dir': '', 'compile_cache_save_format': 'binary', 'backend': 'openxla', 'custom_ops': ['all'], 'splitting_ops': [], 'compile_mm_encoder': False, 'compile_sizes': None, 'compile_ranges_split_points': [4096], 'inductor_compile_config': {'enable_auto_functionalized_v2': False, 'combo_kernels': True, 'benchmark_combo_kernel': True}, 'inductor_passes': {}, 'cudagraph_mode': <CUDAGraphMode.NONE: 0>, 'cudagraph_num_of_warmups': 0, 'cudagraph_capture_sizes': None, 'cudagraph_copy_inputs': False, 'cudagraph_specialize_lora': True, 'use_inductor_graph_partition': False, 'pass_config': {'fuse_norm_quant': True, 'fuse_act_quant': True, 'fuse_attn_quant': False, 'eliminate_noops': True, 'enable_sp': False, 'fuse_gemm_comms': False, 'fuse_allreduce_rms': False}, 'max_cudagraph_capture_size': None, 'dynamic_shapes_config': {'type': <DynamicShapesType.BACKED: 'backed'>, 'evaluate_guards': False}, 'local_cache_dir': None}\n",
            "WARNING 01-29 16:04:45 [interface.py:221] Failed to import from vllm._C: ModuleNotFoundError(\"No module named 'vllm._C'\")\n",
            "INFO 01-29 16:04:45 [tpu_platform.py:81] Automatically using fp8_e5m2 for FP8 KV cache on TPU v6e.\n",
            "INFO 01-29 16:04:45 [tpu_platform.py:81] Automatically using fp8_e5m2 for FP8 KV cache on TPU v6e.\n",
            "INFO 01-29 16:04:45 [tpu_platform.py:81] Automatically using fp8_e5m2 for FP8 KV cache on TPU v6e.\n",
            "INFO 01-29 16:04:47 [parallel_state.py:1203] world_size=1 rank=0 local_rank=0 distributed_init_method=file:///tmp/tmpmpual6lr backend=gloo\n",
            "INFO 01-29 16:04:47 [parallel_state.py:1411] rank 0 in world size 1 is assigned as DP rank 0, PP rank 0, PCP rank 0, TP rank 0, EP rank 0\n",
            "INFO 01-29 16:04:47 [tpu_runner.py:304] Init mesh | mesh=Mesh('data': 1, 'model': 1, axis_types=(Auto, Auto))\n",
            "INFO 01-29 16:04:47 [utils.py:94] Prepared token paddings: [16, 32, 64, 128, 256, 512, 1024, 2048, 4096]\n",
            "INFO 01-29 16:04:48 [utils.py:60] Prepared request paddings: [8, 16, 32, 64, 128]\n",
            "INFO 01-29 16:04:48 [compilation_manager.py:52] Enabling JAX compile cache.\n",
            "INFO 01-29 16:04:48 [tpu_worker.py:249] Init worker | rank=0 | is_first_rank=True | is_last_rank=True | topology_order_id=0 | is_driver_worker=True | hbm=[(0.0, 31.25)]GiB |self.devices=[TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)] | total devices=[TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)] | local_devices=[TpuDevice(id=0, process_index=0, coords=(0,0,0), core_on_chip=0)]\n",
            "INFO 01-29 16:04:48 [model_loader.py:364] Loading model with MODEL_IMPL_TYPE=auto\n",
            "INFO 01-29 16:04:48 [model_loader.py:375] Resolved MODEL_IMPL_TYPE 'auto' to 'flax_nnx'\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:root:Duplicate op registration for aten.__and__\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "INFO 01-29 16:04:48 [weight_utils.py:140] Downloading weights from HF meta-llama/Llama-3.1-8B-Instruct\n",
            "INFO 01-29 16:04:48 [weight_utils.py:162] Loading weights from /root/.cache/huggingface/hub/models--meta-llama--Llama-3.1-8B-Instruct/snapshots/0e9e39f249a16976918f6564b8830bc894c89659/model-00001-of-00004.safetensors\n",
            "INFO 01-29 16:04:48 [weight_utils.py:162] Loading weights from /root/.cache/huggingface/hub/models--meta-llama--Llama-3.1-8B-Instruct/snapshots/0e9e39f249a16976918f6564b8830bc894c89659/model-00002-of-00004.safetensors\n",
            "INFO 01-29 16:04:48 [weight_utils.py:162] Loading weights from /root/.cache/huggingface/hub/models--meta-llama--Llama-3.1-8B-Instruct/snapshots/0e9e39f249a16976918f6564b8830bc894c89659/model-00003-of-00004.safetensors\n",
            "INFO 01-29 16:04:48 [weight_utils.py:162] Loading weights from /root/.cache/huggingface/hub/models--meta-llama--Llama-3.1-8B-Instruct/snapshots/0e9e39f249a16976918f6564b8830bc894c89659/model-00004-of-00004.safetensors\n",
            "INFO 01-29 16:05:02 [tpu_runner.py:536] Init model | hbm=[(14.96, 31.25)]GiB\n",
            "INFO 01-29 16:05:02 [tpu_worker.py:283] Memory statistics | total_hbm_limit_gb=31.25GiB | total_hbm_limit_cap_gb=30.62GiB | total_hbm_used_gb=14.96GiB | total_hbm_avail_gb=15.66GiB\n",
            "INFO 01-29 16:05:02 [kv_cache_utils.py:1291] GPU KV cache size: 128,256 tokens\n",
            "INFO 01-29 16:05:02 [kv_cache_utils.py:1296] Maximum concurrency for 4,096 tokens per request: 31.31x\n",
            "INFO 01-29 16:05:02 [compilation_manager.py:484] Compiling sampling with different input shapes.\n",
            "INFO 01-29 16:05:02 [compilation_manager.py:75] Precompile worker0 sample --> {'num_reqs': 8, 'do_sampling': True}\n",
            "INFO 01-29 16:05:02 [compilation_manager.py:85] Compilation finished in 0.10 [secs].\n",
            "INFO 01-29 16:05:02 [compilation_manager.py:75] Precompile worker0 sample --> {'num_reqs': 8, 'do_sampling': True}\n",
            "INFO 01-29 16:05:02 [compilation_manager.py:85] Compilation finished in 0.07 [secs].\n",
            "INFO 01-29 16:05:02 [compilation_manager.py:75] Precompile worker0 sample --> {'num_reqs': 8, 'do_sampling': False}\n",
            "INFO 01-29 16:05:03 [compilation_manager.py:85] Compilation finished in 0.32 [secs].\n",
            "INFO 01-29 16:05:03 [compilation_manager.py:75] Precompile worker0 sample --> {'num_reqs': 8, 'do_sampling': False}\n",
            "INFO 01-29 16:05:03 [compilation_manager.py:85] Compilation finished in 0.32 [secs].\n",
            "INFO 01-29 16:05:03 [compilation_manager.py:75] Precompile worker0 sample --> {'num_reqs': 16, 'do_sampling': True}\n",
            "INFO 01-29 16:05:03 [compilation_manager.py:85] Compilation finished in 0.10 [secs].\n",
            "INFO 01-29 16:05:03 [compilation_manager.py:75] Precompile worker0 sample --> {'num_reqs': 16, 'do_sampling': True}\n",
            "INFO 01-29 16:05:03 [compilation_manager.py:85] Compilation finished in 0.08 [secs].\n",
            "INFO 01-29 16:05:03 [compilation_manager.py:75] Precompile worker0 sample --> {'num_reqs': 16, 'do_sampling': False}\n",
            "INFO 01-29 16:05:04 [compilation_manager.py:85] Compilation finished in 0.77 [secs].\n",
            "INFO 01-29 16:05:04 [compilation_manager.py:75] Precompile worker0 sample --> {'num_reqs': 16, 'do_sampling': False}\n",
            "INFO 01-29 16:05:05 [compilation_manager.py:85] Compilation finished in 0.74 [secs].\n",
            "INFO 01-29 16:05:05 [compilation_manager.py:75] Precompile worker0 sample --> {'num_reqs': 32, 'do_sampling': True}\n",
            "INFO 01-29 16:05:05 [compilation_manager.py:85] Compilation finished in 0.10 [secs].\n",
            "INFO 01-29 16:05:05 [compilation_manager.py:75] Precompile worker0 sample --> {'num_reqs': 32, 'do_sampling': True}\n",
            "INFO 01-29 16:05:05 [compilation_manager.py:85] Compilation finished in 0.07 [secs].\n",
            "INFO 01-29 16:05:05 [compilation_manager.py:75] Precompile worker0 sample --> {'num_reqs': 32, 'do_sampling': False}\n",
            "INFO 01-29 16:05:05 [compilation_manager.py:85] Compilation finished in 0.01 [secs].\n",
            "INFO 01-29 16:05:05 [compilation_manager.py:75] Precompile worker0 sample --> {'num_reqs': 32, 'do_sampling': False}\n",
            "INFO 01-29 16:05:05 [compilation_manager.py:85] Compilation finished in 0.01 [secs].\n",
            "INFO 01-29 16:05:05 [compilation_manager.py:75] Precompile worker0 sample --> {'num_reqs': 64, 'do_sampling': True}\n",
            "INFO 01-29 16:05:05 [compilation_manager.py:85] Compilation finished in 0.10 [secs].\n",
            "INFO 01-29 16:05:05 [compilation_manager.py:75] Precompile worker0 sample --> {'num_reqs': 64, 'do_sampling': True}\n",
            "INFO 01-29 16:05:05 [compilation_manager.py:85] Compilation finished in 0.07 [secs].\n",
            "INFO 01-29 16:05:05 [compilation_manager.py:75] Precompile worker0 sample --> {'num_reqs': 64, 'do_sampling': False}\n",
            "INFO 01-29 16:05:06 [compilation_manager.py:85] Compilation finished in 0.35 [secs].\n",
            "INFO 01-29 16:05:06 [compilation_manager.py:75] Precompile worker0 sample --> {'num_reqs': 64, 'do_sampling': False}\n",
            "INFO 01-29 16:05:06 [compilation_manager.py:85] Compilation finished in 0.36 [secs].\n",
            "INFO 01-29 16:05:06 [compilation_manager.py:75] Precompile worker0 sample --> {'num_reqs': 128, 'do_sampling': True}\n",
            "INFO 01-29 16:05:06 [compilation_manager.py:85] Compilation finished in 0.10 [secs].\n",
            "INFO 01-29 16:05:06 [compilation_manager.py:75] Precompile worker0 sample --> {'num_reqs': 128, 'do_sampling': True}\n",
            "INFO 01-29 16:05:06 [compilation_manager.py:85] Compilation finished in 0.08 [secs].\n",
            "INFO 01-29 16:05:06 [compilation_manager.py:75] Precompile worker0 sample --> {'num_reqs': 128, 'do_sampling': False}\n",
            "INFO 01-29 16:05:07 [compilation_manager.py:85] Compilation finished in 0.35 [secs].\n",
            "INFO 01-29 16:05:07 [compilation_manager.py:75] Precompile worker0 sample --> {'num_reqs': 128, 'do_sampling': False}\n",
            "INFO 01-29 16:05:07 [compilation_manager.py:85] Compilation finished in 0.37 [secs].\n",
            "INFO 01-29 16:05:07 [compilation_manager.py:557] Compiling gather_logprobs with different input shapes.\n",
            "INFO 01-29 16:05:07 [compilation_manager.py:75] Precompile worker0 gather_logprobs --> {'num_reqs': 8}\n",
            "INFO 01-29 16:05:07 [compilation_manager.py:85] Compilation finished in 0.47 [secs].\n",
            "INFO 01-29 16:05:07 [compilation_manager.py:75] Precompile worker0 gather_logprobs --> {'num_reqs': 16}\n",
            "INFO 01-29 16:05:08 [compilation_manager.py:85] Compilation finished in 0.48 [secs].\n",
            "INFO 01-29 16:05:08 [compilation_manager.py:75] Precompile worker0 gather_logprobs --> {'num_reqs': 32}\n",
            "INFO 01-29 16:05:08 [compilation_manager.py:85] Compilation finished in 0.03 [secs].\n",
            "INFO 01-29 16:05:08 [compilation_manager.py:75] Precompile worker0 gather_logprobs --> {'num_reqs': 64}\n",
            "INFO 01-29 16:05:08 [compilation_manager.py:85] Compilation finished in 0.03 [secs].\n",
            "INFO 01-29 16:05:08 [compilation_manager.py:75] Precompile worker0 gather_logprobs --> {'num_reqs': 128}\n",
            "INFO 01-29 16:05:08 [compilation_manager.py:85] Compilation finished in 0.03 [secs].\n",
            "INFO 01-29 16:05:10 [kv_cache_manager.py:251] Init kv-cache | num_layers=32 | shape=(num_blocks, (256, 8, 2, 128)) | num_blocks=[501, 501, 501, 501, 501, 501, 501, 501, 501, 501, 501, 501, 501, 501, 501, 501, 501, 501, 501, 501, 501, 501, 501, 501, 501, 501, 501, 501, 501, 501, 501, 501] | sharding=NamedSharding(mesh=Mesh('data': 1, 'model': 1, axis_types=(Auto, Auto)), spec=PartitionSpec('data', None, 'model'), memory_kind=device) | dtype=bfloat16 | hbm=[(30.64, 31.25)]Gb\n",
            "INFO 01-29 16:05:10 [compilation_manager.py:90] Precompile all the subgraphs with possible input shapes.\n",
            "INFO 01-29 16:05:10 [compilation_manager.py:75] Precompile worker0 backbone --> {'num_tokens': 16}\n",
            "INFO 01-29 16:05:10 [tuned_block_sizes.py:4386] RPA v3 kernel tuned block sizes for ('TPU v6e', 256, 'q_bfloat16_kv_bfloat16', 'q_head-32_kv_head-8_head-128', 'max_model_len-4096-sw-None'): bkv_p=8, bq=64\n",
            "INFO 01-29 16:05:12 [compilation_manager.py:85] Compilation finished in 1.54 [secs].\n",
            "INFO 01-29 16:05:12 [compilation_manager.py:75] Precompile worker0 backbone --> {'num_tokens': 32}\n",
            "INFO 01-29 16:05:13 [compilation_manager.py:85] Compilation finished in 1.91 [secs].\n",
            "INFO 01-29 16:05:13 [compilation_manager.py:75] Precompile worker0 backbone --> {'num_tokens': 64}\n",
            "INFO 01-29 16:05:15 [compilation_manager.py:85] Compilation finished in 1.44 [secs].\n",
            "INFO 01-29 16:05:15 [compilation_manager.py:75] Precompile worker0 backbone --> {'num_tokens': 128}\n",
            "INFO 01-29 16:05:16 [compilation_manager.py:85] Compilation finished in 1.45 [secs].\n",
            "INFO 01-29 16:05:16 [compilation_manager.py:75] Precompile worker0 backbone --> {'num_tokens': 256}\n",
            "INFO 01-29 16:05:18 [compilation_manager.py:85] Compilation finished in 1.92 [secs].\n",
            "INFO 01-29 16:05:18 [compilation_manager.py:75] Precompile worker0 backbone --> {'num_tokens': 512}\n",
            "INFO 01-29 16:05:20 [compilation_manager.py:85] Compilation finished in 1.46 [secs].\n",
            "INFO 01-29 16:05:20 [compilation_manager.py:75] Precompile worker0 backbone --> {'num_tokens': 1024}\n",
            "INFO 01-29 16:05:21 [compilation_manager.py:85] Compilation finished in 1.44 [secs].\n",
            "INFO 01-29 16:05:21 [compilation_manager.py:75] Precompile worker0 backbone --> {'num_tokens': 2048}\n",
            "INFO 01-29 16:05:23 [compilation_manager.py:85] Compilation finished in 2.09 [secs].\n",
            "INFO 01-29 16:05:23 [compilation_manager.py:75] Precompile worker0 backbone --> {'num_tokens': 4096}\n",
            "INFO 01-29 16:05:25 [compilation_manager.py:85] Compilation finished in 1.53 [secs].\n",
            "INFO 01-29 16:05:25 [compilation_manager.py:418] Compiling select_from_array with different input shapes.\n",
            "INFO 01-29 16:05:25 [compilation_manager.py:397] Compiling select_from_array for worker0 select all logits.\n",
            "INFO 01-29 16:05:25 [compilation_manager.py:75] Precompile select_from_array [worker0 select all logits] --> {'array_size': 16, 'index_size': 8}\n",
            "INFO 01-29 16:05:25 [compilation_manager.py:85] Compilation finished in 0.05 [secs].\n",
            "INFO 01-29 16:05:25 [compilation_manager.py:75] Precompile select_from_array [worker0 select all logits] --> {'array_size': 16, 'index_size': 16}\n",
            "INFO 01-29 16:05:25 [compilation_manager.py:85] Compilation finished in 0.07 [secs].\n",
            "INFO 01-29 16:05:25 [compilation_manager.py:75] Precompile select_from_array [worker0 select all logits] --> {'array_size': 32, 'index_size': 8}\n",
            "INFO 01-29 16:05:25 [compilation_manager.py:85] Compilation finished in 0.06 [secs].\n",
            "INFO 01-29 16:05:25 [compilation_manager.py:75] Precompile select_from_array [worker0 select all logits] --> {'array_size': 32, 'index_size': 16}\n",
            "INFO 01-29 16:05:25 [compilation_manager.py:85] Compilation finished in 0.08 [secs].\n",
            "INFO 01-29 16:05:25 [compilation_manager.py:75] Precompile select_from_array [worker0 select all logits] --> {'array_size': 32, 'index_size': 32}\n",
            "INFO 01-29 16:05:25 [compilation_manager.py:85] Compilation finished in 0.11 [secs].\n",
            "INFO 01-29 16:05:25 [compilation_manager.py:75] Precompile select_from_array [worker0 select all logits] --> {'array_size': 64, 'index_size': 8}\n",
            "INFO 01-29 16:05:25 [compilation_manager.py:85] Compilation finished in 0.06 [secs].\n",
            "INFO 01-29 16:05:25 [compilation_manager.py:75] Precompile select_from_array [worker0 select all logits] --> {'array_size': 64, 'index_size': 16}\n",
            "INFO 01-29 16:05:26 [compilation_manager.py:85] Compilation finished in 0.08 [secs].\n",
            "INFO 01-29 16:05:26 [compilation_manager.py:75] Precompile select_from_array [worker0 select all logits] --> {'array_size': 64, 'index_size': 32}\n",
            "INFO 01-29 16:05:26 [compilation_manager.py:85] Compilation finished in 0.12 [secs].\n",
            "INFO 01-29 16:05:26 [compilation_manager.py:75] Precompile select_from_array [worker0 select all logits] --> {'array_size': 64, 'index_size': 64}\n",
            "INFO 01-29 16:05:26 [compilation_manager.py:85] Compilation finished in 0.20 [secs].\n",
            "INFO 01-29 16:05:26 [compilation_manager.py:75] Precompile select_from_array [worker0 select all logits] --> {'array_size': 128, 'index_size': 8}\n",
            "INFO 01-29 16:05:26 [compilation_manager.py:85] Compilation finished in 0.08 [secs].\n",
            "INFO 01-29 16:05:26 [compilation_manager.py:75] Precompile select_from_array [worker0 select all logits] --> {'array_size': 128, 'index_size': 16}\n",
            "INFO 01-29 16:05:26 [compilation_manager.py:85] Compilation finished in 0.10 [secs].\n",
            "INFO 01-29 16:05:26 [compilation_manager.py:75] Precompile select_from_array [worker0 select all logits] --> {'array_size': 128, 'index_size': 32}\n",
            "INFO 01-29 16:05:26 [compilation_manager.py:85] Compilation finished in 0.14 [secs].\n",
            "INFO 01-29 16:05:26 [compilation_manager.py:75] Precompile select_from_array [worker0 select all logits] --> {'array_size': 128, 'index_size': 64}\n",
            "INFO 01-29 16:05:26 [compilation_manager.py:85] Compilation finished in 0.22 [secs].\n",
            "INFO 01-29 16:05:26 [compilation_manager.py:75] Precompile select_from_array [worker0 select all logits] --> {'array_size': 128, 'index_size': 128}\n",
            "INFO 01-29 16:05:27 [compilation_manager.py:85] Compilation finished in 0.39 [secs].\n",
            "INFO 01-29 16:05:27 [compilation_manager.py:75] Precompile select_from_array [worker0 select all logits] --> {'array_size': 256, 'index_size': 8}\n",
            "INFO 01-29 16:05:27 [compilation_manager.py:85] Compilation finished in 0.11 [secs].\n",
            "INFO 01-29 16:05:27 [compilation_manager.py:75] Precompile select_from_array [worker0 select all logits] --> {'array_size': 256, 'index_size': 16}\n",
            "INFO 01-29 16:05:27 [compilation_manager.py:85] Compilation finished in 0.13 [secs].\n",
            "INFO 01-29 16:05:27 [compilation_manager.py:75] Precompile select_from_array [worker0 select all logits] --> {'array_size': 256, 'index_size': 32}\n",
            "INFO 01-29 16:05:27 [compilation_manager.py:85] Compilation finished in 0.17 [secs].\n",
            "INFO 01-29 16:05:27 [compilation_manager.py:75] Precompile select_from_array [worker0 select all logits] --> {'array_size': 256, 'index_size': 64}\n",
            "INFO 01-29 16:05:28 [compilation_manager.py:85] Compilation finished in 0.25 [secs].\n",
            "INFO 01-29 16:05:28 [compilation_manager.py:75] Precompile select_from_array [worker0 select all logits] --> {'array_size': 256, 'index_size': 128}\n",
            "INFO 01-29 16:05:28 [compilation_manager.py:85] Compilation finished in 0.42 [secs].\n",
            "INFO 01-29 16:05:28 [compilation_manager.py:75] Precompile select_from_array [worker0 select all logits] --> {'array_size': 512, 'index_size': 8}\n",
            "INFO 01-29 16:05:28 [compilation_manager.py:85] Compilation finished in 0.18 [secs].\n",
            "INFO 01-29 16:05:28 [compilation_manager.py:75] Precompile select_from_array [worker0 select all logits] --> {'array_size': 512, 'index_size': 16}\n",
            "INFO 01-29 16:05:28 [compilation_manager.py:85] Compilation finished in 0.20 [secs].\n",
            "INFO 01-29 16:05:28 [compilation_manager.py:75] Precompile select_from_array [worker0 select all logits] --> {'array_size': 512, 'index_size': 32}\n",
            "INFO 01-29 16:05:29 [compilation_manager.py:85] Compilation finished in 0.23 [secs].\n",
            "INFO 01-29 16:05:29 [compilation_manager.py:75] Precompile select_from_array [worker0 select all logits] --> {'array_size': 512, 'index_size': 64}\n",
            "INFO 01-29 16:05:29 [compilation_manager.py:85] Compilation finished in 0.32 [secs].\n",
            "INFO 01-29 16:05:29 [compilation_manager.py:75] Precompile select_from_array [worker0 select all logits] --> {'array_size': 512, 'index_size': 128}\n",
            "INFO 01-29 16:05:29 [compilation_manager.py:85] Compilation finished in 0.49 [secs].\n",
            "INFO 01-29 16:05:29 [compilation_manager.py:75] Precompile select_from_array [worker0 select all logits] --> {'array_size': 1024, 'index_size': 8}\n",
            "INFO 01-29 16:05:30 [compilation_manager.py:85] Compilation finished in 0.31 [secs].\n",
            "INFO 01-29 16:05:30 [compilation_manager.py:75] Precompile select_from_array [worker0 select all logits] --> {'array_size': 1024, 'index_size': 16}\n",
            "INFO 01-29 16:05:30 [compilation_manager.py:85] Compilation finished in 0.34 [secs].\n",
            "INFO 01-29 16:05:30 [compilation_manager.py:75] Precompile select_from_array [worker0 select all logits] --> {'array_size': 1024, 'index_size': 32}\n",
            "INFO 01-29 16:05:30 [compilation_manager.py:85] Compilation finished in 0.38 [secs].\n",
            "INFO 01-29 16:05:30 [compilation_manager.py:75] Precompile select_from_array [worker0 select all logits] --> {'array_size': 1024, 'index_size': 64}\n",
            "INFO 01-29 16:05:31 [compilation_manager.py:85] Compilation finished in 0.47 [secs].\n",
            "INFO 01-29 16:05:31 [compilation_manager.py:75] Precompile select_from_array [worker0 select all logits] --> {'array_size': 1024, 'index_size': 128}\n",
            "INFO 01-29 16:05:32 [compilation_manager.py:85] Compilation finished in 0.67 [secs].\n",
            "INFO 01-29 16:05:32 [compilation_manager.py:75] Precompile select_from_array [worker0 select all logits] --> {'array_size': 2048, 'index_size': 8}\n",
            "INFO 01-29 16:05:32 [compilation_manager.py:85] Compilation finished in 0.03 [secs].\n",
            "INFO 01-29 16:05:32 [compilation_manager.py:75] Precompile select_from_array [worker0 select all logits] --> {'array_size': 2048, 'index_size': 16}\n",
            "INFO 01-29 16:05:32 [compilation_manager.py:85] Compilation finished in 0.04 [secs].\n",
            "INFO 01-29 16:05:32 [compilation_manager.py:75] Precompile select_from_array [worker0 select all logits] --> {'array_size': 2048, 'index_size': 32}\n",
            "INFO 01-29 16:05:32 [compilation_manager.py:85] Compilation finished in 0.05 [secs].\n",
            "INFO 01-29 16:05:32 [compilation_manager.py:75] Precompile select_from_array [worker0 select all logits] --> {'array_size': 2048, 'index_size': 64}\n",
            "INFO 01-29 16:05:32 [compilation_manager.py:85] Compilation finished in 0.07 [secs].\n",
            "INFO 01-29 16:05:32 [compilation_manager.py:75] Precompile select_from_array [worker0 select all logits] --> {'array_size': 2048, 'index_size': 128}\n",
            "INFO 01-29 16:05:32 [compilation_manager.py:85] Compilation finished in 0.12 [secs].\n",
            "INFO 01-29 16:05:32 [compilation_manager.py:75] Precompile select_from_array [worker0 select all logits] --> {'array_size': 4096, 'index_size': 8}\n",
            "INFO 01-29 16:05:32 [compilation_manager.py:85] Compilation finished in 0.03 [secs].\n",
            "INFO 01-29 16:05:32 [compilation_manager.py:75] Precompile select_from_array [worker0 select all logits] --> {'array_size': 4096, 'index_size': 16}\n",
            "INFO 01-29 16:05:32 [compilation_manager.py:85] Compilation finished in 0.04 [secs].\n",
            "INFO 01-29 16:05:32 [compilation_manager.py:75] Precompile select_from_array [worker0 select all logits] --> {'array_size': 4096, 'index_size': 32}\n",
            "INFO 01-29 16:05:32 [compilation_manager.py:85] Compilation finished in 0.05 [secs].\n",
            "INFO 01-29 16:05:32 [compilation_manager.py:75] Precompile select_from_array [worker0 select all logits] --> {'array_size': 4096, 'index_size': 64}\n",
            "INFO 01-29 16:05:32 [compilation_manager.py:85] Compilation finished in 0.07 [secs].\n",
            "INFO 01-29 16:05:32 [compilation_manager.py:75] Precompile select_from_array [worker0 select all logits] --> {'array_size': 4096, 'index_size': 128}\n",
            "INFO 01-29 16:05:32 [compilation_manager.py:85] Compilation finished in 0.12 [secs].\n",
            "INFO 01-29 16:05:32 [compilation_manager.py:462] Compiling compute_logits with different input shapes.\n",
            "INFO 01-29 16:05:32 [compilation_manager.py:75] Precompile worker0 compute_logits --> {'num_reqs': 8}\n",
            "INFO 01-29 16:05:33 [compilation_manager.py:85] Compilation finished in 0.25 [secs].\n",
            "INFO 01-29 16:05:33 [compilation_manager.py:75] Precompile worker0 compute_logits --> {'num_reqs': 16}\n",
            "INFO 01-29 16:05:33 [compilation_manager.py:85] Compilation finished in 0.27 [secs].\n",
            "INFO 01-29 16:05:33 [compilation_manager.py:75] Precompile worker0 compute_logits --> {'num_reqs': 32}\n",
            "INFO 01-29 16:05:33 [compilation_manager.py:85] Compilation finished in 0.35 [secs].\n",
            "INFO 01-29 16:05:33 [compilation_manager.py:75] Precompile worker0 compute_logits --> {'num_reqs': 64}\n",
            "INFO 01-29 16:05:34 [compilation_manager.py:85] Compilation finished in 0.33 [secs].\n",
            "INFO 01-29 16:05:34 [compilation_manager.py:75] Precompile worker0 compute_logits --> {'num_reqs': 128}\n",
            "INFO 01-29 16:05:34 [compilation_manager.py:85] Compilation finished in 0.45 [secs].\n",
            "INFO 01-29 16:05:34 [compilation_manager.py:872] Compiling structured_decoding with different input shapes.\n",
            "INFO 01-29 16:05:34 [compilation_manager.py:75] Precompile structured_decode --> {'num_reqs': 8}\n",
            "INFO 01-29 16:05:35 [compilation_manager.py:85] Compilation finished in 0.61 [secs].\n",
            "INFO 01-29 16:05:35 [compilation_manager.py:75] Precompile structured_decode --> {'num_reqs': 16}\n",
            "INFO 01-29 16:05:35 [compilation_manager.py:85] Compilation finished in 0.57 [secs].\n",
            "INFO 01-29 16:05:35 [compilation_manager.py:75] Precompile structured_decode --> {'num_reqs': 32}\n",
            "INFO 01-29 16:05:36 [compilation_manager.py:85] Compilation finished in 0.58 [secs].\n",
            "INFO 01-29 16:05:36 [compilation_manager.py:75] Precompile structured_decode --> {'num_reqs': 64}\n",
            "INFO 01-29 16:05:36 [compilation_manager.py:85] Compilation finished in 0.12 [secs].\n",
            "INFO 01-29 16:05:36 [compilation_manager.py:75] Precompile structured_decode --> {'num_reqs': 128}\n",
            "INFO 01-29 16:05:36 [compilation_manager.py:85] Compilation finished in 0.12 [secs].\n",
            "INFO 01-29 16:05:36 [core.py:259] init engine (profile, create kv cache, warmup model) took 33.81 seconds\n",
            "WARNING 01-29 16:05:37 [vllm.py:629] Inductor compilation was disabled by user settings,Optimizations settings that are only active duringInductor compilation will be ignored.\n",
            "INFO 01-29 16:05:37 [tpu_platform.py:119] Initialized sharding configuration: ShardingConfigManager(total_devices=1, sharding_strategy=ShardingStrategy(tensor_parallelism=1, expert_parallelism=1, sequence_parallelism=1, data_parallelism=1, attention_data_parallelism=1), device_indexes=None)\n",
            "INFO 01-29 16:05:37 [tpu_platform.py:171] Force using UniProcExecutor for JAX on single host without pipeline parallelism.\n",
            "INFO 01-29 16:05:37 [llm.py:360] Supported tasks: ['generate']\n",
            "\n",
            " Engine initialized successfully!\n",
            "\n",
            "================================================================================\n",
            "Engine ready for inference!\n",
            "================================================================================\n",
            "\n",
            " Example Usage:\n",
            "\n",
            "# Generate text directly\n",
            "prompt = \"Explain quantum computing in simple terms:\"\n",
            "result = generate_text(prompt, temperature=0.7, max_tokens=512)\n",
            "print(result)\n",
            "\n",
            "\n",
            " Engine is ready! Use the generate_text() function above to generate text.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"Explain quantum computing in simple terms. Respond in .md format.\"\n",
        "result = generate_text(prompt, temperature=0.7, max_tokens=4096)\n",
        "print(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "34ebb8bdb2e549c7990a8eec5a8de0ab",
            "03cefe698822472683ef1a7ac4cfaf2f",
            "6fbb496ecf4e4e09a7786a25518ad8f0",
            "695e27403b154b2f9b4eb4a986c7b657",
            "c8de21e2b60a41559440e0c3a3793650",
            "2a9096326b12418492d93e7059966d77",
            "ac07b9b93dfd4379a69d776892b31257",
            "d4da237f52704cdba2c0f06d8ff26c41",
            "acc83e05883244b7b27af18d8b506fbb",
            "b0ff6aeeb5e44e0c9c0b09b9669c8c45",
            "8a97549363f1442fb1f48a4700d1695f",
            "1f1a43f829ef40fa87671ce95306203f",
            "ef617c93316d4e3083bc0e8cd657e7b1",
            "f743585a80a04514acc70de83269e47b",
            "1639636c050144ce9313fd10d2ae083d",
            "9e3fcbe9310a47f19ae0bedf30f611b0",
            "fabba770b3514aaeb9eef4ac47bb0642",
            "7eca025033714bf584468d9ddd2492f9",
            "142ec051041b4cc28a204cf47de4b310",
            "c3dbccc00ca74e0e81d3e0303e132c27",
            "40d46fee804e42fb8994bb0d685fa719",
            "c979caa05e2549ce8c84a9ec96de9d7b"
          ]
        },
        "id": "zM_ux6NeYoRn",
        "outputId": "275af030-799e-40e1-c732-f65844ca7e99"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Adding requests:   0%|          | 0/1 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "34ebb8bdb2e549c7990a8eec5a8de0ab"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Processed prompts:   0%|          | 0/1 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1f1a43f829ef40fa87671ce95306203f"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " \n",
            "\n",
            "Quantum Computing\n",
            "================\n",
            "\n",
            "Quantum computing is a new way of processing information that uses the principles of quantum mechanics to perform calculations. Unlike classical computers, which use bits to store and process information, quantum computers use quantum bits or qubits.\n",
            "\n",
            "### What are qubits?\n",
            "\n",
            "Qubits are the fundamental units of quantum information. They can exist in multiple states at the same time, which allows for much faster processing of complex calculations. This is because qubits can represent not just 0 or 1, but also any superposition of 0 and 1.\n",
            "\n",
            "### How does it work?\n",
            "\n",
            "Imagine you have a coin that can either be heads or tails. A classical computer would represent this as a 0 or 1. But a qubit can represent both heads and tails at the same time, which is known as a superposition. This means that a qubit can process multiple possibilities simultaneously, making it much faster than a classical computer.\n",
            "\n",
            "### Quantum gates\n",
            "\n",
            "Quantum gates are the quantum equivalent of logic gates in classical computing. They are the basic building blocks of quantum algorithms and are used to manipulate qubits. Quantum gates can perform operations such as rotation, entanglement, and measurement.\n",
            "\n",
            "### Quantum algorithms\n",
            "\n",
            "Quantum algorithms are the programs that run on a quantum computer. They are designed to take advantage of the unique properties of qubits and quantum gates to solve complex problems. Some examples of quantum algorithms include Shor's algorithm for factoring large numbers and Grover's algorithm for searching an unsorted database.\n",
            "\n",
            "### Applications\n",
            "\n",
            "Quantum computing has many potential applications, including:\n",
            "\n",
            "*   **Cryptography**: Quantum computers can break many classical encryption algorithms, but they can also be used to create new, quantum-resistant encryption methods.\n",
            "*   **Optimization**: Quantum computers can be used to optimize complex systems, such as logistics and financial portfolios.\n",
            "*   **Simulation**: Quantum computers can simulate complex systems, such as molecules and materials, which can lead to breakthroughs in fields such as chemistry and materials science.\n",
            "*   **Machine learning**: Quantum computers can be used to speed up machine learning algorithms, which can lead to breakthroughs in fields such as image recognition and natural language processing.\n",
            "\n",
            "### Challenges\n",
            "\n",
            "Quantum computing is still a developing field, and there are many challenges that need to be overcome before it can be widely adopted. Some of these challenges include:\n",
            "\n",
            "*   **Error correction**: Quantum computers are prone to errors due to the fragile nature of qubits. Developing robust error correction methods is essential for large-scale quantum computing.\n",
            "*   **Scalability**: Currently, quantum computers are small-scale and can only perform a limited number of operations. Scaling up to larger systems is a significant challenge.\n",
            "*   **Quantum noise**: Quantum computers are sensitive to noise, which can cause errors and reduce the accuracy of calculations.\n",
            "\n",
            "### Conclusion\n",
            "\n",
            "Quantum computing is a new and exciting field that has the potential to revolutionize many areas of science and engineering. While there are many challenges that need to be overcome, the potential benefits of quantum computing make it an area worth exploring. As research and development continue, we can expect to see more breakthroughs and innovations in the field of quantum computing.  [^1]\n",
            "\n",
            "[^1]: For more information on quantum computing, see the resources listed below.\n",
            "\n",
            "### Resources\n",
            "\n",
            "*   **Quantum Computing for Everyone** by Michael A. Nielsen and Isaac L. Chuang\n",
            "*   **Quantum Computation and Quantum Information** by Michael A. Nielsen and Isaac L. Chuang\n",
            "*   **Quantum Computing: A Gentle Introduction** by Eleanor Rieffel and Wolfgang Polak\n",
            "*   **Quantum Computing for the Very Curious** by Michael A. Nielsen\n",
            "\n",
            "### License\n",
            "\n",
            "This work is licensed under a Creative Commons Attribution-ShareAlike 4.0 International License.  [^2]\n",
            "\n",
            "[^2]: See the LICENSE file for more information.  [^3]\n",
            "\n",
            "[^3]: This work is based on the following sources:\n",
            "\n",
            "*   **Quantum Computing for Everyone** by Michael A. Nielsen and Isaac L. Chuang\n",
            "*   **Quantum Computation and Quantum Information** by Michael A. Nielsen and Isaac L. Chuang\n",
            "*   **Quantum Computing: A Gentle Introduction** by Eleanor Rieffel and Wolfgang Polak\n",
            "*   **Quantum Computing for the Very Curious** by Michael A. Nielsen\n",
            "\n",
            "### Acknowledgments\n",
            "\n",
            "This work was supported by the National Science Foundation under grant number [insert grant number].  [^4]\n",
            "\n",
            "[^4]: See the ACKNOWLEDGMENTS file for more information.  [^5]\n",
            "\n",
            "[^5]: This work is dedicated to the memory of [insert name].  [^6]\n",
            "\n",
            "[^6]: See the DEDICATION file for more information.  [^7]\n",
            "\n",
            "[^7]: This work is a collaborative effort between [insert names].  [^8]\n",
            "\n",
            "[^8]: See the COLLABORATORS file for more information.  [^9]\n",
            "\n",
            "[^9]: This work is based on research conducted at [insert institution].  [^10]\n",
            "\n",
            "[^10]: See the INSTITUTION file for more information.  [^11]\n",
            "\n",
            "[^11]: This work is supported by [insert sponsor].  [^12]\n",
            "\n",
            "[^12]: See the SPONSORS file for more information.  [^13]\n",
            "\n",
            "[^13]: This work is a part of the [insert project].  [^14]\n",
            "\n",
            "[^14]: See the PROJECT file for more information.  [^15]\n",
            "\n",
            "[^15]: This work is a part of the [insert series].  [^16]\n",
            "\n",
            "[^16]: See the SERIES file for more information.  [^17]\n",
            "\n",
            "[^17]: This work is a part of the [insert collection].  [^18]\n",
            "\n",
            "[^18]: See the COLLECTION file for more information.  [^19]\n",
            "\n",
            "[^19]: This work is a part of the [insert library].  [^20]\n",
            "\n",
            "[^20]: See the LIBRARY file for more information.  [^21]\n",
            "\n",
            "[^21]: This work is a part of the [insert repository].  [^22]\n",
            "\n",
            "[^22]: See the REPOSITORY file for more information.  [^23]\n",
            "\n",
            "[^23]: This work is a part of the [insert archive].  [^24]\n",
            "\n",
            "[^24]: See the ARCHIVE file for more information.  [^25]\n",
            "\n",
            "[^25]: This work is a part of the [insert database].  [^26]\n",
            "\n",
            "[^26]: See the DATABASE file for more information.  [^27]\n",
            "\n",
            "[^27]: This work is a part of the [insert catalog].  [^28]\n",
            "\n",
            "[^28]: See the CATALOG file for more information.  [^29]\n",
            "\n",
            "[^29]: This work is a part of the [insert index].  [^30]\n",
            "\n",
            "[^30]: See the INDEX file for more information.  [^31]\n",
            "\n",
            "[^31]: This work is a part of the [insert glossary].  [^32]\n",
            "\n",
            "[^32]: See the GLOSSARY file for more information.  [^33]\n",
            "\n",
            "[^33]: This work is a part of the [insert dictionary].  [^34]\n",
            "\n",
            "[^34]: See the DICTIONARY file for more information.  [^35]\n",
            "\n",
            "[^35]: This work is a part of the [insert encyclopedia].  [^36]\n",
            "\n",
            "[^36]: See the ENCYCLOPEDIA file for more information.  [^37]\n",
            "\n",
            "[^37]: This work is a part of the [insert thesaurus].  [^38]\n",
            "\n",
            "[^38]: See the THESAURUS file for more information.  [^39]\n",
            "\n",
            "[^39]: This work is a part of the [insert bibliography].  [^40]\n",
            "\n",
            "[^40]: See the BIBLIOGRAPHY file for more information.  [^41]\n",
            "\n",
            "[^41]: This work is a part of the [insert reference].  [^42]\n",
            "\n",
            "[^42]: See the REFERENCE file for more information.  [^43]\n",
            "\n",
            "[^43]: This work is a part of the [insert citation].  [^44]\n",
            "\n",
            "[^44]: See the CITATION file for more information.  [^45]\n",
            "\n",
            "[^45]: This work is a part of the [insert footnote].  [^46]\n",
            "\n",
            "[^46]: See the FOOTNOTE file for more information.  [^47]\n",
            "\n",
            "[^47]: This work is a part of the [insert endnote].  [^48]\n",
            "\n",
            "[^48]: See the ENDNOTE file for more information.  [^49]\n",
            "\n",
            "[^49]: This work is a part of the [insert bibliography].  [^50]\n",
            "\n",
            "[^50]: See the BIBLIOGRAPHY file for more information.  [^51]\n",
            "\n",
            "[^51]: This work is a part of the [insert reference].  [^52]\n",
            "\n",
            "[^52]: See the REFERENCE file for more information.  [^53]\n",
            "\n",
            "[^53]: This work is a part of the [insert citation].  [^54]\n",
            "\n",
            "[^54]: See the CITATION file for more information.  [^55]\n",
            "\n",
            "[^55]: This work is a part of the [insert footnote].  [^56]\n",
            "\n",
            "[^56]: See the FOOTNOTE file for more information.  [^57]\n",
            "\n",
            "[^57]: This work is a part of the [insert endnote].  [^58]\n",
            "\n",
            "[^58]: See the ENDNOTE file for more information.  [^59]\n",
            "\n",
            "[^59]: This work is a part of the [insert bibliography].  [^60]\n",
            "\n",
            "[^60]: See the BIBLIOGRAPHY file for more information.  [^61]\n",
            "\n",
            "[^61]: This work is a part of the [insert reference].  [^62]\n",
            "\n",
            "[^62]: See the REFERENCE file for more information.  [^63]\n",
            "\n",
            "[^63]: This work is a part of the [insert citation].  [^64]\n",
            "\n",
            "[^64]: See the CITATION file for more information.  [^65]\n",
            "\n",
            "[^65]: This work is a part of the [insert footnote].  [^66]\n",
            "\n",
            "[^66]: See the FOOTNOTE file for more information.  [^67]\n",
            "\n",
            "[^67]: This work is a part of the [insert endnote].  [^68]\n",
            "\n",
            "[^68]: See the ENDNOTE file for more information.  [^69]\n",
            "\n",
            "[^69]: This work is a part of the [insert bibliography].  [^70]\n",
            "\n",
            "[^70]: See the BIBLIOGRAPHY file for more information.  [^71]\n",
            "\n",
            "[^71]: This work is a part of the [insert reference].  [^72]\n",
            "\n",
            "[^72]: See the REFERENCE file for more information.  [^73]\n",
            "\n",
            "[^73]: This work is a part of the [insert citation].  [^74]\n",
            "\n",
            "[^74]: See the CITATION file for more information.  [^75]\n",
            "\n",
            "[^75]: This work is a part of the [insert footnote].  [^76]\n",
            "\n",
            "[^76]: See the FOOTNOTE file for more information.  [^77]\n",
            "\n",
            "[^77]: This work is a part of the [insert endnote].  [^78]\n",
            "\n",
            "[^78]: See the ENDNOTE file for more information.  [^79]\n",
            "\n",
            "[^79]: This work is a part of the [insert bibliography].  [^80]\n",
            "\n",
            "[^80]: See the BIBLIOGRAPHY file for more information.  [^81]\n",
            "\n",
            "[^81]: This work is a part of the [insert reference].  [^82]\n",
            "\n",
            "[^82]: See the REFERENCE file for more information.  [^83]\n",
            "\n",
            "[^83]: This work is a part of the [insert citation].  [^84]\n",
            "\n",
            "[^84]: See the CITATION file for more information.  [^85]\n",
            "\n",
            "[^85]: This work is a part of the [insert footnote].  [^86]\n",
            "\n",
            "[^86]: See the FOOTNOTE file for more information.  [^87]\n",
            "\n",
            "[^87]: This work is a part of the [insert endnote].  [^88]\n",
            "\n",
            "[^88]: See the ENDNOTE file for more information.  [^89]\n",
            "\n",
            "[^89]: This work is a part of the [insert bibliography].  [^90]\n",
            "\n",
            "[^90]: See the BIBLIOGRAPHY file for more information.  [^91]\n",
            "\n",
            "[^91]: This work is a part of the [insert reference].  [^92]\n",
            "\n",
            "[^92]: See the REFERENCE file for more information.  [^93]\n",
            "\n",
            "[^93]: This work is a part of the [insert citation].  [^94]\n",
            "\n",
            "[^94]: See the CITATION file for more information.  [^95]\n",
            "\n",
            "[^95]: This work is a part of the [insert footnote].  [^96]\n",
            "\n",
            "[^96]: See the FOOTNOTE file for more information.  [^97]\n",
            "\n",
            "[^97]: This work is a part of the [insert endnote].  [^98]\n",
            "\n",
            "[^98]: See the ENDNOTE file for more information.  [^99]\n",
            "\n",
            "[^99]: This work is a part of the [insert bibliography].  [^100]\n",
            "\n",
            "[^100]: See the BIBLIOGRAPHY file for more information.  [^101]\n",
            "\n",
            "[^101]: This work is a part of the [insert reference].  [^102]\n",
            "\n",
            "[^102]: See the REFERENCE file for more information.  [^103]\n",
            "\n",
            "[^103]: This work is a part of the [insert citation].  [^104]\n",
            "\n",
            "[^104]: See the CITATION file for more information.  [^105]\n",
            "\n",
            "[^105]: This work is a part of the [insert footnote].  [^106]\n",
            "\n",
            "[^106]: See the FOOTNOTE file for more information.  [^107]\n",
            "\n",
            "[^107]: This work is a part of the [insert endnote].  [^108]\n",
            "\n",
            "[^108]: See the ENDNOTE file for more information.  [^109]\n",
            "\n",
            "[^109]: This work is a part of the [insert bibliography].  [^110]\n",
            "\n",
            "[^110]: See the BIBLIOGRAPHY file for more information.  [^111]\n",
            "\n",
            "[^111]: This work is a part of the [insert reference].  [^112]\n",
            "\n",
            "[^112]: See the REFERENCE file for more information.  [^113]\n",
            "\n",
            "[^113]: This work is a part of the [insert citation].  [^114]\n",
            "\n",
            "[^114]: See the CITATION file for more information.  [^115]\n",
            "\n",
            "[^115]: This work is a part of the [insert footnote].  [^116]\n",
            "\n",
            "[^116]: See the FOOTNOTE file for more information.  [^117]\n",
            "\n",
            "[^117]: This work is a part of the [insert endnote].  [^118]\n",
            "\n",
            "[^118]: See the ENDNOTE file for more information.  [^119]\n",
            "\n",
            "[^119]: This work is a part of the [insert bibliography].  [^120]\n",
            "\n",
            "[^120]: See the BIBLIOGRAPHY file for more information.  [^121]\n",
            "\n",
            "[^121]: This work is a part of the [insert reference].  [^122]\n",
            "\n",
            "[^122]: See the REFERENCE file for more information.  [^123]\n",
            "\n",
            "[^123]: This work is a part of the [insert citation].  [^124]\n",
            "\n",
            "[^124]: See the CITATION file for more information.  [^125]\n",
            "\n",
            "[^125]: This work is a part of the [insert footnote].  [^126]\n",
            "\n",
            "[^126]: See the FOOTNOTE file for more information.  [^127]\n",
            "\n",
            "[^127]: This work is a part of the [insert endnote].  [^128]\n",
            "\n",
            "[^128]: See the ENDNOTE file for more information.  [^129]\n",
            "\n",
            "[^129]: This work is a part of the [insert bibliography].  [^130]\n",
            "\n",
            "[^130]: See the BIBLIOGRAPHY file for more information.  [^131]\n",
            "\n",
            "[^131]: This work is a part of the [insert reference].  [^132]\n",
            "\n",
            "[^132]: See the REFERENCE file for more information.  [^133]\n",
            "\n",
            "[^133]: This work is a part of the [insert citation].  [^134]\n",
            "\n",
            "[^134]: See the CITATION file for more information.  [^135]\n",
            "\n",
            "[^135]: This work is a part of the [insert footnote].  [^136]\n",
            "\n",
            "[^136]: See the FOOTNOTE file for more information.  [^137]\n",
            "\n",
            "[^137]: This work is a part of the [insert endnote].  [^138]\n",
            "\n",
            "[^138]: See the ENDNOTE file for more information.  [^139]\n",
            "\n",
            "[^139]: This work is a part of the [insert bibliography].  [^140]\n",
            "\n",
            "[^140]: See the BIBLIOGRAPHY file for more information.  [^141]\n",
            "\n",
            "[^141]: This work is a part of the [insert reference].  [^142]\n",
            "\n",
            "[^142]: See the REFERENCE file for more information.  [^143]\n",
            "\n",
            "[^143]: This work is a part of the [insert citation].  [^144]\n",
            "\n",
            "[^144]: See the CITATION file for more information.  [^145]\n",
            "\n",
            "[^145]: This work is a part of the [insert footnote].  [^146]\n",
            "\n",
            "[^146]: See the FOOTNOTE file for more information.  [^147]\n",
            "\n",
            "[^147]: This work is a part of the [insert endnote].  [^148]\n",
            "\n",
            "[^148]: See the ENDNOTE file for more information.  [^149]\n",
            "\n",
            "[^149]: This work is a part of the [insert bibliography].  [^150]\n",
            "\n",
            "[^150]: See the BIBLIOGRAPHY file for more information.  [^151]\n",
            "\n",
            "[^151]: This work is a part of the [insert reference].  [^152]\n",
            "\n",
            "[^152]: See the REFERENCE file for more information.  [^153]\n",
            "\n",
            "[^153]: This work is a part of the [insert citation].  [^154]\n",
            "\n",
            "[^154]: See the CITATION file for more information.  [^155]\n",
            "\n",
            "[^155]: This work is a part of the [insert footnote].  [^156]\n",
            "\n",
            "[^156]: See the FOOTNOTE file for more information.  [^157]\n",
            "\n",
            "[^157]: This work is a part of the [insert endnote].  [^158]\n",
            "\n",
            "[^158]: See the ENDNOTE file for more information.  [^159]\n",
            "\n",
            "[^159]: This work is a part of the [insert bibliography].  [^160]\n",
            "\n",
            "[^160]: See the BIBLIOGRAPHY file for more information.  [^161]\n",
            "\n",
            "[^161]: This work is a part of the [insert reference].  [^162]\n",
            "\n",
            "[^162]: See the REFERENCE file for more information.  [^163]\n",
            "\n",
            "[^163]: This work is a part of the [insert citation].  [^164]\n",
            "\n",
            "[^164]: See the CITATION file for more information.  [^165]\n",
            "\n",
            "[^165]: This work is a part of the [insert footnote].  [^166]\n",
            "\n",
            "[^166]: See the FOOTNOTE file for more information.  [^167]\n",
            "\n",
            "[^167]: This work is a part of the [insert endnote].  [^168]\n",
            "\n",
            "[^168]: See the ENDNOTE file for more information.  [^169]\n",
            "\n",
            "[^169]: This work is a part of the [insert bibliography].  [^170]\n",
            "\n",
            "[^170]: See the BIBLIOGRAPHY file for more information.  [^171]\n",
            "\n",
            "[^171]: This work is a part of the [insert reference].  [^172]\n",
            "\n",
            "[^172]: See the REFERENCE file for more information.  [^173]\n",
            "\n",
            "[^173]: This work is a part of the [insert citation].  [^174]\n",
            "\n",
            "[^174]: See the CITATION file for more information.  [^175]\n",
            "\n",
            "[^175]: This work\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "09D26CpOgP4D"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "Overview of Colaboratory Features",
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "V6E1",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "TPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "34ebb8bdb2e549c7990a8eec5a8de0ab": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_03cefe698822472683ef1a7ac4cfaf2f",
              "IPY_MODEL_6fbb496ecf4e4e09a7786a25518ad8f0",
              "IPY_MODEL_695e27403b154b2f9b4eb4a986c7b657"
            ],
            "layout": "IPY_MODEL_c8de21e2b60a41559440e0c3a3793650"
          }
        },
        "03cefe698822472683ef1a7ac4cfaf2f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2a9096326b12418492d93e7059966d77",
            "placeholder": "",
            "style": "IPY_MODEL_ac07b9b93dfd4379a69d776892b31257",
            "value": "Addingrequests:100%"
          }
        },
        "6fbb496ecf4e4e09a7786a25518ad8f0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d4da237f52704cdba2c0f06d8ff26c41",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_acc83e05883244b7b27af18d8b506fbb",
            "value": 1
          }
        },
        "695e27403b154b2f9b4eb4a986c7b657": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b0ff6aeeb5e44e0c9c0b09b9669c8c45",
            "placeholder": "",
            "style": "IPY_MODEL_8a97549363f1442fb1f48a4700d1695f",
            "value": "1/1[00:00&lt;00:00,219.85it/s]"
          }
        },
        "c8de21e2b60a41559440e0c3a3793650": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2a9096326b12418492d93e7059966d77": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ac07b9b93dfd4379a69d776892b31257": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d4da237f52704cdba2c0f06d8ff26c41": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "acc83e05883244b7b27af18d8b506fbb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b0ff6aeeb5e44e0c9c0b09b9669c8c45": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8a97549363f1442fb1f48a4700d1695f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1f1a43f829ef40fa87671ce95306203f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_ef617c93316d4e3083bc0e8cd657e7b1",
              "IPY_MODEL_f743585a80a04514acc70de83269e47b",
              "IPY_MODEL_1639636c050144ce9313fd10d2ae083d"
            ],
            "layout": "IPY_MODEL_9e3fcbe9310a47f19ae0bedf30f611b0"
          }
        },
        "ef617c93316d4e3083bc0e8cd657e7b1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fabba770b3514aaeb9eef4ac47bb0642",
            "placeholder": "",
            "style": "IPY_MODEL_7eca025033714bf584468d9ddd2492f9",
            "value": "Processedprompts:100%"
          }
        },
        "f743585a80a04514acc70de83269e47b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_142ec051041b4cc28a204cf47de4b310",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c3dbccc00ca74e0e81d3e0303e132c27",
            "value": 1
          }
        },
        "1639636c050144ce9313fd10d2ae083d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_40d46fee804e42fb8994bb0d685fa719",
            "placeholder": "",
            "style": "IPY_MODEL_c979caa05e2549ce8c84a9ec96de9d7b",
            "value": "1/1[01:04&lt;00:00,64.52s/it,est.speedinput:0.23toks/s,output:63.25toks/s]"
          }
        },
        "9e3fcbe9310a47f19ae0bedf30f611b0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": "inline-flex",
            "flex": null,
            "flex_flow": "row wrap",
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "100%"
          }
        },
        "fabba770b3514aaeb9eef4ac47bb0642": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7eca025033714bf584468d9ddd2492f9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "142ec051041b4cc28a204cf47de4b310": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": "2",
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c3dbccc00ca74e0e81d3e0303e132c27": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "40d46fee804e42fb8994bb0d685fa719": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c979caa05e2549ce8c84a9ec96de9d7b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}