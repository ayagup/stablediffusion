{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.13",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [],
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "name": "t2v.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ayagup/stablediffusion/blob/main/t2v.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# This Python 3 environment comes with many helpful analytics libraries installed\n",
        "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
        "# For example, here's several helpful packages to load\n",
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "# Input data files are available in the read-only \"../input/\" directory\n",
        "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
        "\n",
        "import os\n",
        "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirname, filename))\n",
        "\n",
        "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\"\n",
        "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "id": "6jB83fDpPzLh"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install torch>=2.0.0 diffusers>=0.25.0 transformers>=4.35.0 accelerate>=0.24.0 safetensors>=0.4.0 opencv-python>=4.8.0 imageio>=2.31.0 imageio-ffmpeg>=0.4.9 pillow>=10.0.0 numpy>=1.24.0 huggingface-hub>=0.19.0"
      ],
      "metadata": {
        "trusted": true,
        "id": "lxYckebjPzLi"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Simple Text-to-Video Inference Example\n",
        "A simplified version for quick testing with publicly available models\n",
        "\"\"\"\n",
        "\n",
        "import torch\n",
        "from diffusers import DiffusionPipeline, DPMSolverMultistepScheduler\n",
        "from diffusers.utils import export_to_video\n",
        "import os\n",
        "\n",
        "# Suppress TensorFlow warnings\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
        "\n",
        "\n",
        "def generate_video(\n",
        "    prompt: str,\n",
        "    output_path: str = \"output.mp4\",\n",
        "    model_id: str = \"damo-vilab/text-to-video-ms-1.7b\",\n",
        "    num_frames: int = 16,\n",
        "    height: int = 256,\n",
        "    width: int = 256,\n",
        "):\n",
        "    \"\"\"\n",
        "    Generate a video from a text prompt using OpenSora\n",
        "\n",
        "    Args:\n",
        "        prompt: Text description of the video to generate\n",
        "        output_path: Path to save the output video\n",
        "        model_id: Hugging Face model identifier\n",
        "        num_frames: Number of frames to generate\n",
        "        height: Video height in pixels\n",
        "        width: Video width in pixels\n",
        "    \"\"\"\n",
        "\n",
        "    # Determine device and check for multiple GPUs\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "    if torch.cuda.is_available():\n",
        "        num_gpus = torch.cuda.device_count()\n",
        "        print(f\"Using device: {device}\")\n",
        "        print(f\"Number of GPUs available: {num_gpus}\")\n",
        "        for i in range(num_gpus):\n",
        "            print(f\"  GPU {i}: {torch.cuda.get_device_name(i)}\")\n",
        "            print(f\"    Memory: {torch.cuda.get_device_properties(i).total_memory / 1e9:.2f} GB\")\n",
        "    else:\n",
        "        print(f\"Using device: {device}\")\n",
        "\n",
        "    # Load model\n",
        "    print(f\"\\nLoading model: {model_id}...\")\n",
        "    pipe = DiffusionPipeline.from_pretrained(\n",
        "        model_id,\n",
        "        torch_dtype=torch.float16 if device == \"cuda\" else torch.float32,\n",
        "        variant=\"fp16\" if device == \"cuda\" else None,\n",
        "    )\n",
        "\n",
        "    # Use faster scheduler\n",
        "    pipe.scheduler = DPMSolverMultistepScheduler.from_config(pipe.scheduler.config)\n",
        "\n",
        "    # Multi-GPU setup\n",
        "    if device == \"cuda\" and torch.cuda.device_count() > 1:\n",
        "        print(f\"\\nðŸš€ Multi-GPU mode enabled: Using {torch.cuda.device_count()} GPUs\")\n",
        "        # Enable sequential CPU offload for better multi-GPU memory management\n",
        "        pipe.enable_sequential_cpu_offload()\n",
        "        pipe.enable_vae_slicing()\n",
        "        # Note: DiffusionPipeline handles multi-GPU automatically via accelerate\n",
        "    else:\n",
        "        pipe = pipe.to(device)\n",
        "        # Enable optimizations for single GPU\n",
        "        if device == \"cuda\":\n",
        "            pipe.enable_model_cpu_offload()\n",
        "            pipe.enable_vae_slicing()\n",
        "\n",
        "    # Generate video\n",
        "    print(f\"Generating video for prompt: '{prompt}'\")\n",
        "    result = pipe(\n",
        "        prompt=prompt,\n",
        "        num_frames=num_frames,\n",
        "        height=height,\n",
        "        width=width,\n",
        "        num_inference_steps=50,\n",
        "        guidance_scale=7.5,\n",
        "    )\n",
        "\n",
        "    # Extract frames\n",
        "    video_frames = result.frames[0]\n",
        "\n",
        "    # Export to video file\n",
        "    export_to_video(video_frames, output_path, fps=8)\n",
        "    print(f\"Video saved to: {output_path}\")\n",
        "\n",
        "    return video_frames\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Example usage\n",
        "    prompt = \"A beautiful woman standing in the middle of a room. full body frame. She is combing her hair.\"\n",
        "\n",
        "    try:\n",
        "        frames = generate_video(\n",
        "            prompt=prompt,\n",
        "            output_path=\"/kaggle/working/sunset_video.mp4\",\n",
        "            num_frames=160,\n",
        "            height=256,\n",
        "            width=256,\n",
        "        )\n",
        "        print(f\"Successfully generated {len(frames)} frames!\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error: {e}\")\n",
        "        import traceback\n",
        "        traceback.print_exc()\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-18T10:24:40.193858Z",
          "iopub.execute_input": "2025-10-18T10:24:40.194565Z",
          "iopub.status.idle": "2025-10-18T10:34:09.075967Z",
          "shell.execute_reply.started": "2025-10-18T10:24:40.19454Z",
          "shell.execute_reply": "2025-10-18T10:34:09.074867Z"
        },
        "id": "Fhm0JfxNPzLj",
        "outputId": "d7c77953-30f7-4df4-86f2-4411e36a397b",
        "colab": {
          "referenced_widgets": [
            "e8998fc9848b4d3398a9169108549870",
            "dfabbd4b997d4f188fb9df30d43284b1",
            "079af84ca7d342e5b2f89a81faf7014a",
            "4323c940011c41e4ab394ea832c8c2d4",
            "41bc3e9af094447abf48794b0323157c",
            "54925ea538c44e49abd684a75e6dbdeb",
            "6e912de945b84a85a120199420b55fd1",
            "2ba56d26fdba41f6a4c38e4740251486",
            "0a77cc771a634a90b2bdf1b8e8c34207",
            "e6eac5ec90c54a8f9eced94248d76f7a",
            "6cfe730c83c64b72b9c1f742ea5f34ca",
            "8dfb15f882cc419d9f57169431617bd0",
            "650d2aeefee541819f3eb67f44a6189a",
            "d9878f9c84584d83953c04cb7db22d7c",
            "04e60fdd91d74c21a8dc97ab0a5d8b74"
          ]
        }
      },
      "outputs": [
        {
          "name": "stderr",
          "text": "2025-10-18 10:24:57.728035: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1760783097.936641      37 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1760783097.996536      37 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "Using device: cuda\nNumber of GPUs available: 2\n  GPU 0: Tesla T4\n    Memory: 15.83 GB\n  GPU 1: Tesla T4\n    Memory: 15.83 GB\n\nLoading model: damo-vilab/text-to-video-ms-1.7b...\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "model_index.json:   0%|          | 0.00/384 [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e8998fc9848b4d3398a9169108549870"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Fetching 12 files:   0%|          | 0/12 [00:00<?, ?it/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "dfabbd4b997d4f188fb9df30d43284b1"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "scheduler_config.json:   0%|          | 0.00/465 [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "079af84ca7d342e5b2f89a81faf7014a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "config.json:   0%|          | 0.00/644 [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "4323c940011c41e4ab394ea832c8c2d4"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "special_tokens_map.json:   0%|          | 0.00/460 [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "41bc3e9af094447abf48794b0323157c"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "tokenizer_config.json:   0%|          | 0.00/755 [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "54925ea538c44e49abd684a75e6dbdeb"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "config.json:   0%|          | 0.00/787 [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6e912de945b84a85a120199420b55fd1"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "merges.txt: 0.00B [00:00, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "2ba56d26fdba41f6a4c38e4740251486"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "vocab.json: 0.00B [00:00, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "0a77cc771a634a90b2bdf1b8e8c34207"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "text_encoder/model.fp16.safetensors:   0%|          | 0.00/681M [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "e6eac5ec90c54a8f9eced94248d76f7a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "unet/diffusion_pytorch_model.fp16.safete(â€¦):   0%|          | 0.00/2.82G [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "6cfe730c83c64b72b9c1f742ea5f34ca"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "config.json:   0%|          | 0.00/657 [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8dfb15f882cc419d9f57169431617bd0"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "vae/diffusion_pytorch_model.fp16.safeten(â€¦):   0%|          | 0.00/167M [00:00<?, ?B/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "650d2aeefee541819f3eb67f44a6189a"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Loading pipeline components...:   0%|          | 0/5 [00:00<?, ?it/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d9878f9c84584d83953c04cb7db22d7c"
            }
          },
          "metadata": {}
        },
        {
          "name": "stderr",
          "text": "The TextToVideoSDPipeline has been deprecated and will not receive bug fixes or feature updates after Diffusers version 0.33.1. \n",
          "output_type": "stream"
        },
        {
          "name": "stdout",
          "text": "\nðŸš€ Multi-GPU mode enabled: Using 2 GPUs\nGenerating video for prompt: 'A beautiful woman standing in the middle of a room. full body frame. She is combing her hair.'\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "  0%|          | 0/50 [00:00<?, ?it/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "04e60fdd91d74c21a8dc97ab0a5d8b74"
            }
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "text": "Video saved to: /kaggle/working/sunset_video.mp4\nSuccessfully generated 160 frames!\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "%pip uninstall numpy -y"
      ],
      "metadata": {
        "trusted": true,
        "id": "xODLUN-CPzLk"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install \"numpy<2.0.0\""
      ],
      "metadata": {
        "trusted": true,
        "id": "p_ah8tCkPzLl"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "MQfDDg5zPzLl"
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}