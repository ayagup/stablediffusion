{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.13",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [],
      "dockerImageVersionId": 31154,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "name": "text_to_image",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ayagup/stablediffusion/blob/main/text_to_image.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# This Python 3 environment comes with many helpful analytics libraries installed\n",
        "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
        "# For example, here's several helpful packages to load\n",
        "\n",
        "import numpy as np # linear algebra\n",
        "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
        "\n",
        "# Input data files are available in the read-only \"../input/\" directory\n",
        "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
        "\n",
        "import os\n",
        "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
        "    for filename in filenames:\n",
        "        print(os.path.join(dirname, filename))\n",
        "\n",
        "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\"\n",
        "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
      ],
      "metadata": {
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "trusted": true,
        "id": "qotUGaB872zd"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Simple Text-to-Image Generation Pipeline\n",
        "Generate images from text prompts using HuggingFace Diffusion models\n",
        "\"\"\"\n",
        "\n",
        "import torch\n",
        "from diffusers import DiffusionPipeline, StableDiffusionPipeline, StableDiffusionXLPipeline\n",
        "from PIL import Image\n",
        "import os\n",
        "import gc\n",
        "import time\n",
        "from typing import Optional\n",
        "\n",
        "# Suppress warnings\n",
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
        "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n",
        "\n",
        "\n",
        "def print_header():\n",
        "    \"\"\"Print a nice header\"\"\"\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"üé® Text-to-Image Generation Pipeline\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "\n",
        "def clear_gpu_memory():\n",
        "    \"\"\"Clear GPU memory cache\"\"\"\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()\n",
        "        gc.collect()\n",
        "        print(\"üßπ GPU memory cache cleared\")\n",
        "\n",
        "\n",
        "def generate_image(\n",
        "    prompt: str,\n",
        "    model_name: str = \"stabilityai/stable-diffusion-2-1\",\n",
        "    output_path: str = \"generated_image.png\",\n",
        "    negative_prompt: Optional[str] = None,\n",
        "    num_inference_steps: int = 50,\n",
        "    guidance_scale: float = 7.5,\n",
        "    width: int = 512,\n",
        "    height: int = 512,\n",
        "    seed: Optional[int] = None,\n",
        ") -> Image.Image:\n",
        "    \"\"\"\n",
        "    Generate image from text prompt\n",
        "\n",
        "    Args:\n",
        "        prompt: Text description of desired image\n",
        "        model_name: HuggingFace model identifier\n",
        "        output_path: Output path for generated image\n",
        "        negative_prompt: What to avoid in generation\n",
        "        num_inference_steps: Number of denoising steps (higher = better quality)\n",
        "        guidance_scale: How closely to follow prompt (7-12 recommended)\n",
        "        width: Image width in pixels\n",
        "        height: Image height in pixels\n",
        "        seed: Random seed for reproducibility\n",
        "\n",
        "    Returns:\n",
        "        Generated PIL Image\n",
        "    \"\"\"\n",
        "\n",
        "    print_header()\n",
        "    clear_gpu_memory()\n",
        "\n",
        "    # Device setup\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "    if device == \"cuda\":\n",
        "        gpu_name = torch.cuda.get_device_name(0)\n",
        "        gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3\n",
        "        print(f\"\\nüñ•Ô∏è  Using GPU: {gpu_name}\")\n",
        "        print(f\"   Memory: {gpu_memory:.1f} GB\\n\")\n",
        "    else:\n",
        "        print(\"\\nüíª Using CPU (Warning: This will be very slow!)\\n\")\n",
        "\n",
        "    # Load model\n",
        "    print(f\"Loading model: {model_name}\")\n",
        "    start_time = time.time()\n",
        "\n",
        "    # Determine model type and load appropriately\n",
        "    if \"xl\" in model_name.lower():\n",
        "        pipe = StableDiffusionXLPipeline.from_pretrained(\n",
        "            model_name,\n",
        "            torch_dtype=torch.float16 if device == \"cuda\" else torch.float32,\n",
        "            use_safetensors=True,\n",
        "            variant=\"fp16\" if device == \"cuda\" else None\n",
        "        )\n",
        "    else:\n",
        "        pipe = StableDiffusionPipeline.from_pretrained(\n",
        "            model_name,\n",
        "            torch_dtype=torch.float16 if device == \"cuda\" else torch.float32,\n",
        "            use_safetensors=True,\n",
        "            variant=\"fp16\" if device == \"cuda\" else None\n",
        "        )\n",
        "\n",
        "    pipe = pipe.to(device)\n",
        "\n",
        "    # Enable memory optimizations\n",
        "    if device == \"cuda\":\n",
        "        pipe.enable_attention_slicing()\n",
        "        pipe.enable_vae_slicing()\n",
        "\n",
        "    load_time = time.time() - start_time\n",
        "    print(f\"‚úì Model loaded in {load_time:.2f}s\")\n",
        "\n",
        "    # Set seed for reproducibility\n",
        "    if seed is not None:\n",
        "        generator = torch.Generator(device=device).manual_seed(seed)\n",
        "        print(f\"üé≤ Using seed: {seed}\")\n",
        "    else:\n",
        "        generator = None\n",
        "\n",
        "    # Print generation parameters\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"üé¨ Generation Parameters\")\n",
        "    print(\"=\"*70)\n",
        "    print(f\"Prompt: {prompt}\")\n",
        "    if negative_prompt:\n",
        "        print(f\"Negative: {negative_prompt}\")\n",
        "    print(f\"Model: {model_name}\")\n",
        "    print(f\"Device: {device}\")\n",
        "    print(f\"Resolution: {width}x{height}\")\n",
        "    print(f\"Steps: {num_inference_steps}\")\n",
        "    print(f\"Guidance Scale: {guidance_scale}\")\n",
        "    if seed is not None:\n",
        "        print(f\"Seed: {seed}\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    # Generate image\n",
        "    print(\"\\nüé® Generating image...\")\n",
        "    print(f\"Progress: This may take {num_inference_steps * 0.5:.0f}-{num_inference_steps:.0f} seconds...\")\n",
        "\n",
        "    generation_start = time.time()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        image = pipe(\n",
        "            prompt=prompt,\n",
        "            negative_prompt=negative_prompt,\n",
        "            num_inference_steps=num_inference_steps,\n",
        "            guidance_scale=guidance_scale,\n",
        "            width=width,\n",
        "            height=height,\n",
        "            generator=generator,\n",
        "        ).images[0]\n",
        "\n",
        "    generation_time = time.time() - generation_start\n",
        "    print(f\"‚úì Generation completed in {generation_time:.2f}s\")\n",
        "\n",
        "    # Save image\n",
        "    image.save(output_path)\n",
        "    print(f\"\\nüíæ Image saved: {output_path}\")\n",
        "\n",
        "    # Summary\n",
        "    total_time = time.time() - start_time + load_time\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"‚úÖ Generation Complete!\")\n",
        "    print(\"=\"*70)\n",
        "    print(f\"Total time: {total_time:.2f}s\")\n",
        "    print(f\"  - Model loading: {load_time:.2f}s\")\n",
        "    print(f\"  - Image generation: {generation_time:.2f}s\")\n",
        "    print(f\"\\nOutput: {output_path}\")\n",
        "    print(f\"Resolution: {width}x{height}\")\n",
        "    print(f\"Quality: {num_inference_steps} steps\")\n",
        "    print(\"=\"*70 + \"\\n\")\n",
        "\n",
        "    clear_gpu_memory()\n",
        "\n",
        "    return image\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-18T20:03:50.247562Z",
          "iopub.execute_input": "2025-10-18T20:03:50.247865Z",
          "iopub.status.idle": "2025-10-18T20:04:20.690597Z",
          "shell.execute_reply.started": "2025-10-18T20:03:50.247838Z",
          "shell.execute_reply": "2025-10-18T20:04:20.689954Z"
        },
        "id": "odzruEaI72zd",
        "outputId": "4211b2ac-ede6-4f81-8b4a-aee48f0d23e4"
      },
      "outputs": [
        {
          "name": "stderr",
          "text": "2025-10-18 20:04:07.087134: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1760817847.278311      37 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1760817847.335665      37 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "# if __name__ == \"__main__\":\n",
        "#     import argparse\n",
        "\n",
        "#     parser = argparse.ArgumentParser(\n",
        "#         description='Simple Text-to-Image Generation',\n",
        "#         formatter_class=argparse.RawDescriptionHelpFormatter,\n",
        "#         epilog=\"\"\"\n",
        "# Examples:\n",
        "#   # Basic generation\n",
        "#   python simple_text_to_image.py --prompt \"a beautiful sunset over mountains\"\n",
        "\n",
        "#   # High quality with more steps\n",
        "#   python simple_text_to_image.py --prompt \"a cat in a hat\" --steps 100\n",
        "\n",
        "#   # With negative prompt\n",
        "#   python simple_text_to_image.py \\\n",
        "#     --prompt \"a portrait of a person\" \\\n",
        "#     --negative \"blurry, low quality, distorted\"\n",
        "\n",
        "#   # Different resolution\n",
        "#   python simple_text_to_image.py \\\n",
        "#     --prompt \"a landscape painting\" \\\n",
        "#     --width 768 --height 512\n",
        "\n",
        "#   # Reproducible generation with seed\n",
        "#   python simple_text_to_image.py \\\n",
        "#     --prompt \"a fantasy castle\" \\\n",
        "#     --seed 42\n",
        "#         \"\"\"\n",
        "#     )\n",
        "\n",
        "#     parser.add_argument('--prompt', type=str, required=True,\n",
        "#                         help='Text prompt describing the desired image')\n",
        "#     parser.add_argument('--negative', type=str, default=None,\n",
        "#                         help='Negative prompt (what to avoid)')\n",
        "#     parser.add_argument('--model', type=str,\n",
        "#                         default='stabilityai/stable-diffusion-2-1',\n",
        "#                         help='HuggingFace model name')\n",
        "#     parser.add_argument('--output', type=str, default='generated_image.png',\n",
        "#                         help='Output path for generated image')\n",
        "#     parser.add_argument('--steps', type=int, default=50,\n",
        "#                         help='Number of inference steps (20-100)')\n",
        "#     parser.add_argument('--guidance', type=float, default=7.5,\n",
        "#                         help='Guidance scale (7-12 recommended)')\n",
        "#     parser.add_argument('--width', type=int, default=512,\n",
        "#                         help='Image width')\n",
        "#     parser.add_argument('--height', type=int, default=512,\n",
        "#                         help='Image height')\n",
        "#     parser.add_argument('--seed', type=int, default=None,\n",
        "#                         help='Random seed for reproducibility')\n",
        "\n",
        "#     args = parser.parse_args()\n",
        "\n",
        "try:\n",
        "    generate_image(\n",
        "        prompt='very realistic image of a man with the head of a dog',\n",
        "        model_name='stabilityai/stable-diffusion-2-1',\n",
        "        output_path='/kaggle/working/output.jpg',\n",
        "        negative_prompt='bad quality, low resolution image, low quality image',\n",
        "        num_inference_steps=50,\n",
        "        guidance_scale=7.5,\n",
        "        width=512,\n",
        "        height=512,\n",
        "        seed=None,\n",
        "    )\n",
        "except Exception as e:\n",
        "    print(f\"\\n‚ùå Error: {e}\")\n",
        "    import traceback\n",
        "    traceback.print_exc()\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-10-18T20:06:49.452765Z",
          "iopub.execute_input": "2025-10-18T20:06:49.453059Z",
          "iopub.status.idle": "2025-10-18T20:07:06.70835Z",
          "shell.execute_reply.started": "2025-10-18T20:06:49.453038Z",
          "shell.execute_reply": "2025-10-18T20:07:06.707393Z"
        },
        "id": "6AMH25vE72ze",
        "outputId": "a11f866b-3ed8-49b9-8f3e-db8560b1a0a3",
        "colab": {
          "referenced_widgets": [
            "cf014b16c3674486bbbeea6cd3c50295",
            "f6b262af8e0b478882708a7fbea72c29"
          ]
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "text": "\n======================================================================\nüé® Text-to-Image Generation Pipeline\n======================================================================\nüßπ GPU memory cache cleared\n\nüñ•Ô∏è  Using GPU: Tesla P100-PCIE-16GB\n   Memory: 15.9 GB\n\nLoading model: stabilityai/stable-diffusion-2-1\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "Loading pipeline components...:   0%|          | 0/6 [00:00<?, ?it/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "cf014b16c3674486bbbeea6cd3c50295"
            }
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "text": "‚úì Model loaded in 1.66s\n\n======================================================================\nüé¨ Generation Parameters\n======================================================================\nPrompt: very realistic image of a man with the head of a dog\nNegative: bad quality, low resolution image, low quality image\nModel: stabilityai/stable-diffusion-2-1\nDevice: cuda\nResolution: 512x512\nSteps: 50\nGuidance Scale: 7.5\n======================================================================\n\nüé® Generating image...\nProgress: This may take 25-50 seconds...\n",
          "output_type": "stream"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": "  0%|          | 0/50 [00:00<?, ?it/s]",
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "f6b262af8e0b478882708a7fbea72c29"
            }
          },
          "metadata": {}
        },
        {
          "name": "stdout",
          "text": "‚úì Generation completed in 14.65s\n\nüíæ Image saved: /kaggle/working/output.jpg\n\n======================================================================\n‚úÖ Generation Complete!\n======================================================================\nTotal time: 17.98s\n  - Model loading: 1.66s\n  - Image generation: 14.65s\n\nOutput: /kaggle/working/output.jpg\nResolution: 512x512\nQuality: 50 steps\n======================================================================\n\nüßπ GPU memory cache cleared\n",
          "output_type": "stream"
        }
      ],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "trusted": true,
        "id": "CgkCOjGx72zf"
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}